[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "",
    "text": "‎\nWhile conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving the PPLs to handle the computational details for them (Bessiere et al. 2013; Daniel Roy 2015; Ghahramani 2015). The inference process generates a posterior distribution - which has a central role in Bayesian statistics - together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivated the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\nWe collectively call all these tasks Exploratory analysis of Bayesian models, building on concepts from Exploratory data analysis to examine and gain deeper insights into Bayesian models.\nIn the words of Persi Diaconis (Diaconis 2011):\nIn this book we discuss how to use both numerical and visual summaries to successfully perform the many tasks that are central to the iterative and interactive modeling process. To do so, we first discuss some general principles of data visualization and uncertainty representation that are not exclusive of Bayesian statistics.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#citations",
    "href": "index.html#citations",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "Citations",
    "text": "Citations\nIf you are using specific methods or functions from the book, please consider citing the scientific paper and/or corresponding package.\nIf you want to cite this online book in your research. The following citation is recommended, as it always resolves to the latest version of the book:\n\nMartin et al. (2025). Exploratory Analysis of Bayesian Models. Zenodo. https://zenodo.org/records/15127549\n\nYou can use the following BibTeX entry:\n@book{eabm_2025,\n  author       = {Osvaldo A Martin and Oriol Abril-Pla and Jordan Deklerk},\n  title        = {Exploratory analysis of Bayesian models},\n  month        = nov,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {v0.3.0},\n  doi          = {10.5281/zenodo.15127548},\n  url          = {https://doi.org/10.5281/zenodo.15127548},\n                  },",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#donations",
    "href": "index.html#donations",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "Donations",
    "text": "Donations\nIf you find this book useful, please consider supporting the authors by making a donation. This will help us to keep the book updated and to provide more resources in the future.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "License",
    "text": "License\nThis book is licensed under the CC-BY-NC 4.0. License. See the LICENSE file for details.\n\n\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel Mekhnacha. 2013. Bayesian Programming. 1 edition. Boca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” In Exploring Data Tables, Trends, and Shapes, 1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine Learning and Artificial Intelligence.” Nature 521 (7553): 452–59. https://doi.org/10.1038/nature14541.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html",
    "href": "Chapters/Elements_of_visualization.html",
    "title": "1  Elements of Visualization",
    "section": "",
    "text": "1.1 Data visualization\nVisualizations occupy a central place in modern statistics. Visualizations help us identify patterns, detect anomalies, formulate hypotheses, and gain an intuitive understanding of the data (Wilke 2019; Healy 2019; Unwin 2024). These are tasks usually associated with the exploratory data analysis (EDA) phase (Tukey 1977; Downey 2025). But visualizations are also essential for other tasks like understanding model behaviour, diagnostic computational issues, validating assumptions and communicating results.\nTo understand why visualizations are so effective in these tasks, it helps to consider how the human visual system processes information. Humans are naturally skilled at processing visual information. Compared to words, tables, or raw numbers, well-crafted visualizations tend to convey information more efficiently and intuitively. However, our visual system can be fooled, as you may have experienced with visual illusions. The reason is that our visual system is not a perfect measurement device. Instead, it has been evolutionary-tuned to process information in ways that tend to be useful in the natural settings our ancestors lived. In other words, our brains don’t just see, they guess, infer, and create. Effective data visualization requires us to account for both the strengths and the limitations of our visual system.\nGiven these perceptual constraints, designing effective data visualizations requires careful attention to both how accurately data is represented and how clearly it is perceived. Data visualization inherently involves both an aesthetic and a scientific dimension. On one hand, the scientific component ensures that the visual representation accurately and faithfully conveys the underlying data, supporting robust analysis and clear communication. On the other hand, the aesthetic component is concerned with the clarity, elegance, and visual appeal of the graphics, which can greatly influence how easily the viewer grasps the message being presented. The challenge usually is to generate nice-looking graphics without losing the rigor and veracity of what you want to show.\nStriking this balance depends not just on the data or the design choices, but also on the audience. Different audiences have different needs and expectations. For example, a visualization intended for a scientific audience may prioritize accuracy and detail, while one aimed at a general audience may focus on clarity and simplicity. The same data can be visualized in many different ways, each with its own strengths and weaknesses. The choice of visualization method should be guided by the specific goals of the analysis and the characteristics of the data. In this book, we will focus on visualizations that are useful for statistical analysis.\nAt its core, data visualization is the process of representing data in a visual format with the goal of making complex information more accessible, understandable, and actionable. By using simple visual elements like points, lines, and shapes, we can reveal structure, highlight relationships, and expose patterns that might otherwise remain hidden in raw numbers.\nTwo well-known synthetic—and somewhat extreme—examples that highlight the importance of looking at data, and not just relying on numerical summaries,are Anscombe’s quartet, which consists of four datasets with nearly identical summary statistics but very different visual patterns, and the Datasaurus Dozen, a collection of datasets that appear statistically similar but reveal drastically different shapes when plotted. While these are constructed examples, similar situations often arise in real-world data. For instance, I’ve repeatedly seen cases where a researcher claims a linear regression fits the data well—only to discover upon visual inspection that the data actually consist of two distinct clusters, likely generated by different underlying processes or belonging to separate classes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of Visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#nomenclature",
    "href": "Chapters/Elements_of_visualization.html#nomenclature",
    "title": "1  Elements of Visualization",
    "section": "1.2 Nomenclature",
    "text": "1.2 Nomenclature\nWhen discussing data visualization, there are many terms that are not always used consistently across different sources. To help avoid confusion, we will define a few key terms, as they are used in this book and in the ArviZ library:\nAt a higher level we have a figure, all the element of our visualization are contained inside a figure, a synonym for figure is chart. A figure can contain one or more plots, other terms for plot are subplot or panel. We compose plots by adding visuals to them. They are the basic elements that we can see in a plot, like a point, a line, or a bar. Other names for visuals are aesthetics, geometric object or glyphs. Visuals has properties that can be modified, such as color, size, and shape. We call these properties aesthetics. It’s very common to use different aesthetics to represent different variables in a plot. For example, if we are plotting information for different countries, we can use different color to represent each country. We can say that the color is an aesthetic that encodes the country variable, this is often referred to as mapping or to be less ambiguous as aesthetic mappings. Alternative to a color we can use faceting, that is to create one plot per variable (like country). In practice, we usually combine faceting and aesthetic mappings to represent different variables. For example, we can use different color to represent different countries and create a separate plot for each year. This allows us to create reach visualizations while keeping them easy to read.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of Visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#coordinate-systems-and-axes",
    "href": "Chapters/Elements_of_visualization.html#coordinate-systems-and-axes",
    "title": "1  Elements of Visualization",
    "section": "1.3 Coordinate systems and axes",
    "text": "1.3 Coordinate systems and axes\nData visualization requires defining position scales to determine where different data values are located in a graphic. In 2D visualizations, two numbers are required to uniquely specify a point. Thus, we need two position scales. The arrangement of these scales is known as a coordinate system. The most common coordinate system is the 2D Cartesian system, using x and y values with orthogonal axes. Conventionally with the x-axis running horizontally and the y-axis vertically. Figure 1.1 shows a Cartesian coordinate system.\n\n\n\n\n\n\nFigure 1.1: Cartesian coordinate system\n\n\n\nIn practice, we typically shift the axes so that they do not necessarily pass through the origin (0,0), and instead their location is determined by the data. We do this because it is usually more convenient and easier to read to have the axes to the left and bottom of the figure than in the middle. For instance Figure 1.2 plots the exact same points shown in Figure 1.1 but with the axes placed automatically by matplotlib.\n\n\n\n\n\n\nFigure 1.2: Cartesian coordinate system with axes automatically placed by matplotlib based on the data\n\n\n\nUsually, data has units, such as degrees Celsius for temperature, centimetres for length, or kilograms for weight. In case we are plotting variables of different types (and hence different units) we can adjust the aspect ratio of the axes as we wish. We can make a figure short and wide if it fits better on a page or screen. But we can also change the aspect ratio to highlight important differences, for example, if we want to emphasize changes along the y-axis we can make the figure tall and narrow. When both the x and y axes use the same units, it’s important to maintain an equal ratio to ensure that the relationship between data points on the graph accurately reflects their quantitative values.\nAfter the cartesian coordinate system, the most common coordinate system is the polar coordinate system. In this system, the position of a point is determined by the distance from the origin and the angle with respect to a reference axis. Polar coordinates are useful for representing periodic data, such as days of the week, or data that is naturally represented in a circular shape, such as wind direction. Figure Figure 1.3 shows a polar coordinate system.\n\n\n\n\n\n\nFigure 1.3: Polar coordinate system",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of Visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#good-practices-and-sources-of-error",
    "href": "Chapters/Elements_of_visualization.html#good-practices-and-sources-of-error",
    "title": "1  Elements of Visualization",
    "section": "1.4 Good practices and sources of error",
    "text": "1.4 Good practices and sources of error\nUsing visualization to deceive third parties should not be the goal of an intellectually honest person, but without being careful we can deceive ourselves and other without even realizing it. For example, it has been known for decades that bar-plots are more effective for comparing values than a pie-charts. The reason is that our perceptual apparatus is quite good at evaluating lengths, but not very good at evaluating areas. Figure 1.4 shows different visual elements ordered according to the precision with which the human brain can detect differences and make comparisons between them (Cleveland and McGill 1984; Heer and Bostock 2010).\n\n\n\n\n\n\nFigure 1.4: Scale of elementary perceptual tasks, taken from The Truthful Art\n\n\n\n\n1.4.1 General principles for using colours\nHuman eyes work by essentially perceiving 3 wavelengths, this feature is used in technological devices such as screens to generate all colours from combinations of 3 components, Red, Green, and Blue. This is known as the RGB color model. But this is not the only possible system. A very common alternative is the CYMK color model, Cyan, Yellow, Magenta, and Black.\nTo analyze the perceptual attributes of color, it is better to think in terms of Hue, Saturation, and Lightness, HSL is an alternative representation of the RGB color model.\nThe hue is what we colloquially call “different colours”. Green, red, etc. Saturation is how colourful or washed out we perceive a given color. Two colours with different hues will look more different when they have more saturation. The lightness corresponds to the amount of light emitted (active screens) or reflected (impressions), ranging from black to white:\nVarying the tone is useful to easily distinguish categories as shown in Figure 1.5.\n\n\n\n\n\n\nFigure 1.5: Tone variations can be help to distinguish categories.\n\n\n\nIn principle, most humans are capable of distinguishing millions of tones, but if we want to associate categories with colours, the effectiveness of distinguishing them decreases drastically as the number of categories increases. This happens not only because the tones will be increasingly closer to each other, but also because we have a limited working memory. Associating a few colours (say 4) with categories (countries, temperature ranges, etc.) is usually easy. But unless there are pre-existing associations, remembering many categories becomes challenging and this exacerbates when colours are close to each other. This requires us to continually alternate between the graphic and the legend or text where the color-category association is indicated. Adding other elements besides color such as shapes can help, but in general, it will be more useful to try to keep the number of categories relatively low. In addition, it is important to take into account the presentation context, if we want to show a figure during a presentation where we only have a few seconds to dedicate to that figure, it is advisable to keep the figure as simple as possible. This may involve removing items and displaying only a subset of the data. If the figure is part of a text, where the reader will have the time to analyze for a longer period, perhaps the complexity can be somewhat greater.\nAlthough we mentioned before that human eyes are capable of distinguishing three main colours (red, green, and blue), the ability to distinguish these 3 colours varies between people, to the point that many individuals have difficulty distinguishing some colours. The most common case occurs with red and green. This is why it is important to avoid using those colours. An easy way to avoid this problem is to use color-blind-friendly palettes. We’ll see later that this is an easy thing to do when using ArviZ.\nVarying the lightness as in Figure 1.6 is useful when we want to represent a continuous scale. With the hue-based palette (left), it’s quite difficult to determine that our data shows two “spikes”, whereas this is easier to see with the lightness-modifying palette (right). Varying the lightness helps to see the structure of the data since changes in lightness are more intuitively processed as quantitative changes.\n\n\n\n\n\n\nFigure 1.6: Hue-based palette (left) vs lightness-modifying palette (right)\n\n\n\nOne detail that we should note is that the graph on the right of Figure 1.6 does not change only the lightness, it is not a map in gray or blue scales. That palette also changes the hue but in a very subtle way. This makes it aesthetically more pleasing and the subtle variation in hue contributes to increasing the perceptual distance between two values and therefore the ability to distinguish small differences.\nWhen using colours to represent numerical variables it is important to use uniformly perceptual maps like those offered by matplotlib or colorcet. These are maps where the colours vary in such a way that they adequately reflect changes in the data. Not all colormaps are perceptually uniform. Obtaining them is not trivial. Figure 1.7 shows the same image using different colormaps. We can see that widely used maps such as jet (also called rainbow) generate distortions in the image. In contrast viridis, a perceptually uniform color map does not generate such distortions (more details here).\n\n\n\n\n\n\nFigure 1.7: non-uniformly perceptual maps like jet can be very misleading\n\n\n\nA common criticism of perceptually smooth maps is that they appear more “flat” or “boring” at first glance. And instead maps like Jet, show greater contrast. But that is precisely one of the problems with maps like Jet, the magnitude of these contrasts does not correlate with changes in the data, so even extremes can occur, such as showing contrasts that are not there and hiding differences that are truly there.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of Visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#style-sheets",
    "href": "Chapters/Elements_of_visualization.html#style-sheets",
    "title": "1  Elements of Visualization",
    "section": "1.5 Style sheets",
    "text": "1.5 Style sheets\nMatplotlib allows users to easily switch between plotting styles by defining style sheets. ArviZ is delivered with a few additional styles that can be applied globally by writing az.style.use(name_of_style) or inside a with statement.\n\nvariatcetrinotenuitummavibrant\n\n\n\n\nCode\nazp.style.use('arviz-variat')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure(figsize=(4.5, 2.5))\nfor i in range(8):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\n\nFigure 1.8: arviz-variat style use a color-blind friendly palette\n\n\n\n\n\n\n\n\n\nCode\nazp.style.use('arviz-cetrino')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure(figsize=(4.5, 2.5))\nfor i in range(8):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\n\nFigure 1.9: arviz-cetrino style use a color-blind friendly palette\n\n\n\n\n\n\n\n\n\nCode\nazp.style.use('arviz-tenui')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure(figsize=(4.5, 2.5))\nfor i in range(8):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\n\nFigure 1.10: arviz-tenui style use a color-blind friendly palette\n\n\n\n\n\n\n\n\n\nCode\nazp.style.use('arviz-tumma')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure(figsize=(4.5, 2.5))\nfor i in range(8):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\n\nFigure 1.11: arviz-tumma style use a color-blind friendly palette\n\n\n\n\n\n\n\n\n\nCode\nazp.style.use('arviz-vibrant')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure(figsize=(4.5, 2.5))\nfor i in range(8):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\n\nFigure 1.12: arviz-vibrant style use a color-blind friendly palette\n\n\n\n\n\n\n\n\nThe color palettes in ArviZ were designed with the help of colorcyclepicker to minimize the risk of confusion for people with color vision deficiencies. We also follow the recommendation from Wilke (2019) and restrict the color palette to a maximum of 8 colors. If you need to use many colors it’s recommended to explore alternatives like using different line styles, different markers, faceting or direct labelling.\nIf you need to create figures in grayscale you can use the first 3 colours of the arviz-vibrant pallete and then convert to grayscale. But remember to check that the result looks goods after the conversion.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDowney, Allen B. 2025. Think Stats: Exploratory Data Analysis. Sebastopol: O’Reilly Media. https://allendowney.github.io/ThinkStats/.\n\n\nHealy, Kieran. 2019. Data Visualization: A Practical Introduction. Princeton, New Jersey ; Oxford, Oxfordshire: Princeton University Press. https://kieranhealy.org/publications/dataviz/.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. CHI ’10. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1753326.1753357.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. 1 edition. Pearson.\n\n\nUnwin, Antony. 2024. Getting (More Out of) Graphics: Practice and Principles of Data Visualisation. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003131212.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media. https://clauswilke.com/dataviz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of Visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html",
    "href": "Chapters/DataTree.html",
    "title": "2  Working with DataTree",
    "section": "",
    "text": "2.0.1 Get the dataset corresponding to a single group\nDuring a modern Bayesian analysis we usually generate many sets of data including posterior samples, prior/posterior predictive samples, statistics generated by the sampling method, etc. To keep all this data tidy and avoid confusion ArviZ relies on the data-structures provided by xarray (Hoyer and Hamman 2017). If you are not familiar with xarray this chapter introduces some basic elements in the context of Bayesian stats. For a deeper understanding we recommend that you check their xarray’s documentation, you may find xarray useful for problems outside Bayesian analysis.\nWe need to become familiar with 3 Data Structures:\nThe best way to understand these data-structures is to explore them. In a real scenario DataTree objects will be generated by probabilistic programming languages (ppls), or ArviZ will be used to transform the output of ppls into DataTree. But we don’t need to fit a model to play with DataTree objects, ArviZ comes equipped with a few DataTree objects. Let’s start by loading the centered_eight DataTree.\nIn the context of Bayesian Stats a DataTree has groups like posterior, observed_data, posterior_predictive, log_likelihood, etc.\nDataTree/DataSet/DataArray objects have a nice representation in Jupyter notebooks, you can see the content of the dt object by just writing its name in a cell.\nThis is an HTML representation of a DataTree, so if you are reading this from a browser you should be able to interact with it. If you click on Groups, all the groups will be expanded.\nAn important concept is that of dimensions and coordinates. If your data was geographical data, like things related to maps, then dimensions would be like latitude and longitude, and coordinates would be the actual values of latitude and longitude. The xarray documentation is full of examples related to maps. But the idea is very general and applies to any kind of data.\nLet’s see the dimensions and coordinates for the posterior in our dt object. We can see 3 dimensions chain, draw, and school. As usual the posterior group of a DataTree object will be generated from an MCMC sampler. The chain dimension is used to index the different chains of the MCMC sampler, the draw dimension is used to index the different samples generated by the MCMC sampler. The chain and draw dimensions are ubiquitous, you will see them essentially in any DataTree object when working with ArviZ. The coordinates for chain are the integers [0, 1, 2, 3] and for draw are the integers [0, 1, 2, ..., 499]. Then, we also have the school dimension. This problem-specific, we have it here because the model from which the posterior was generated has a parameter conditional on school. The coordinates for school are the names of the schools, if you click on the  symbol by the school coordinate, you will be able to see the names of each school. They are: ['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter', 'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon']\nWe can access each group using a dictionary-like notation:\ndt[\"posterior\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataTree 'posterior'&gt;\nGroup: /posterior\n    Dimensions:  (chain: 4, draw: 500, school: 8)\n    Coordinates:\n      * chain    (chain) int64 32B 0 1 2 3\n      * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n      * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n    Data variables:\n        mu       (chain, draw) float64 16kB 1.716 1.903 1.903 ... 5.409 7.721 10.24\n        theta    (chain, draw, school) float64 128kB 2.317 1.45 ... 14.92 14.02\n        tau      (chain, draw) float64 16kB 0.8775 0.8027 0.8027 ... 2.99 3.052\n    Attributes: (6)xarray.DataTree'posterior'Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float641.716 1.903 1.903 ... 7.721 10.24array([[ 1.71572331,  1.90348113,  1.90348113, ...,  6.80504593,\n         5.36844617,  6.93340474],\n       [-1.61229304,  3.54456311,  3.43239461, ..., -2.25857534,\n         2.49923447,  4.12738384],\n       [ 9.45345034,  2.41281038,  3.9574903 , ...,  7.88657889,\n         5.51847519,  7.04122405],\n       [-1.22591242,  1.54915161,  0.60776222, ...,  5.40883573,\n         7.72143998, 10.23715678]], shape=(4, 500))theta(chain, draw, school)float642.317 1.45 2.086 ... 14.92 14.02array([[[  2.31739097,   1.45017375,   2.08554978, ...,   2.712972  ,\n           3.08376362,   1.46044828],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        ...,\n        [ 13.45620242,  15.41284151,   3.18021529, ...,   9.1504244 ,\n          10.89570091,   8.78010934],\n        [ 13.01074853,   2.04893139,  22.49301396, ...,  10.42960666,\n           5.62905271,  12.18663532],\n        [ -3.53384061,   6.09061694,  -9.87347695, ...,   0.74308358,\n          15.34509814,   9.61065645]],\n\n       [[  7.78676911,  -5.94397836,  -6.27914573, ...,  -6.42590905,\n           5.85366982,   8.28407247],\n        [  2.05025159,  11.56639495,  11.75480422, ...,   9.72646376,\n           5.20422231,  -0.10662151],\n        [  7.01759107,  11.95371561,  13.50082917, ...,   5.3979042 ,\n           5.73876592,  -5.64141763],\n...\n        [  4.96289912,  10.95509264,   9.57022027, ...,  14.12152223,\n          10.81136422,   1.80438853],\n        [  9.0931749 ,  11.40969674,  10.32896321, ...,  10.39389155,\n           7.03775798,   4.59510911],\n        [  9.25814554,  12.06169304,  11.1609667 , ...,  11.25333602,\n           7.08131602,   6.19248663]],\n\n       [[  2.88627921,   2.23466779,   0.47189951, ...,   2.6996961 ,\n          -5.88574408,  -7.09217484],\n        [  0.64364262,   4.43423044,  -3.28831784, ...,  -4.16933152,\n           8.47142025,   3.14086796],\n        [  6.80468094,  -5.54836088,   0.83352477, ...,   0.77659959,\n           0.39565091,   2.48932359],\n        ...,\n        [ 11.40639049,   4.44693711,   9.21077455, ...,   4.81230178,\n           9.69325672,   4.91465554],\n        [  7.08613879,  12.31188923,   6.5843008 , ...,  11.85993774,\n           7.95226848,   9.75446752],\n        [ 10.46439019,  13.71430563,  10.26166615, ...,  15.07090015,\n          14.92321025,  14.02312893]]], shape=(4, 500, 8))tau(chain, draw)float640.8775 0.8027 0.8027 ... 2.99 3.052array([[ 0.8774941 ,  0.80271439,  0.80271439, ...,  5.08600601,\n        10.11493858,  8.07920452],\n       [12.24115337,  5.47983028,  6.64791029, ...,  7.92542651,\n         6.28841169,  6.79958791],\n       [ 6.4434113 ,  4.12594277,  3.11972735, ...,  4.77141179,\n         5.46401958,  4.77263378],\n       [ 5.50797127,  2.86390738,  4.18963173, ...,  2.23648602,\n         2.98965551,  3.05155936]], shape=(4, 500))Attributes: (6)created_at :2025-01-19T14:32:33.071271+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0sampling_time :3.159093141555786tuning_steps :1000\nAlternatively, we can use the dot notation, as groups are attributes of the DataTree. For instance, to access the posterior group we can write:\ndt.posterior;\nThe dot notation works at the group level and for DataSets and DataArrays as long as there is no conflict with a method or attribute of these objects. If there is a conflict, you can always use the dictionary-like notation.\nNotice that we still get a DataTree, but with 0 groups. If you want the DataSet you can do.\ndt[\"posterior\"].to_dataset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 165kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 16kB 1.716 1.903 1.903 ... 5.409 7.721 10.24\n    theta    (chain, draw, school) float64 128kB 2.317 1.45 ... 14.92 14.02\n    tau      (chain, draw) float64 16kB 0.8775 0.8027 0.8027 ... 2.99 3.052\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float641.716 1.903 1.903 ... 7.721 10.24array([[ 1.71572331,  1.90348113,  1.90348113, ...,  6.80504593,\n         5.36844617,  6.93340474],\n       [-1.61229304,  3.54456311,  3.43239461, ..., -2.25857534,\n         2.49923447,  4.12738384],\n       [ 9.45345034,  2.41281038,  3.9574903 , ...,  7.88657889,\n         5.51847519,  7.04122405],\n       [-1.22591242,  1.54915161,  0.60776222, ...,  5.40883573,\n         7.72143998, 10.23715678]], shape=(4, 500))theta(chain, draw, school)float642.317 1.45 2.086 ... 14.92 14.02array([[[  2.31739097,   1.45017375,   2.08554978, ...,   2.712972  ,\n           3.08376362,   1.46044828],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        ...,\n        [ 13.45620242,  15.41284151,   3.18021529, ...,   9.1504244 ,\n          10.89570091,   8.78010934],\n        [ 13.01074853,   2.04893139,  22.49301396, ...,  10.42960666,\n           5.62905271,  12.18663532],\n        [ -3.53384061,   6.09061694,  -9.87347695, ...,   0.74308358,\n          15.34509814,   9.61065645]],\n\n       [[  7.78676911,  -5.94397836,  -6.27914573, ...,  -6.42590905,\n           5.85366982,   8.28407247],\n        [  2.05025159,  11.56639495,  11.75480422, ...,   9.72646376,\n           5.20422231,  -0.10662151],\n        [  7.01759107,  11.95371561,  13.50082917, ...,   5.3979042 ,\n           5.73876592,  -5.64141763],\n...\n        [  4.96289912,  10.95509264,   9.57022027, ...,  14.12152223,\n          10.81136422,   1.80438853],\n        [  9.0931749 ,  11.40969674,  10.32896321, ...,  10.39389155,\n           7.03775798,   4.59510911],\n        [  9.25814554,  12.06169304,  11.1609667 , ...,  11.25333602,\n           7.08131602,   6.19248663]],\n\n       [[  2.88627921,   2.23466779,   0.47189951, ...,   2.6996961 ,\n          -5.88574408,  -7.09217484],\n        [  0.64364262,   4.43423044,  -3.28831784, ...,  -4.16933152,\n           8.47142025,   3.14086796],\n        [  6.80468094,  -5.54836088,   0.83352477, ...,   0.77659959,\n           0.39565091,   2.48932359],\n        ...,\n        [ 11.40639049,   4.44693711,   9.21077455, ...,   4.81230178,\n           9.69325672,   4.91465554],\n        [  7.08613879,  12.31188923,   6.5843008 , ...,  11.85993774,\n           7.95226848,   9.75446752],\n        [ 10.46439019,  13.71430563,  10.26166615, ...,  15.07090015,\n          14.92321025,  14.02312893]]], shape=(4, 500, 8))tau(chain, draw)float640.8775 0.8027 0.8027 ... 2.99 3.052array([[ 0.8774941 ,  0.80271439,  0.80271439, ...,  5.08600601,\n        10.11493858,  8.07920452],\n       [12.24115337,  5.47983028,  6.64791029, ...,  7.92542651,\n         6.28841169,  6.79958791],\n       [ 6.4434113 ,  4.12594277,  3.11972735, ...,  4.77141179,\n         5.46401958,  4.77263378],\n       [ 5.50797127,  2.86390738,  4.18963173, ...,  2.23648602,\n         2.98965551,  3.05155936]], shape=(4, 500))Attributes: (6)created_at :2025-01-19T14:32:33.071271+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0sampling_time :3.159093141555786tuning_steps :1000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#ploting",
    "href": "Chapters/DataTree.html#ploting",
    "title": "2  Working with DataTree",
    "section": "2.1 Ploting",
    "text": "2.1 Ploting\nXarray has some plotting capabilities, for instance, we can do:\n\ndt.posterior[\"mu\"].plot.hist(figsize=(9, 3));\n\n\n\n\n\n\n\n\nBut in most scenarios calling a plotting function from ArviZ and passing the DataTree as an argument will be a much better idea.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#add-a-new-variable",
    "href": "Chapters/DataTree.html#add-a-new-variable",
    "title": "2  Working with DataTree",
    "section": "2.2 Add a new variable",
    "text": "2.2 Add a new variable\nWe can add variables to existing groups. For instance, we may want to transform a parameter from the posterior. Like computing and adding the \\(\\log\\) of the parameter \\(\\tau\\) to the posterior group.\n\nposterior[\"log_tau\"] = np.log(posterior[\"tau\"])\nposterior\n\n/tmp/ipykernel_2718/1785098962.py:1: DeprecationWarning: Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. Please also drop the following variables: {'sample'} to avoid an error in the future.\n  posterior[\"log_tau\"] = np.log(posterior[\"tau\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:  (sample: 100, school: 8)\nCoordinates:\n  * sample   (sample) object 800B MultiIndex\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * chain    (sample) int64 800B 3 1 0 2 1 0 2 1 3 1 3 ... 1 3 0 3 2 3 0 1 0 2 1\n  * draw     (sample) int64 800B 80 443 428 346 215 341 ... 424 169 116 493 477\nData variables:\n    mu       (sample) float64 800B 1.769 1.749 2.423 7.356 ... 5.487 7.236 7.666\n    theta    (school, sample) float64 6kB 0.756 8.756 9.273 ... 5.49 14.74 9.631\n    tau      (sample) float64 800B 1.586 3.391 3.355 4.597 ... 1.298 8.146 2.599\n    log_tau  (sample) float64 800B 0.4612 1.221 1.211 ... 0.2605 2.098 0.955\nAttributes: (6)xarray.DatasetDimensions:sample: 100school: 8Coordinates: (4)sample(sample)objectMultiIndex[100 values with dtype=object]school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')chain(sample)int643 1 0 2 1 0 2 1 ... 3 2 3 0 1 0 2 1[100 values with dtype=int64]draw(sample)int6480 443 428 346 ... 169 116 493 477[100 values with dtype=int64]Data variables: (4)mu(sample)float641.769 1.749 2.423 ... 7.236 7.666array([ 1.76908953,  1.74941235,  2.42316116,  7.35586767,  5.59635735,\n        8.65723301,  1.3923193 ,  0.58742843, -0.14053877,  3.67178189,\n        7.23640503,  2.25269353,  6.50565442,  1.13617748, -1.22752549,\n        4.55607029,  2.33994608,  5.63386095, -0.09858765,  6.49653552,\n        4.0937781 ,  2.08101033,  6.4696694 ,  5.02362916,  1.60737948,\n        4.83591405,  4.58138712,  2.14626524,  2.75829293,  9.32709336,\n        4.66699411,  5.66061903,  1.18299553,  1.60263511,  4.01878854,\n        7.54943726,  7.85698569,  9.83090492,  9.13470747,  3.32441956,\n        7.6273376 ,  1.7623804 ,  0.59469248,  2.54760968,  7.67484558,\n        9.37980905, 10.02306344,  3.97588033,  3.13255029,  0.47750471,\n        1.23220729,  3.9166985 ,  7.3392849 , -1.28492081,  3.50981912,\n        2.86365131,  2.30055489,  4.2021031 , 11.37598606,  1.2172498 ,\n       11.07669846,  3.58799455, 14.31027975,  1.5053629 ,  5.42429713,\n        2.20244217,  2.32856157,  6.99313769,  5.5300193 ,  0.96034525,\n        1.30461063,  8.52851981,  8.09751218,  5.85602095,  3.22923726,\n        0.14294513,  4.97931461,  3.35312919,  8.40794715,  4.74157006,\n       11.1759265 ,  2.69544302, -1.03086887,  4.13381158,  0.95808847,\n        4.82856048, 10.25694361,  5.70521392,  5.47053786,  1.65159337,\n        5.51582416,  7.25895488,  7.934659  ,  4.0631143 ,  3.95285052,\n       -2.19552525,  1.28501541,  5.48741113,  7.23570884,  7.66626588])theta(school, sample)float640.756 8.756 9.273 ... 14.74 9.631array([[ 7.55980024e-01,  8.75627692e+00,  9.27319724e+00,\n         4.21495547e+00,  8.10166738e+00,  3.53879300e+01,\n         8.07446352e-01,  1.52398923e-02,  5.81228699e+00,\n         1.28943883e+00,  1.47691487e+01,  9.24244058e+00,\n         3.33205468e+00,  3.61138251e+00,  1.54798751e+00,\n         1.53748735e+01,  9.37453968e+00,  5.13734587e+00,\n         3.64231053e+00,  8.27143194e+00,  1.06297899e+01,\n        -7.95927409e+00,  4.95300371e+00,  7.21379563e+00,\n         2.74721947e+01,  4.71103187e+00,  3.47343288e+00,\n         1.53689908e+00,  7.08120927e+00,  1.68493534e+01,\n         5.88693731e+00,  1.62074103e+01,  7.36337531e-01,\n        -9.00486551e-01,  1.25186834e+01,  1.64180489e+01,\n         1.25945319e+01,  1.67929368e+01,  9.08684725e+00,\n         1.13789124e+01,  1.04281234e+01,  3.69825964e+00,\n         2.81500627e+01,  1.17835083e+00,  4.37139438e+00,\n         1.40321298e+01,  1.27432050e+01,  1.21753833e+01,\n        -1.52028323e-01,  9.36231705e+00,  2.91156942e+01,\n         8.05362431e+00,  7.89297346e+00,  5.60467156e+00,\n         1.40249524e-01,  6.06725954e+00,  9.99840449e+00,\n         4.71273898e+00,  8.91268325e+00,  1.76217592e+01,\n...\n         3.13779386e+00,  3.95204737e+00,  1.62631314e+00,\n         1.07406278e+01,  1.15657109e+01,  2.17062013e+00,\n         3.24034425e+00,  4.41333980e-01,  1.13905265e+01,\n         2.14625444e-01,  1.19962956e+01,  5.42796037e+00,\n         5.80592416e+00,  3.03240922e+00, -1.37519538e+01,\n         5.95172754e+00,  1.42444188e+01, -1.11177121e+00,\n         9.24735829e+00,  2.06616143e+00,  2.11497745e+01,\n        -3.91023558e+00,  5.04713220e+00,  5.39605502e+00,\n         5.06575823e-01,  2.19169988e+01,  5.34572480e+00,\n         8.86704899e-02,  1.23999755e+00,  1.19718410e+01,\n         1.26073048e+01,  3.30317494e+00,  1.04613816e+01,\n         4.38658680e+00,  6.01870624e+00,  2.87939902e-01,\n         8.16665049e+00,  9.44021684e-01,  1.01012008e+01,\n         1.53908154e+00, -1.82757286e+00,  6.62044373e+00,\n        -9.67142249e-01,  7.46528656e+00,  1.01382364e+01,\n         7.02230204e+00,  4.74629995e+00,  2.60910968e+00,\n         1.59269425e+01,  5.28223146e+00,  8.46350808e+00,\n         5.24521889e+00, -3.83952606e+00, -7.54912513e+00,\n        -4.62283310e+00,  5.48961086e+00,  1.47378459e+01,\n         9.63051222e+00]])tau(sample)float641.586 3.391 3.355 ... 8.146 2.599array([ 1.58592937,  3.39102612,  3.35522746,  4.59671716,  2.01776408,\n       12.6700529 ,  3.42342529,  1.75672827,  8.39667615,  4.73306552,\n        4.78927457,  4.33988172,  5.97256612,  5.34705923,  1.97930619,\n        6.73720342,  5.04379219,  1.23610017,  1.96876713,  1.38645317,\n        4.55025758,  3.76640617,  1.94320486,  2.57169562,  8.14001312,\n        4.96704739,  4.34109204,  2.25811987,  2.40790429,  4.09832409,\n        2.98317974,  5.17913326,  1.67166494,  5.5639664 ,  5.15031305,\n       12.29891757,  4.05020515,  6.12866576,  1.42603648,  4.14862605,\n        1.43506229,  2.38392544,  8.49974286,  1.5331241 ,  8.24089986,\n        9.48281125,  1.41184474,  5.73584255,  3.94643805,  3.61823559,\n       17.50980935,  2.79424286,  5.98771727,  3.43403485,  3.39204566,\n        2.75100958,  9.55337583,  0.97061676,  3.07949319,  6.77917674,\n        3.9122227 ,  6.03927764,  8.97045988,  3.93742667,  4.43582205,\n        4.98601389,  2.17552135,  7.15023149,  2.04916998,  2.39866682,\n        5.16598965,  5.58320757,  2.61784754,  3.12308569,  5.8832904 ,\n        4.81194286,  2.56661146,  2.85001766,  2.26484811,  2.87660659,\n        3.99829314,  1.74062627,  1.21914532,  4.60006791,  3.06281359,\n        2.71385783,  2.23679946,  2.91416135,  3.21516035,  1.55180647,\n        7.78390569,  1.91900475,  2.44709692,  1.89637297, 11.92903187,\n        6.37865694,  6.22193319,  1.29753741,  8.14613061,  2.59854714])log_tau(sample)float640.4612 1.221 1.211 ... 2.098 0.955array([ 0.46117059,  1.22113257,  1.21051957,  1.52534239,  0.70199001,\n        2.53924117,  1.2306416 ,  0.56345314,  2.12783593,  1.55457309,\n        1.56637895,  1.4678471 ,  1.78717667,  1.67654673,  0.68274637,\n        1.90764492,  1.61815822,  0.2119614 ,  0.67740753,  0.32674881,\n        1.51518384,  1.32612128,  0.6643386 ,  0.94456545,  2.09679179,\n        1.60282558,  1.46812594,  0.81453255,  0.87875678,  1.41057813,\n        1.09298976,  1.64463772,  0.5138201 ,  1.71631124,  1.6390575 ,\n        2.50951126,  1.39876754,  1.81297707,  0.3548989 ,  1.42277721,\n        0.36120825,  0.86874847,  2.14003591,  0.42730755,  2.10910954,\n        2.24948082,  0.34489718,  1.74673465,  1.37281341,  1.2859865 ,\n        2.86276126,  1.02756118,  1.78971025,  1.23373591,  1.22143318,\n        1.01196797,  2.25689458, -0.02982357,  1.12476504,  1.91385567,\n        1.36410568,  1.79828441,  2.19393694,  1.37052738,  1.48971295,\n        1.60663677,  0.77726834,  1.96714473,  0.71743482,  0.87491309,\n        1.64209669,  1.71976344,  0.96235243,  1.13882152,  1.7721162 ,\n        1.57110092,  0.94258653,  1.04732519,  0.8175077 ,  1.05661133,\n        1.38586756,  0.55424498,  0.19815006,  1.52607107,  1.11933397,\n        0.99837117,  0.80504603,  1.06958208,  1.16787723,  0.43941972,\n        2.05205823,  0.65180669,  0.89490239,  0.6399431 ,  2.47897508,\n        1.85295756,  1.82808066,  0.26046817,  2.09754304,  0.95495249])Attributes: (6)created_at :2025-01-19T14:32:33.071271+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0sampling_time :3.159093141555786tuning_steps :1000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#advance-operations-with-datatrees",
    "href": "Chapters/DataTree.html#advance-operations-with-datatrees",
    "title": "2  Working with DataTree",
    "section": "2.3 Advance operations with DataTrees",
    "text": "2.3 Advance operations with DataTrees\nNow we delve into more advanced operations with DataTree. While these operations are not essential to use ArviZ, they can be useful in some cases. Exploring these advanced functionalities will help you become more familiar with DataTree and provide additional insights that may enhance your overall experience with ArviZ.\n\n2.3.1 Compute and store posterior pushforward quantities\nWe use “posterior push-forward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables.\nYou can use xarray for these push-forward operations and store them as a new variable in the posterior group. You’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them (like mcse), or save and share the DataTree object with the push forward quantities included.\nThe first thing we are going to do is to store the posterior group in a variable called post to make the code more readable. And to compute the log of \\(\\tau\\).\n\npost = dt.posterior\npost[\"log_tau\"] = np.log(post[\"tau\"])\n\nCompute the rolling mean of \\(\\log(\\tau)\\) with xarray.DataArray.rolling, storing the result in the posterior:\n\npost[\"mlogtau\"] = post[\"log_tau\"].rolling({\"draw\": 50}).mean()\n\nUsing xarray for push-forward calculations has all the advantages of working with xarray. It also inherits the disadvantages of working with xarray, but we believe those to be outweighed by the advantages, and we have already shown how to extract the data as NumPy arrays.\nSome examples of these advantages are specifying operations with named dimensions instead of positional ones (as seen in some previous sections), automatic alignment and broadcasting of arrays (as we’ll see now), or integration with Dask (as shown in the dask_for_arviz guide).\nIn this cell, you will compute pairwise differences between schools on their mean effects (variable theta). To do so, subtract the variable theta after renaming the school dimension to the original variable. Xarray then aligns and broadcasts the two variables because they have different dimensions, and the result is a 4D variable with all the pointwise differences.\nEventually, store the result in the theta_school_diff variable. Notice that the theta_school_diff variable in the posterior has kept the named dimensions and coordinates:\n\npost[\"theta_school_diff\"] = post.theta - post.theta.rename(school=\"school_bis\")\npost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataTree 'posterior'&gt;\nGroup: /posterior\n    Dimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\n    Coordinates:\n      * chain              (chain) int64 32B 0 1 2 3\n      * draw               (draw) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n      * school             (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n      * school_bis         (school_bis) &lt;U16 512B 'Choate' ... 'Mt. Hermon'\n    Data variables:\n        mu                 (chain, draw) float64 16kB 1.716 1.903 ... 7.721 10.24\n        theta              (chain, draw, school) float64 128kB 2.317 1.45 ... 14.02\n        tau                (chain, draw) float64 16kB 0.8775 0.8027 ... 2.99 3.052\n        log_tau            (chain, draw) float64 16kB -0.1307 -0.2198 ... 1.116\n        mlogtau            (chain, draw) float64 16kB nan nan nan ... 1.335 1.335\n        theta_school_diff  (chain, draw, school, school_bis) float64 1MB 0.0 ... 0.0\n    Attributes: (6)xarray.DataTree'posterior'Dimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (6)mu(chain, draw)float641.716 1.903 1.903 ... 7.721 10.24array([[ 1.71572331,  1.90348113,  1.90348113, ...,  6.80504593,\n         5.36844617,  6.93340474],\n       [-1.61229304,  3.54456311,  3.43239461, ..., -2.25857534,\n         2.49923447,  4.12738384],\n       [ 9.45345034,  2.41281038,  3.9574903 , ...,  7.88657889,\n         5.51847519,  7.04122405],\n       [-1.22591242,  1.54915161,  0.60776222, ...,  5.40883573,\n         7.72143998, 10.23715678]], shape=(4, 500))theta(chain, draw, school)float642.317 1.45 2.086 ... 14.92 14.02array([[[  2.31739097,   1.45017375,   2.08554978, ...,   2.712972  ,\n           3.08376362,   1.46044828],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        [  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,\n           2.48750253,   1.98437916],\n        ...,\n        [ 13.45620242,  15.41284151,   3.18021529, ...,   9.1504244 ,\n          10.89570091,   8.78010934],\n        [ 13.01074853,   2.04893139,  22.49301396, ...,  10.42960666,\n           5.62905271,  12.18663532],\n        [ -3.53384061,   6.09061694,  -9.87347695, ...,   0.74308358,\n          15.34509814,   9.61065645]],\n\n       [[  7.78676911,  -5.94397836,  -6.27914573, ...,  -6.42590905,\n           5.85366982,   8.28407247],\n        [  2.05025159,  11.56639495,  11.75480422, ...,   9.72646376,\n           5.20422231,  -0.10662151],\n        [  7.01759107,  11.95371561,  13.50082917, ...,   5.3979042 ,\n           5.73876592,  -5.64141763],\n...\n        [  4.96289912,  10.95509264,   9.57022027, ...,  14.12152223,\n          10.81136422,   1.80438853],\n        [  9.0931749 ,  11.40969674,  10.32896321, ...,  10.39389155,\n           7.03775798,   4.59510911],\n        [  9.25814554,  12.06169304,  11.1609667 , ...,  11.25333602,\n           7.08131602,   6.19248663]],\n\n       [[  2.88627921,   2.23466779,   0.47189951, ...,   2.6996961 ,\n          -5.88574408,  -7.09217484],\n        [  0.64364262,   4.43423044,  -3.28831784, ...,  -4.16933152,\n           8.47142025,   3.14086796],\n        [  6.80468094,  -5.54836088,   0.83352477, ...,   0.77659959,\n           0.39565091,   2.48932359],\n        ...,\n        [ 11.40639049,   4.44693711,   9.21077455, ...,   4.81230178,\n           9.69325672,   4.91465554],\n        [  7.08613879,  12.31188923,   6.5843008 , ...,  11.85993774,\n           7.95226848,   9.75446752],\n        [ 10.46439019,  13.71430563,  10.26166615, ...,  15.07090015,\n          14.92321025,  14.02312893]]], shape=(4, 500, 8))tau(chain, draw)float640.8775 0.8027 0.8027 ... 2.99 3.052array([[ 0.8774941 ,  0.80271439,  0.80271439, ...,  5.08600601,\n        10.11493858,  8.07920452],\n       [12.24115337,  5.47983028,  6.64791029, ...,  7.92542651,\n         6.28841169,  6.79958791],\n       [ 6.4434113 ,  4.12594277,  3.11972735, ...,  4.77141179,\n         5.46401958,  4.77263378],\n       [ 5.50797127,  2.86390738,  4.18963173, ...,  2.23648602,\n         2.98965551,  3.05155936]], shape=(4, 500))log_tau(chain, draw)float64-0.1307 -0.2198 ... 1.095 1.116array([[-0.13068505, -0.2197563 , -0.2197563 , ...,  1.62649285,\n         2.3140134 ,  2.08929342],\n       [ 2.5048035 ,  1.70107413,  1.89430256, ...,  2.07007614,\n         1.83870853,  1.91686201],\n       [ 1.86305811,  1.41729454,  1.13774561, ...,  1.56264223,\n         1.69818471,  1.56289831],\n       [ 1.70619637,  1.05218691,  1.43261284, ...,  0.80490589,\n         1.09515817,  1.11565273]], shape=(4, 500))mlogtau(chain, draw)float64nan nan nan ... 1.332 1.335 1.335array([[       nan,        nan,        nan, ..., 1.61215551, 1.64001175,\n        1.66914816],\n       [       nan,        nan,        nan, ..., 1.31088051, 1.31352885,\n        1.32441238],\n       [       nan,        nan,        nan, ..., 1.50889741, 1.52656086,\n        1.53913392],\n       [       nan,        nan,        nan, ..., 1.33207961, 1.33509146,\n        1.33454261]], shape=(4, 500))theta_school_diff(chain, draw, school, school_bis)float640.0 0.8672 0.2318 ... -0.9001 0.0array([[[[ 0.00000000e+00,  8.67217220e-01,  2.31841190e-01, ...,\n          -3.95581030e-01, -7.66372647e-01,  8.56942694e-01],\n         [-8.67217220e-01,  0.00000000e+00, -6.35376031e-01, ...,\n          -1.26279825e+00, -1.63358987e+00, -1.02745264e-02],\n         [-2.31841190e-01,  6.35376031e-01,  0.00000000e+00, ...,\n          -6.27422220e-01, -9.98213837e-01,  6.25101504e-01],\n         ...,\n         [ 3.95581030e-01,  1.26279825e+00,  6.27422220e-01, ...,\n           0.00000000e+00, -3.70791617e-01,  1.25252372e+00],\n         [ 7.66372647e-01,  1.63358987e+00,  9.98213837e-01, ...,\n           3.70791617e-01,  0.00000000e+00,  1.62331534e+00],\n         [-8.56942694e-01,  1.02745264e-02, -6.25101504e-01, ...,\n          -1.25252372e+00, -1.62331534e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  1.46221462e-01, -2.23669874e+00, ...,\n          -6.69769083e-01, -1.59833232e+00, -1.09520896e+00],\n         [-1.46221462e-01,  0.00000000e+00, -2.38292020e+00, ...,\n          -8.15990545e-01, -1.74455379e+00, -1.24143042e+00],\n         [ 2.23669874e+00,  2.38292020e+00,  0.00000000e+00, ...,\n           1.56692966e+00,  6.38366417e-01,  1.14148979e+00],\n...\n           0.00000000e+00,  3.90766926e+00,  2.10547022e+00],\n         [ 8.66129686e-01, -4.35962075e+00,  1.36796768e+00, ...,\n          -3.90766926e+00,  0.00000000e+00, -1.80219904e+00],\n         [ 2.66832873e+00, -2.55742171e+00,  3.17016672e+00, ...,\n          -2.10547022e+00,  1.80219904e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00, -3.24991544e+00,  2.02724044e-01, ...,\n          -4.60650996e+00, -4.45882006e+00, -3.55873874e+00],\n         [ 3.24991544e+00,  0.00000000e+00,  3.45263948e+00, ...,\n          -1.35659453e+00, -1.20890463e+00, -3.08823300e-01],\n         [-2.02724044e-01, -3.45263948e+00,  0.00000000e+00, ...,\n          -4.80923401e+00, -4.66154411e+00, -3.76146278e+00],\n         ...,\n         [ 4.60650996e+00,  1.35659453e+00,  4.80923401e+00, ...,\n           0.00000000e+00,  1.47689901e-01,  1.04777123e+00],\n         [ 4.45882006e+00,  1.20890463e+00,  4.66154411e+00, ...,\n          -1.47689901e-01,  0.00000000e+00,  9.00081326e-01],\n         [ 3.55873874e+00,  3.08823300e-01,  3.76146278e+00, ...,\n          -1.04777123e+00, -9.00081326e-01,  0.00000000e+00]]]],\n      shape=(4, 500, 8, 8))Attributes: (6)created_at :2025-01-19T14:32:33.071271+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0sampling_time :3.159093141555786tuning_steps :1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis same operation using NumPy would require manual alignment of the two arrays to make sure they broadcast correctly. The code could be something like:\ntheta_school_diff = theta[:, :, :, None] - theta[:, :, None, :]\n\n\n\n\n2.3.2 Advanced subsetting\nTo select the value corresponding to the difference between the Choate and Deerfield schools do:\n\npost[\"theta_school_diff\"].sel(school=\"Choate\", school_bis=\"Deerfield\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500)&gt; Size: 16kB\n0.8672 0.1462 0.1462 0.1462 0.2902 1.08 ... 5.899 -5.935 6.959 -5.226 -3.25\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n    school      &lt;U16 64B 'Choate'\n    school_bis  &lt;U16 64B 'Deerfield'xarray.DataArray'theta_school_diff'chain: 4draw: 5000.8672 0.1462 0.1462 0.1462 0.2902 ... 5.899 -5.935 6.959 -5.226 -3.25array([[  0.86721722,   0.14622146,   0.14622146, ...,  -1.95663908,\n         10.96181714,  -9.62445754],\n       [ 13.73074748,  -9.51614337,  -4.93612454, ...,  -3.9351428 ,\n        -10.80023681,  -9.78617497],\n       [  5.32847654,  -0.60356191,   2.41006387, ...,  -5.99219352,\n         -2.31652184,  -2.8035475 ],\n       [  0.65161141,  -3.79058782,  12.35304181, ...,   6.95945338,\n         -5.22575044,  -3.24991544]], shape=(4, 500))Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school()&lt;U16'Choate'array('Choate', dtype='&lt;U16')school_bis()&lt;U16'Deerfield'array('Deerfield', dtype='&lt;U16')\n\n\nFor more advanced subsetting (the equivalent to what is sometimes called “fancy indexing” in NumPy) you need to provide the indices as DataArray objects:\n\nschool_idx = xr.DataArray([\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"], dims=[\"pairwise_school_diff\"])\nschool_bis_idx = xr.DataArray(\n    [\"Deerfield\", \"Choate\", \"Lawrenceville\"], dims=[\"pairwise_school_diff\"]\n)\npost[\"theta_school_diff\"].sel(school=school_idx, school_bis=school_bis_idx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500,\n                                       pairwise_school_diff: 3)&gt; Size: 48kB\n0.8672 0.7541 -1.253 0.1462 1.946 0.4254 ... 2.964 -2.105 -3.25 0.4516 -1.048\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n    school      (pairwise_school_diff) &lt;U16 192B 'Choate' ... 'Mt. Hermon'\n    school_bis  (pairwise_school_diff) &lt;U16 192B 'Deerfield' ... 'Lawrenceville'\nDimensions without coordinates: pairwise_school_diffxarray.DataArray'theta_school_diff'chain: 4draw: 500pairwise_school_diff: 30.8672 0.7541 -1.253 0.1462 1.946 ... 2.964 -2.105 -3.25 0.4516 -1.048array([[[  0.86721722,   0.75411617,  -1.25252372],\n        [  0.14622146,   1.94553437,   0.42543987],\n        [  0.14622146,   1.94553437,   0.42543987],\n        ...,\n        [ -1.95663908,   1.70876245,  -0.37031506],\n        [ 10.96181714, -16.16926971,   1.75702866],\n        [ -9.62445754,  15.08629623,   8.86757287]],\n\n       [[ 13.73074748,  -3.62364239,  14.70998151],\n        [ -9.51614337,  -2.39672702,  -9.83308527],\n        [ -4.93612454,  -8.81665712, -11.03932183],\n        ...,\n        [ -3.9351428 ,   1.65585731,   0.29525286],\n        [-10.80023681,  -1.09872777,   4.05977227],\n        [ -9.78617497,  -5.75724627,   3.30319138]],\n\n       [[  5.32847654, -10.49176966,  18.12889564],\n        [ -0.60356191,   2.66116402, -13.09310346],\n        [  2.41006387,  -2.23454358,   7.7906261 ],\n        ...,\n        [ -5.99219352,  -3.27144338, -12.3171337 ],\n        [ -2.31652184,   1.02285356,  -5.79878244],\n        [ -2.8035475 ,  -0.10771826,  -5.06084939]],\n\n       [[  0.65161141,  -3.43191122,  -9.79187094],\n        [ -3.79058782,   1.03525582,   7.31019948],\n        [ 12.35304181, -10.28499886,   1.712724  ],\n        ...,\n        [  6.95945338,  -7.25561273,   0.10235377],\n        [ -5.22575044,   2.96402813,  -2.10547022],\n        [ -3.24991544,   0.4516401 ,  -1.04777123]]], shape=(4, 500, 3))Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(pairwise_school_diff)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(pairwise_school_diff)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')\n\n\nUsing lists or NumPy arrays instead of DataArrays does column/row-based indexing. As you can see, the result has 9 values of theta_school_diff instead of the 3 pairs of difference we selected in the previous cell:\n\npost[\"theta_school_diff\"].sel(\n    school=[\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"],\n    school_bis=[\"Deerfield\", \"Choate\", \"Lawrenceville\"],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500, school: 3,\n                                       school_bis: 3)&gt; Size: 144kB\n0.8672 0.0 -0.3956 1.621 0.7541 0.3585 ... 0.4516 -4.155 0.3088 3.559 -1.048\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school      (school) &lt;U16 192B 'Choate' 'Hotchkiss' 'Mt. Hermon'\n  * school_bis  (school_bis) &lt;U16 192B 'Deerfield' 'Choate' 'Lawrenceville'xarray.DataArray'theta_school_diff'chain: 4draw: 500school: 3school_bis: 30.8672 0.0 -0.3956 1.621 0.7541 ... 0.4516 -4.155 0.3088 3.559 -1.048array([[[[ 8.67217220e-01,  0.00000000e+00, -3.95581030e-01],\n         [ 1.62133339e+00,  7.54116172e-01,  3.58535141e-01],\n         [ 1.02745264e-02, -8.56942694e-01, -1.25252372e+00]],\n\n        [[ 1.46221462e-01,  0.00000000e+00, -6.69769083e-01],\n         [ 2.09175584e+00,  1.94553437e+00,  1.27576529e+00],\n         [ 1.24143042e+00,  1.09520896e+00,  4.25439874e-01]],\n\n        [[ 1.46221462e-01,  0.00000000e+00, -6.69769083e-01],\n         [ 2.09175584e+00,  1.94553437e+00,  1.27576529e+00],\n         [ 1.24143042e+00,  1.09520896e+00,  4.25439874e-01]],\n\n        ...,\n\n        [[-1.95663908e+00,  0.00000000e+00,  4.30577802e+00],\n         [-2.47876637e-01,  1.70876245e+00,  6.01454047e+00],\n         [-6.63273217e+00, -4.67609308e+00, -3.70315064e-01]],\n\n        [[ 1.09618171e+01,  0.00000000e+00,  2.58114188e+00],\n         [-5.20745257e+00, -1.61692697e+01, -1.35881278e+01],\n...\n         [-1.29336248e+00,  2.49722534e+00,  7.31019948e+00]],\n\n        [[ 1.23530418e+01,  0.00000000e+00,  6.02808134e+00],\n         [ 2.06804296e+00, -1.02849989e+01, -4.25691751e+00],\n         [ 8.03768447e+00, -4.31535735e+00,  1.71272400e+00]],\n\n        ...,\n\n        [[ 6.95945338e+00,  0.00000000e+00,  6.59408871e+00],\n         [-2.96159355e-01, -7.25561273e+00, -6.61524023e-01],\n         [ 4.67718433e-01, -6.49173494e+00,  1.02353766e-01]],\n\n        [[-5.22575044e+00,  0.00000000e+00, -4.77379895e+00],\n         [-2.26172231e+00,  2.96402813e+00, -1.80977082e+00],\n         [-2.55742171e+00,  2.66832873e+00, -2.10547022e+00]],\n\n        [[-3.24991544e+00,  0.00000000e+00, -4.60650996e+00],\n         [-2.79827534e+00,  4.51640099e-01, -4.15486987e+00],\n         [ 3.08823300e-01,  3.55873874e+00, -1.04777123e+00]]]],\n      shape=(4, 500, 3, 3))Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')\n\n\n\n\n2.3.3 Add new chains using concat\nAfter checking the mcse and realizing you need more samples, you rerun the model with two chains and obtain an dt_rerun object.\n\ndt_rerun = (\n    dt.posterior.sel(chain=[0, 1])\n    .copy()\n    .to_dataset()\n    .assign_coords(coords={\"chain\": [4, 5]})\n)\n\nYou can combine the two into a single DataTree object using the concat function from ArviZ:\n\ndt_complete = xr.concat([dt.posterior.to_dataset(), dt_rerun], dim=\"chain\")\ndt_complete.dims\n\nFrozenMappingWarningOnValuesAccess({'chain': 6, 'draw': 500, 'school': 8, 'school_bis': 8})\n\n\n\n\n2.3.4 Add groups to DataTrees\nThis will be simplified in the future, but for now, you can add groups to a DataTree by converting the DataTree to a dictionary, adding the new group, and then converting the dictionary back to a DataTree.\n\nrng = np.random.default_rng(3)\nds = azp.dict_to_dataset(\n    {\"obs\": rng.normal(size=(4, 500, 2))},\n    dims={\"obs\": [\"new_school\"]},\n    coords={\"new_school\": [\"Essex College\", \"Moordale\"]},\n)\ndicto = {k:v for k,v in dt.items()}\ndicto[\"predictions\"] = ds\n\nnew_dt = xr.DataTree.from_dict(dicto)\nnew_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataTree&gt;\nGroup: /\n├── Group: /posterior\n│       Dimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\n│       Coordinates:\n│         * chain              (chain) int64 32B 0 1 2 3\n│         * draw               (draw) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n│         * school             (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n│         * school_bis         (school_bis) &lt;U16 512B 'Choate' ... 'Mt. Hermon'\n│       Data variables:\n│           mu                 (chain, draw) float64 16kB 1.716 1.903 ... 7.721 10.24\n│           theta              (chain, draw, school) float64 128kB 2.317 1.45 ... 14.02\n│           tau                (chain, draw) float64 16kB 0.8775 0.8027 ... 2.99 3.052\n│           log_tau            (chain, draw) float64 16kB -0.1307 -0.2198 ... 1.116\n│           mlogtau            (chain, draw) float64 16kB nan nan nan ... 1.335 1.335\n│           theta_school_diff  (chain, draw, school, school_bis) float64 1MB 0.0 ... 0.0\n│       Attributes: (6)\n├── Group: /posterior_predictive\n│       Dimensions:  (chain: 4, draw: 500, school: 8)\n│       Coordinates:\n│         * chain    (chain) int64 32B 0 1 2 3\n│         * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n│         * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n│       Data variables:\n│           obs      (chain, draw, school) float64 128kB 38.88 -14.98 ... 27.05 20.99\n│       Attributes: (4)\n├── Group: /log_likelihood\n│       Dimensions:  (chain: 4, draw: 500, school: 8)\n│       Coordinates:\n│         * chain    (chain) int64 32B 0 1 2 3\n│         * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n│         * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n│       Data variables:\n│           obs      (chain, draw, school) float64 128kB -5.093 -3.436 ... -3.269 -3.816\n│       Attributes: (4)\n...\n├── Group: /observed_data\n│       Dimensions:  (school: 8)\n│       Coordinates:\n│         * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n│       Data variables:\n│           obs      (school) float64 64B 28.0 8.0 -3.0 7.0 -1.0 1.0 18.0 12.0\n│       Attributes: (4)\n├── Group: /constant_data\n│       Dimensions:  (school: 8)\n│       Coordinates:\n│         * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n│       Data variables:\n│           sigma    (school) float64 64B 15.0 10.0 16.0 11.0 9.0 11.0 10.0 18.0\n│       Attributes: (4)\n└── Group: /predictions\n        Dimensions:     (chain: 4, draw: 500, new_school: 2)\n        Coordinates:\n          * chain       (chain) int64 32B 0 1 2 3\n          * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n          * new_school  (new_school) &lt;U13 104B 'Essex College' 'Moordale'\n        Data variables:\n            obs         (chain, draw, new_school) float64 32kB 2.041 -2.556 ... -0.2822\n        Attributes: (4)xarray.DataTreeGroups: (9)(6/9)/posteriorDimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (6)mu(chain, draw)float641.716 1.903 1.903 ... 7.721 10.24array([[ 1.71572331,  1.90348113,  1.90348113, ...,  6.80504593,5.36844617,  6.93340474],[-1.61229304,  3.54456311,  3.43239461, ..., -2.25857534,2.49923447,  4.12738384],[ 9.45345034,  2.41281038,  3.9574903 , ...,  7.88657889,5.51847519,  7.04122405],[-1.22591242,  1.54915161,  0.60776222, ...,  5.40883573,7.72143998, 10.23715678]], shape=(4, 500))theta(chain, draw, school)float642.317 1.45 2.086 ... 14.92 14.02array([[[  2.31739097,   1.45017375,   2.08554978, ...,   2.712972  ,3.08376362,   1.46044828],[  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,2.48750253,   1.98437916],[  0.8891702 ,   0.74294874,   3.12586895, ...,   1.55893929,2.48750253,   1.98437916],...,[ 13.45620242,  15.41284151,   3.18021529, ...,   9.1504244 ,10.89570091,   8.78010934],[ 13.01074853,   2.04893139,  22.49301396, ...,  10.42960666,5.62905271,  12.18663532],[ -3.53384061,   6.09061694,  -9.87347695, ...,   0.74308358,15.34509814,   9.61065645]],[[  7.78676911,  -5.94397836,  -6.27914573, ...,  -6.42590905,5.85366982,   8.28407247],[  2.05025159,  11.56639495,  11.75480422, ...,   9.72646376,5.20422231,  -0.10662151],[  7.01759107,  11.95371561,  13.50082917, ...,   5.3979042 ,5.73876592,  -5.64141763],...[  4.96289912,  10.95509264,   9.57022027, ...,  14.12152223,10.81136422,   1.80438853],[  9.0931749 ,  11.40969674,  10.32896321, ...,  10.39389155,7.03775798,   4.59510911],[  9.25814554,  12.06169304,  11.1609667 , ...,  11.25333602,7.08131602,   6.19248663]],[[  2.88627921,   2.23466779,   0.47189951, ...,   2.6996961 ,-5.88574408,  -7.09217484],[  0.64364262,   4.43423044,  -3.28831784, ...,  -4.16933152,8.47142025,   3.14086796],[  6.80468094,  -5.54836088,   0.83352477, ...,   0.77659959,0.39565091,   2.48932359],...,[ 11.40639049,   4.44693711,   9.21077455, ...,   4.81230178,9.69325672,   4.91465554],[  7.08613879,  12.31188923,   6.5843008 , ...,  11.85993774,7.95226848,   9.75446752],[ 10.46439019,  13.71430563,  10.26166615, ...,  15.07090015,14.92321025,  14.02312893]]], shape=(4, 500, 8))tau(chain, draw)float640.8775 0.8027 0.8027 ... 2.99 3.052array([[ 0.8774941 ,  0.80271439,  0.80271439, ...,  5.08600601,10.11493858,  8.07920452],[12.24115337,  5.47983028,  6.64791029, ...,  7.92542651,6.28841169,  6.79958791],[ 6.4434113 ,  4.12594277,  3.11972735, ...,  4.77141179,5.46401958,  4.77263378],[ 5.50797127,  2.86390738,  4.18963173, ...,  2.23648602,2.98965551,  3.05155936]], shape=(4, 500))log_tau(chain, draw)float64-0.1307 -0.2198 ... 1.095 1.116array([[-0.13068505, -0.2197563 , -0.2197563 , ...,  1.62649285,2.3140134 ,  2.08929342],[ 2.5048035 ,  1.70107413,  1.89430256, ...,  2.07007614,1.83870853,  1.91686201],[ 1.86305811,  1.41729454,  1.13774561, ...,  1.56264223,1.69818471,  1.56289831],[ 1.70619637,  1.05218691,  1.43261284, ...,  0.80490589,1.09515817,  1.11565273]], shape=(4, 500))mlogtau(chain, draw)float64nan nan nan ... 1.332 1.335 1.335array([[       nan,        nan,        nan, ..., 1.61215551, 1.64001175,1.66914816],[       nan,        nan,        nan, ..., 1.31088051, 1.31352885,1.32441238],[       nan,        nan,        nan, ..., 1.50889741, 1.52656086,1.53913392],[       nan,        nan,        nan, ..., 1.33207961, 1.33509146,1.33454261]], shape=(4, 500))theta_school_diff(chain, draw, school, school_bis)float640.0 0.8672 0.2318 ... -0.9001 0.0array([[[[ 0.00000000e+00,  8.67217220e-01,  2.31841190e-01, ...,-3.95581030e-01, -7.66372647e-01,  8.56942694e-01],[-8.67217220e-01,  0.00000000e+00, -6.35376031e-01, ...,-1.26279825e+00, -1.63358987e+00, -1.02745264e-02],[-2.31841190e-01,  6.35376031e-01,  0.00000000e+00, ...,-6.27422220e-01, -9.98213837e-01,  6.25101504e-01],...,[ 3.95581030e-01,  1.26279825e+00,  6.27422220e-01, ...,0.00000000e+00, -3.70791617e-01,  1.25252372e+00],[ 7.66372647e-01,  1.63358987e+00,  9.98213837e-01, ...,3.70791617e-01,  0.00000000e+00,  1.62331534e+00],[-8.56942694e-01,  1.02745264e-02, -6.25101504e-01, ...,-1.25252372e+00, -1.62331534e+00,  0.00000000e+00]],[[ 0.00000000e+00,  1.46221462e-01, -2.23669874e+00, ...,-6.69769083e-01, -1.59833232e+00, -1.09520896e+00],[-1.46221462e-01,  0.00000000e+00, -2.38292020e+00, ...,-8.15990545e-01, -1.74455379e+00, -1.24143042e+00],[ 2.23669874e+00,  2.38292020e+00,  0.00000000e+00, ...,1.56692966e+00,  6.38366417e-01,  1.14148979e+00],...0.00000000e+00,  3.90766926e+00,  2.10547022e+00],[ 8.66129686e-01, -4.35962075e+00,  1.36796768e+00, ...,-3.90766926e+00,  0.00000000e+00, -1.80219904e+00],[ 2.66832873e+00, -2.55742171e+00,  3.17016672e+00, ...,-2.10547022e+00,  1.80219904e+00,  0.00000000e+00]],[[ 0.00000000e+00, -3.24991544e+00,  2.02724044e-01, ...,-4.60650996e+00, -4.45882006e+00, -3.55873874e+00],[ 3.24991544e+00,  0.00000000e+00,  3.45263948e+00, ...,-1.35659453e+00, -1.20890463e+00, -3.08823300e-01],[-2.02724044e-01, -3.45263948e+00,  0.00000000e+00, ...,-4.80923401e+00, -4.66154411e+00, -3.76146278e+00],...,[ 4.60650996e+00,  1.35659453e+00,  4.80923401e+00, ...,0.00000000e+00,  1.47689901e-01,  1.04777123e+00],[ 4.45882006e+00,  1.20890463e+00,  4.66154411e+00, ...,-1.47689901e-01,  0.00000000e+00,  9.00081326e-01],[ 3.55873874e+00,  3.08823300e-01,  3.76146278e+00, ...,-1.04777123e+00, -9.00081326e-01,  0.00000000e+00]]]],shape=(4, 500, 8, 8))Attributes: (6)created_at :2025-01-19T14:32:33.071271+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0sampling_time :3.159093141555786tuning_steps :1000/posterior_predictiveDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float6438.88 -14.98 -10.34 ... 27.05 20.99array([[[ 3.88807857e+01, -1.49849216e+01, -1.03439794e+01, ...,-4.53421946e-01,  2.84270717e+00,  8.20564213e+00],[-1.09136043e+01, -5.46343681e+00, -1.39191554e+01, ...,7.92596227e-02,  1.04706624e+01,  5.74433629e+00],[ 2.43568740e+01,  5.04120248e+00, -3.97189974e+00, ...,9.19419545e+00, -9.01841969e+00, -8.28673872e+00],...,[ 1.46836502e+01,  1.07776376e+01,  7.72374832e+00, ...,2.07985780e+01,  2.71985495e+01, -3.98398295e-02],[ 1.91972825e+01,  5.05393350e+00,  3.67205661e+01, ...,4.12409152e+00, -8.43032701e+00,  1.60217614e+01],[ 2.97328933e+01, -1.71746504e+01,  1.29356421e+01, ...,2.04783532e+01,  3.40664989e+00, -1.61720811e+01]],[[-1.92403726e+01, -3.71664170e+00,  1.19845274e+01, ...,-6.27350379e+00,  1.82161770e+01, -1.00228619e+01],[ 1.86320278e+01,  1.91741424e+01,  1.92685264e+01, ...,1.14283196e+01,  8.80065979e+00, -1.17813599e+01],[ 5.39739048e+00,  1.55544134e+01,  3.15388958e+01, ...,3.36005209e+00, -1.49180461e+01, -3.10678778e+01],...5.34484994e+00,  1.14307939e+01, -1.40588923e+01],[ 1.85696015e+01,  3.77431269e+00, -1.76569634e+01, ...,2.31808477e-01,  1.75124715e+01, -8.91929377e+00],[-2.13677255e+01,  5.50849049e+00,  1.54116922e+01, ...,3.75344366e+00,  1.48430312e+01,  2.66509638e+01]],[[-2.37078023e+01, -1.02510294e+01, -1.12880472e+01, ...,-5.29543186e-01, -6.49050815e+00, -3.15408752e+01],[-2.16410465e+01, -6.31439238e+00, -1.53583364e+01, ...,-5.99830907e+00,  4.52838067e+00,  2.75665937e+01],[-8.61319882e+00, -9.19845147e+00, -2.09085127e+01, ...,1.09366122e+01, -7.29127952e+00, -1.24081502e+00],...,[ 7.05075518e+00, -1.37615911e+00,  3.61806210e+00, ...,8.70889421e-01, -3.48952595e+00, -2.64420233e+01],[ 4.86374019e+00,  1.23143123e+01,  2.73959199e+01, ...,1.06871822e+01,  8.50271794e+00,  2.35086509e+01],[ 3.61134297e+00,  1.76460822e+01,  2.11929881e+00, ...,1.49536945e+00,  2.70513257e+01,  2.09934382e+01]]],shape=(4, 500, 8))Attributes: (4)created_at :2025-01-19T14:32:33.992006+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0/log_likelihoodDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64-5.093 -3.436 ... -3.269 -3.816array([[[-5.09275853, -3.43602475, -3.74204057, ..., -3.3289589 ,-4.33399417, -3.98073336],[-5.26031561, -3.48484759, -3.76482075, ..., -3.31812477,-4.42471152, -3.96411378],[-5.26031561, -3.48484759, -3.76482075, ..., -3.31812477,-4.42471152, -3.96411378],...,[-4.09703773, -3.49627472, -3.76612698, ..., -3.59133553,-3.47387895, -3.82530982],[-4.12627242, -3.39859971, -4.96085101, ..., -3.68426142,-3.98672531, -3.80936405],[-5.83672896, -3.23975234, -3.78380203, ..., -3.31710656,-3.25676615, -3.81812042]],[[-4.53493252, -4.19369629, -3.71252881, ..., -3.54470209,-3.95919031, -3.83061911],[-5.12340972, -3.28511949, -4.11673086, ..., -3.63150806,-4.04018326, -4.035499  ],[-4.60534759, -3.29968296, -4.22331898, ..., -3.39675761,-3.97321293, -4.28958748],...[-4.80633988, -3.26518649, -4.00014139, ..., -4.02829804,-3.47990605, -3.96972772],[-4.42136215, -3.27965379, -4.0385219 , ..., -3.68148339,-3.82237738, -3.89392821],[-4.40756009, -3.30401038, -4.08319323, ..., -3.75125901,-3.81761193, -3.86135846]],[[-5.02854201, -3.3877189 , -3.71507039, ..., -3.32877169,-6.07416748, -4.37182748],[-5.29003382, -3.28509719, -3.69168961, ..., -3.42725525,-3.67549279, -3.93042792],[-4.62530329, -4.13931404, -3.72023021, ..., -3.31704004,-4.77108916, -3.9488982 ],...,[-4.2388729 , -3.28464491, -3.98274408, ..., -3.37689019,-3.56653355, -3.88678268],[-4.5989656 , -3.31448557, -3.87093902, ..., -3.80418194,-3.72630817, -3.8170918 ],[-4.31031676, -3.38479007, -4.03502684, ..., -4.13497526,-3.2688568 , -3.81562673]]], shape=(4, 500, 8))Attributes: (4)created_at :2025-01-19T14:32:33.869112+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0.../observed_dataDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float6428.0 8.0 -3.0 7.0 ... 1.0 18.0 12.0array([28.,  8., -3.,  7., -1.,  1., 18., 12.])Attributes: (4)created_at :2025-01-19T14:32:29.216855+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0/constant_dataDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter','Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)sigma(school)float6415.0 10.0 16.0 ... 11.0 10.0 18.0array([15., 10., 16., 11.,  9., 11., 10., 18.])Attributes: (4)created_at :2025-01-19T14:32:29.217914+00:00arviz_version :0.20.0inference_library :pymcinference_library_version :5.20.0/predictionsDimensions:chain: 4draw: 500new_school: 2Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))new_school(new_school)&lt;U13'Essex College' 'Moordale'array(['Essex College', 'Moordale'], dtype='&lt;U13')Data variables: (1)obs(chain, draw, new_school)float642.041 -2.556 ... -1.015 -0.2822array([[[ 2.04091912, -2.55566503],[ 0.41809885, -0.56776961],[-0.45264929, -0.21559716],...,[-0.80265585,  0.40858787],[ 0.89066617,  0.91324226],[ 0.30152948, -2.85103878]],[[-1.02941822,  0.81504467],[-0.86725243, -1.00340203],[-2.30495532,  1.26656886],...,[-1.40028095,  1.9391935 ],[-0.37582993, -0.76872586],[ 0.11466401, -0.89829659]],[[ 0.02963037, -0.96028439],[ 0.56533507,  0.05565896],[-1.36828642,  1.0376982 ],...,[ 0.23222422,  0.36513287],[ 0.31840946, -0.56685801],[ 2.39826354,  0.91078977]],[[-1.42283401, -0.74058959],[ 0.83390251,  0.53293412],[ 0.13188271, -0.03434879],...,[ 1.57846099,  0.24653314],[ 0.64302486,  1.42710376],[-1.01529472, -0.28215614]]], shape=(4, 500, 2))Attributes: (4)created_at :2026-01-26T08:51:33.741510+00:00creation_library :ArviZcreation_library_version :0.8.2creation_library_language :Python\n\n\n\n\n\n\nHoyer, Stephan, and Joe Hamman. 2017. “Xarray: N-D Labeled Arrays and Datasets in Python.” Journal of Open Research Software 5 (1). https://doi.org/10.5334/jors.148.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html",
    "href": "Chapters/Distributions.html",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "",
    "text": "3.1 Random variables\nFrom a Bayesian perspective probabilities represent a degree of (un)certainty about the occurrence of an event. It is a measure of the likelihood that a particular hypothesis or event is true, given the available data and prior knowledge. We assign the value 0 to something impossible and 1 to something certain. When we are unsure we assign a value in between. For example, we could say that the probability of rain tomorrow is 0.32. This means that we are 32% certain that it will rain tomorrow.\nIn practice we usually do not care about individual probabilities, instead we work with probability distributions. A probability distribution describes the probabilities associated with each possible outcome of an experiment. In statistics, the term “experiment” is used in a very wide sense. It could mean a well-planned experiment in a laboratory, but it could also mean the result of a poll, the observation of the weather tomorrow, or the number of people that will visit a website next week.\nLet’s consider the experiment of observing the weather tomorrow. The possible outcomes of this experiment include the following outcomes:\nNotice that we are omitting the possibility of snow, or hail. In other words, we are assigning 0 probability to those outcomes. It is usually the case that we do not ponder all the possible outcomes of an experiment, either because we deliberately assume them to be irrelevant, because we don’t know about them, or because is too complex/expensive/time-consuming/etc to take them all into account.\nAnother important thing to notice, from this example, is that these outcomes are words (or strings if you want). To work with them we need to assign a number to each outcome. For example, we could assign the numbers 0 to Rainy, 1 to Sunny, and 2 to Cloudy. This mapping from the outcomes to the numbers is called a random variable. This is a funny and potentially misleading name as its mathematical definition is not random (the mapping is deterministic) nor a variable (it is a function). The mapping is arbitrary, -1 to Rainy, 0 to Sunny, and 4 to Cloudy is also valid. But once we pick one mapping, we keep using it for the rest of the experiment or analysis. One common source of confusion is understanding where the randomness comes from if the mapping is deterministic. The randomness comes from the uncertainty about the outcome of the experiment, i.e. the weather tomorrow. We are not sure if it will be rainy tomorrow until tomorrow comes.\nRandom variables can be classified into two main types: discrete and continuous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#random-variables",
    "href": "Chapters/Distributions.html#random-variables",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "",
    "text": "Rainy\nSunny\nCloudy\n\n\n\n\n\nDiscrete Random Variables: They can take on a countable number of distinct values. We already saw an example of a discrete random variable, the weather tomorrow. It can take on three values: Rainy, Sunny, and Cloudy. No intermediate values are allowed in our experiment, even when it is true that it can be partially sunny and still rain. And it has to be at least partially cloudy to rain. But we are not considering those possibilities.\nContinuous Random Variables: They can take on any value within a certain range. For example, the temperature tomorrow is a continuous random variable. If we use a Celsius scale, then it can take on any value between -273.15 Celsius to \\(+ \\infty\\). Of course, in practice, the expected temperature is restricted to a much narrower range. The lowest recorded temperature on Earth is −89.2 °C and the highest is 56.7 °C, and that range will be even narrower if we consider a particular region of our planet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#probability-mass-and-density-functions",
    "href": "Chapters/Distributions.html#probability-mass-and-density-functions",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "3.2 Probability mass and density functions",
    "text": "3.2 Probability mass and density functions\nThe probability distribution of a discrete random variable is often described using a probability mass function (PMF), which gives the probability of each possible outcome. For instance, the following plot shows the probability mass function of a categorical distribution with three possible outcomes, like Rainy, Sunny, and Cloudy.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.1: PMF of a Categorical disribution\n\n\n\n\n\nUsually, there is more than one probability distribution that we can use to represent the same set of probabilities, for instance, we could use a binomial distribution.\n\npz.Binomial(2, 0.6).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.2: PMF of a Binomial distribution\n\n\n\n\n\nThe probability distribution of a continuous random variable is described using a probability density function (PDF), which specifies the likelihood of the random variable falling within a particular interval. For instance, we could use a normal distribution to describe the temperature tomorrow.\n\npz.Normal(30, 4).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.3: PDF of a normal distribution\n\n\n\n\n\nor maybe a skew normal like this if we expect higher temperatures like during summer.\n\npz.SkewNormal(38, 5, -2).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.4: PDF of a skew-normal distribution\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we use plot_pdf() instead of plot_pmf(), this is because PreliZ uses the same method to plot both discrete and continuous distributions. In the case of discrete distributions, it will plot the probability mass function (PMF), and in the case of continuous distributions, it will plot the probability density function (PDF).\n\n\nOne issue when interpreting a PDF is that the y-axis is a density, not a probability. To get probability from a PDF we need to integrate the density over a given interval. This is something straightforward to do with a computer. But not that easy to do “visually”, human eyes/brains are not very good at that task. One way to alleviate this issue is to accompany a PDF with a point interval, like in the following plot.\n\npz.SkewNormal(38, 5, -2).plot_pdf(pointinterval=True);\n\n\n\n\n\n\n\nFigure 3.5: PDF of a SkewNormal distribution with a pointinterval\n\n\n\n\n\nThe point interval shows the quantiles of the distribution. Quantiles divide a dataset into equal probability intervals. For example, deciles divide a dataset into 10 equal-probability intervals, and quartiles divide a dataset into 4 equal-probability intervals. The most common quantile is the median (or 50th percentile), which divides a dataset into two equal-probability intervals where half of the data falls below the median and half of the data falls above the median.\nThe point interval in Figure 3.5 shows the 5th, 25th, 50th, 75th, and 95th percentiles. The point is the median. The tick line is the interquartile range (the central 50% of the distribution) and the thin line is the central 90% of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#cumulative-distribution-functions",
    "href": "Chapters/Distributions.html#cumulative-distribution-functions",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "3.3 Cumulative distribution functions",
    "text": "3.3 Cumulative distribution functions\nIn the previous section, we saw that we can use PMFs and PDFs to represent the probability distribution of a random variable. But there are other ways to represent a distribution. For example, we could use the cumulative distribution function (CDF).\nThe CDF is defined as the probability that the random variable takes a value less than or equal to \\(x\\). The CDF is defined for both discrete and continuous random variables. Figure 3.6 shows the CDF of a categorical distribution with three possible outcomes. Compare it with Figure 3.1\n\npz.Categorical([0.15, 0.6, 0.25]).plot_cdf();\n\n\n\n\n\n\n\nFigure 3.6: CDF of a Categorical distribution\n\n\n\n\n\nFigure 3.7 shows the CDF of a normal distribution (compare it with Figure 3.3).\n\npz.Normal(30, 4).plot_cdf();\n\n\n\n\n\n\n\nFigure 3.7: CDF of a normal distribution\n\n\n\n\n\nThe CDF is usually easier to read than the PDF, as we already saw y-axis for a PDF is a density that has no intrinsic meaning, and to get probability from a PDF we need to evaluate areas. Instead for a CDF the y-axis is a probability. For the PMF/PDF it is easier to get the mode (the highest value for the point/curve), and for the CDF it is easier to get the median (the value of \\(x\\) for which \\(y=0.5\\)), or other quantiles. From the CDF it is also easier to quickly get quantities like the probability of getting a temperature equal or lower than 35 degrees. It is the value of the CDF at 35. From Figure 3.7 we can see that it is roughly 0.9 or 90%, if you want more accuracy you could use a matplotlib/ArviZ style with a grid (like arviz-darkgrid) or use the cdf() function.\n\npz.Normal(30, 4).cdf(35)\n\n0.894350157794624\n\n\nFrom the CDF we can also easily get the probability of a range of values. For example, the probability of the temperature being between 25 and 35 degrees is the difference between the CDF at 35 and the CDF at 25. From Figure 3.7 we can get that it is roughly 0.9 or 90%. Again even when you can get a good estimate just by looking at the graph you can use the cdf() function to get a more accurate estimate. But the fact that you can get a good estimate by looking at the graph is a good feature.\n\nnp.diff(pz.Normal(30, 4).cdf([25, 35]))\n\narray([0.78870032])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#inverse-cumulative-distribution-functions",
    "href": "Chapters/Distributions.html#inverse-cumulative-distribution-functions",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "3.4 Inverse cumulative distribution functions",
    "text": "3.4 Inverse cumulative distribution functions\nSometimes we may want to use the inverse of the CDF. This is known as the quantile function or the percent point function (PPF). The PPF is also defined for both discrete and continuous random variables. For example, Figure 3.8 shows the PPF of a categorical distribution with three possible outcomes and Figure 3.9 shows the PPF of a normal distribution.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_ppf();\n\n\n\n\n\n\n\nFigure 3.8: PPF of a Categorical distribution\n\n\n\n\n\n\npz.Normal(30, 4).plot_ppf();\n\n\n\n\n\n\n\nFigure 3.9: PPF of a normal distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#sec-dist",
    "href": "Chapters/Distributions.html#sec-dist",
    "title": "3  Random Variables, Distributions, and Uncertainty",
    "section": "3.5 Distributions in ArviZ",
    "text": "3.5 Distributions in ArviZ\nThe PMF/PDF, CDF, and PPF are convenient ways to represent distributions for which we know the analytical form. But in practice, we often work with distributions that we don’t know their analytical form. Instead, we have a set of samples from the distribution. A clear example is a posterior distribution, computed using an MCMC method. For those cases, we still want useful visualization that we can use for ourselves or to show others. Some common methods are:\n\nHistograms\nKernel density estimation (KDE)\nEmpirical cumulative distribution function (ECDF)\nQuantile dot plots\n\nWe will discuss these methods in the next subsections with special emphasis on how they are implemented in ArviZ.\n\n3.5.1 Histograms\nHistograms are a very simple and effective way to represent a distribution. The basic idea is to divide the range of the data into a set of bins and count how many data points fall into each bin. Then we use as many bars as bins, with the height of the bars being proportional to the counts. The following video shows a step-by-step animation of a histogram being built.\n\nHistograms can be used to represent both discrete and continuous random variables. Discrete variables are usually represented using integers.  Arguably the most important parameter of a histogram is the number of bins. Too few bins and we will miss details, too many and we will plot noise. You can pick the number of bins with a bit of trial and error, especially when you have good idea of what you want to show. However, there are many methods to compute the number of bins automatically from the data, like the Freedman–Diaconis rule or the Sturges’ rule. By default, ArviZ computes the number of bins using both rules and then picks the one that gives the largest number of bins. This is the same approach used by np.histogram(., bins=\"auto) and plt.hist(., bins=\"auto). Additionally, when the data is of type integers, ArviZ will preserve that structure and will associate bins to integers, instead of floats. If the number of unique integers is relatively small then, it will associate one bin to each integer. For example, in the following figure each bar is associated with an integer in the interval [0, 9].\n\nd_values = azp.convert_to_dataset(pz.Poisson(3).rvs((1, 500), random_state=rng))\nazp.plot_dist(d_values, kind=\"hist\");\n\n\n\n\n\n\n\nFigure 3.10: Histogram from a sample of integers. Each bin corresponds to a single integer.\n\n\n\n\n\nWhen the discrete values take higher values, like in Figure 3.11, bins are still associated with integers but many integers are binned together.\n\nd_values = azp.convert_to_dataset(pz.Poisson(100).rvs((1, 500), random_state=rng))\nazp.plot_dist(d_values, kind=\"hist\");\n\n\n\n\n\n\n\nFigure 3.11: Histogram from a sample of integers. Bins group together many integers.\n\n\n\n\n\nIf you don’t like the default binning criteria of ArviZ, you can change it by passing the bins argument using the hist_kwargs.\n\nd_values = azp.convert_to_dataset(pz.Poisson(100).rvs((1, 500), random_state=rng))\nazp.plot_dist(d_values, kind=\"hist\", stats={\"density\":{\"bins\":20}})\n\n\n\n\n\n\n\nFigure 3.12: Histogram from a sample of integers, with bins automatically computed by Matplotlib, not ArviZ.\n\n\n\n\n\n\n\n3.5.2 KDE\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function from a sample. Intuitively you can think of it as the smooth version of a histogram. Conceptually you place a kernel function like a Gaussian on top of a data point, then you sum all the Gaussians, generally evaluated over a grid and not over the data points. Results are normalized so the total area under the curve is one. The following video shows a step-by-step animation of a KDE being built. You can see a version with border corrections and without them. Border corrections avoid adding a positive density outside the range of the data.\n\nThe following block of code shows a very simple example of a KDE.\n\n# synthetic data, 7 points\ndata = np.array([7.9, 6.5, 7.4, 8.3, 5.1, 5.4, 7.2])\nn_obs = len(data)\n# compute the KDE\nbandwidth = 0.4\ngrid = np.linspace(data.min() - bandwidth * 3, data.max() + bandwidth * 3, 200)\nkernels = np.transpose([pz.Normal(x_i, bandwidth).pdf(grid) for x_i in data])\nkernels *= 1 / n_obs  # normalize the results\n\n# plot the results\n_, ax = plt.subplots(figsize=(9, 4))\nax.plot(grid, kernels, \"k--\", alpha=0.5)\nax.plot(data, np.zeros(n_obs), \"C1o\")\nax.plot(grid, kernels.sum(1))\nax.set(xticks=[], yticks=[]);\n\n\n\n\n\n\n\n\nThe most important parameter of a KDE is the bandwidth which controls the degree of smoothness of the resulting curve. It is analogous to the number of bins for the histograms. ArviZ’s default method to compute the bandwidth works well for a wide range of distributions including multimodal ones. Compared to other KDEs in the Python ecosystem, the KDE implemented in ArviZ automatically handles the boundaries of a distribution. ArviZ will assign a density of zero to any point outside the range of the data.\nThe following example shows a KDE computed from a sample from a Gamma distribution. Notice that ArviZ computes a KDE instead of a histogram, and notice that there is no density for negative values.\n\nc_values = azp.convert_to_dataset(pz.Gamma(2, 3).rvs((1,1000), random_state=rng))\nazp.plot_dist(c_values);\n\n\n\n\n\n\n\nFigure 3.13: KDE from a sample of floats. By default, ArviZ computes a KDE.\n\n\n\n\n\n\n\n3.5.3 ECDF\nBoth histograms and KDEs are ways to approximate the PMF/PDF of a distribution from a sample. But sometimes we may want to approximate the CDF instead. The empirical cumulative distribution function (ECDF) is a non-parametric way to estimate the CDF. It is a step function that jumps up by 1/N at each observed data point, where N is the total number of data points. The following video shows a step-by-step animation of an ECDF being built.\n\nThe following block of code shows a very simple example of an ECDF.\n\nazp.plot_dist(c_values, kind=\"ecdf\");\n\n\n\n\n\n\n\nFigure 3.14: Empirical cumulative distribution function\n\n\n\n\n\n\n\n3.5.4 Quantile dot plots\nA quantile dot plot (Wilkinson 1999; Kay et al. 2016) displays the distribution of a sample in terms of its quantiles. Reading the median or other quantiles from quantile dot plots is generally easy, we just need to count the number of dots.\nThe following video shows a step-by-step animation of a quantile dot plot being built.\n\nFrom Figure 3.15 we can easily see that 30% of the data is below 1. We do this by noticing that we have a total of 10 dots and 3 of them are below 1.\n\nd_values = azp.convert_to_dataset(pz.Normal(2, 2).rvs((4, 1000), random_state=123))\nazp.plot_dist(\n    d_values,\n    kind=\"dot\",\n    stats={\"dist\": {\"nquantiles\": 10}},\n);\n\n\n\n\n\n\n\nFigure 3.15: Quantile dot plot\n\n\n\n\n\nThe number of quantiles is something you will need to choose by yourself, usually, it is a good idea to keep this number relatively small and “round”, as the main feature of a quantile dot plot is that finding probability intervals reduces to counting dots (Kay et al. 2016; Fernandes et al. 2018). It is easier to count and compute proportions if you have 10, or 20 dots than if you have 11 or 57. But sometimes a larger number could be a good idea too. When we are interested in the tails of a distribution, using more quantiles can help. A choice like 100 is often a good default because each dot represents exactly 1% of the distribution, ensuring small probabilities can still be estimated accurately. 100 is the default in ArviZ.\n\n\n\n\nFernandes, Michael, Logan Walls, Sean Munson, Jessica Hullman, and Matthew Kay. 2018. “Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–12. CHI ’18. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3173574.3173718.\n\n\nKay, Matthew, Tara Kola, Jessica R. Hullman, and Sean A. Munson. 2016. “When (Ish) Is My Bus? User-Centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems.” In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5092–5103. CHI ’16. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2858036.2858558.\n\n\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276–81. https://doi.org/10.1080/00031305.1999.10474474.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables, Distributions, and Uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html",
    "href": "Chapters/MCMC_diagnostics.html",
    "title": "4  MCMC Diagnostics",
    "section": "",
    "text": "4.1 From the MCMC theory to practical diagnostics\nThe Achilles heel of computing posterior distributions is, more often than not, the computation of the denominator of Bayes’s theorem. Markov Chain Monte Carlo Methods (MCMC), such as Metropolis, Hamiltonian Monte Carlo (and its variants like NUTS) are clever mathematical and computational devices that let us circumvent this problem. Check Brooks et al. (2011) for a thorough treatment of Bayesian inference and MCMC techniques. The main idea is based on sampling values of the parameters of interest from an easy-to-sample distribution and then applying a rule, known as the Metropolis acceptance criterion, to decide if you accept or not that proposal. By applying this rule we ensure that even when we propose samples from an arbitrary distribution we end up getting samples from the correct posterior distribution. The better performance of NUTS over Metropolis can be explained by the fact that the former uses a clever way to propose values.\nWhile MCMC methods can be successfully used to solve a huge variety of Bayesian models, they have some trade-offs. Most notably, finite MCMC chains are not guaranteed to converge to the true posterior distribution. Thus, a key step is to check whether we have a valid sample, otherwise, any analysis from it will be totally flawed.\nThere are several tests we can perform, some are visual and some are numerical. These tests are designed to spot problems with our samples, but they are unable to prove we have the correct distribution; they can only provide evidence that the sample seems reasonable.\nIn this chapter, we will cover the following topics:\nThe theory describes certain behaviors of MCMCs methods, many diagnoses are based on evaluating whether the theoretical results are empirically verified. For example, MCMC theory says that:\nWe are going to see that many diagnostics need multiple chains. Each chain is an independent MCMC run. The logic is that by comparing independent runs we can more easily sport issues than running a single instance. This multiple-chain approach also takes advantage of modern hardware. If you have a CPU with 4 cores you can get 4 independent chains in essentially the same time that one single chain.\nTo keep the focus on the diagnostics and not on any particular Bayesian model. We are going to first create 3 synthetic samples, we will use them to emulate samples from a posterior distribution.\nShow the code for more details\ngood_sample = pz.Gamma(2, 5).rvs((4, 1000), random_state=rng)\nbad_sample0 = pz.Normal(np.sort(good_sample, axis=None),\n                        0.05).rvs(4000, random_state=rng).reshape(4, -1)\n\nbad_sample1 = good_sample.copy()\nstuck = []\nfor i in pz.DiscreteUniform(0, 900).rvs(4, random_state=rng):\n    stuck.append((i, i+50))\n    bad_sample1[0,i:i+50] = pz.Beta(i, 150).rvs(50, random_state=rng)\n\nsample = azp.convert_to_dataset({\"good_sample\":good_sample,\n          \"bad_sample_0\":bad_sample0,\n          \"bad_sample_1\":bad_sample1})",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "href": "Chapters/MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "title": "4  MCMC Diagnostics",
    "section": "",
    "text": "The initial value is irrelevant, we must always arrive at the same result\nThe samples are not really independent, but the value of a point only depends on the previous point, there are no long-range correlations.\nIf we look at the sample as a sequence we should not be able to find any patterns.\n\nFor example, for a sufficiently long sample, the first portion must be indistinguishable from the last (and so should any other combination of regions).\n\nFor the same problem, each sample generated will be different from the others, but for practical purposes, the samples should be indistinguishable from each other.\n\n\n\n\ngood_sample: A random sample from a Gamma(2, 5). This is an example of a good sample because we are generating independent and identically distributed (iid) draws. This is the ideal scenario.\nbad_sample_0: We sorted good_sample, split it into four chains, and then added a small Gaussian error. This is a representation of a bad sample because values are not independent (we sorted the values!) and they do not come from the same distribution. This represents a scenario where the sampler has very poor mixing.\nbad_sample_1: we start from good_chains, and turn into a poor sample by randomly introducing portions where consecutive samples are highly correlated to each other. This represents a common scenario, a sampler can resolve a region of the parameter space very well, but get stuck into one or more regions.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter reading this chapter a good exercise is to come back here and modify these synthetic samples and run one or more diagnostics. If you want to make the exercise even more fun challenge yourself to predict what the diagnostics will be before running. Or even the other way around, how you should change the samples to get a given result. This is a good test of your understanding and a good way to correct possible misunderstandings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#trace-plots",
    "href": "Chapters/MCMC_diagnostics.html#trace-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.2 Trace plots",
    "text": "4.2 Trace plots\nA trace plot is created by drawing the sampled values at each iteration step. Ideally, we should expect to see a very noisy plot, some people call it a caterpillar. The reason is that draws should be uncorrelated from each other, the value of a draw should not provide any hint about the previous or next draw. Also, the draws from the first iterations should be indistinguishable from the ones coming from the last iterations the middle iterations, or any other region. The ideal scenario is the lack of any clear pattern as we can see at the right panel of Figure 4.1.\nIn ArviZ by calling the function azp.plot_trace_dist(.) we get a trace plot on the right and on the left a KDE Figure 4.1 is an example of this.\n\nazp.plot_trace_dist(sample, var_names=\"good_sample\", compact=False);\n\n\n\n\n\n\n\nFigure 4.1: Trace plot and Kernel Density Estimate (KDE) of a sample without issues\n\n\n\n\n\nAs discussed in Section 3.5, there are many methods to represent distributions. We can use the argument kind to define other methods:\n\nkind=\"kde\" for kernel density estimation (KDE)\nkind=\"hist\"for histograms\nkind=\"ecdf\" for empirical cumulative distribution function (ECDF)\nkind=\"dot\" for quantile dot plots\n\nInstead of a KDE we use other visualizations like ECDFs, histograms or quantile dot plots, this is specified with the kind argument. Inspecting the KDE, ECDF, histogram, or quantile dot plot may also help to spot differences between chains, ideally, distributions should overlap. Overlayed KDEs and ECDFs are usually easy to interpret than overlayed histograms or quantile dot plots.\nAlternatively, if we just want the trace without the density representation we can use the function azp.plot_trace(.)\n\nazp.plot_trace(sample, var_names=\"good_sample\");\n\n\n\n\n\n\n\nFigure 4.2: Trace plot of a sample without issues\n\n\n\n\n\nNow let’s see an example of a bad sample. Figure 4.3 shows two problems. On the one hand, each chain is visiting a different region of the parameter space. We can see this from the trace plot itself and the KDE. On the other hand, even within each region, the sampler is having trouble properly exploring the space, notice how it keeps moving up, instead of being stationary.\n\n\nCode\nazp.plot_trace_dist(sample, var_names=\"bad_sample_0\", compact=False);\n\n\n\n\n\n\n\n\nFigure 4.3: Trace plot of 4 chains that has not converged\n\n\n\n\n\nFinally from Figure 4.4, we can see another common scenario. While the overall trace appears acceptable, there are specific regions where the sampler gets stuck. To highlight these problematic areas, we’ve added gray bands across the trace. In the first region, the issue is less apparent because the stuck chain is overlapped by others. In contrast, the second and third regions make the issue easier to spot, though we still need to inspect the trace closely to see the problem.\n\n\nCode\npc = azp.plot_trace_dist(sample, var_names=\"bad_sample_1\", compact=False)\npc.coords = {\"column\": \"trace\"}\nazp.add_bands(pc, stuck)\n\n\n\n\n\n\n\n\nFigure 4.4: Trace plot showing a sampler being stuck in 3 regions (see gray bands). Because of the overlap, it’s not easy to spot the problem for the first region.\n\n\n\n\n\nTrace plots are very popular in the Bayesian literature, they can be useful to spot some problems, but they have some limitations as we saw in the previous example. For example, it is difficult to spot problems when the chains overlap. In the next section we will see a modern alternative to trace plots, the rank plot.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#sec-rank-plots",
    "href": "Chapters/MCMC_diagnostics.html#sec-rank-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.3 Rank plots",
    "text": "4.3 Rank plots\nRank plots provide an alternative way to diagnose MCMC samples (Vehtari et al. 2021). Unlike trace plots, which display sample values ordered by iteration, rank plots transform the sampled values into ranks before visualization.\nLet see how it works: For each parameter, we pool all chains together and sort the draws from lowest to highest. The smallest value is assigned rank 1, the next rank 2, and so on, up to the total number of samples (equal to the number of chains multiplied by the number of draws per chain). This process converts parameter values into ranks. Next, we separate the ranks back into their original chains, collecting all ranks from the first chain, the second chain, and so on. If the MCMC sampling has no issues, the chains should be indistinguishable, and the ranks within each chain should follow a uniform distribution. In a well-mixed sampler, ranks should be evenly distributed among chains, with no chain favouring a specific range.\nTo assess uniformity, one simple approach is to plot a histogram of the ranks. However, histogram results can be sensitive to bin choices. An alternative is to plot the empirical cumulative distribution function (CDF) of the ranks. This method avoids binning and does not requires any complex processing. Instead of using raw ranks (ranging from 1 to the total number of samples), we can use fractional ranks, which values range from 0 to 1. In the ideal case, the empirical CDF follows the standard uniform distribution—a diagonal line from (0,0) to (1,1), as shown in Figure 5.6. Deviations from this line may indicate sampling issues.\n\npz.Uniform(0, 1).plot_cdf(figsize=(6, 4));\n\n\n\n\n\n\n\nFigure 4.5: The CDF of the standard Uniform distribution.\n\n\n\n\n\nThe disadvantage of such visualization is that all the “action” is close to the diagonal line and most of the figure is just blank space, specially as the MCMC sample size increase. A simple trick to improve the data-ink ratio is to plot the \\(\\Delta\\)-ECDF, i.e. (observed ECDF - expected ECDF). Then a uniform distribution is a flat line that goes from (0,0) to (1, 0).\nThe following figures shows fractional rank plots for good_sample, bad_sample_0 and bad_sample_1.\n\n\nCode\nazp.plot_rank(sample, var_names=[\"good_sample\"]);\n\n\n\n\n\n\n\n\nFigure 4.6: Rank plot for good_sample\n\n\n\n\n\nFor good_sample we can see that ranks looks relative flat and deviations are within the envelope. In sharp contrast bad_sample_0 looks awful, deviations are very large as chains belong to different distributions. The envelopes computation does not take into account autocorrelation, so it is recommend to thin the draws as discussed in Section 4.4.3. plot_rank does this automatically. To disable thinning we can do azp.plot_rank(., thin=False), as we did for Figure 4.7.\n\n\nCode\nazp.plot_rank(sample, var_names=[\"bad_sample_0\"], thin=False);\n\n\n\n\n\n\n\n\nFigure 4.7: Rank plot for bad_sample_0\n\n\n\n\n\nFinally, bad_sample_1 also shows problems, see how chain 0 (blue) get out of the envelope. Notice also how the envelope and lines looks very “squared-shaped” compared with those from Figure 4.6. This is indicating a relatively low ESS (high thinning). This is a consequence of the chain 0 (blue) being stuck in some regions as shown in Figure 4.4.\n\n\nCode\nazp.plot_rank(sample, var_names=[\"bad_sample_1\"]);\n\n\n\n\n\n\n\n\nFigure 4.8: Rank plot for bad_sample_1\n\n\n\n\n\nRank plots provide a more reliable way to diagnose MCMC sampling issues compared to trace plots. While trace plots can reveal mixing problems, they often become cluttered, making it difficult to identify subtle issues like chains exploring different regions unevenly. In contrast, rank plots summarize sampling behaviour more effectively by checking for uniformity across chains, helping to detect biases, poor mixing, or convergence issues in a more interpretable way. This makes them a powerful method for visually assessing MCMC performance.\nFinally, we want to mention that we can use \\(\\Delta\\)-ECDF plots to visually inspect any quantity that is expected to be uniformly distributed, not just the ranks. For example, in Chapter 5 we use them to compare the posterior predictive distribution to the observed data (see Figure 5.8 and Figure 5.9).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#hat-r-r-hat",
    "href": "Chapters/MCMC_diagnostics.html#hat-r-r-hat",
    "title": "4  MCMC Diagnostics",
    "section": "4.4 \\(\\hat R\\) (R-hat)",
    "text": "4.4 \\(\\hat R\\) (R-hat)\nPlots are often useful for discovering patterns, but sometimes we want numbers, for example when quickly evaluating a large number of parameters it may be easier to look at numbers than plots. Number are also easier to plug into automatic routine, that call for human attention only if some threshold is exceeded. This may be necessary when running a large number of models, for example in a simulation study.\n\\(\\hat R\\) is a numerical diagnostic that answers the question Did the chains mix properly? But I also like to think of it as the score assigned by a jury in a trace (or rank) plot contest. The central idea is to compare the variance between chains with the variance within each chain. The version implemented in ArviZ and described in Vehtari et al. (2021) does several other things under the hood, but the main idea is the same.\nIdeally, we should get \\(\\hat R = 1\\), in practice \\(\\hat R \\lessapprox 1.01\\) are considered safe and in the first modeling phases, even higher values like \\(\\hat R \\approx 1.1\\) may be fine.\nUsing ArviZ we can get the \\(\\hat R\\) with azp.rhat(⋅), azp.summary(⋅) and azp.plot_forest(⋅, r_hat=True)\n\nazp.rhat(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 1.0\n    bad_sample_0  float64 8B 3.238\n    bad_sample_1  float64 8B 1.02xarray.DatasetDimensions:Data variables: (3)good_sample()float641.0array(1.00030008)bad_sample_0()float643.238array(3.23771295)bad_sample_1()float641.02array(1.02046142)\n\n\n\n4.4.1 Effective Sample Size (ESS)\nSince the samples of an MCMC are (auto)correlated, the amount of “useful” information is less than a sample of the same size but iid. Figure 4.9 can help us develop intuition. In this figure, we analyze the error incurred while computing an estimate (such as the mean) from samples of different size, considering varying degrees of autocorrelation. The results represent averages from 1000 repetitions.\nWe can see that the error goes down as the sample size increases and we can also see that the lower the autocorrelation the smaller the sample size to achieve an estimate with a given error. In other words the higher the autocorrelation the larger the number of sample we will need to achieve the a given precision.\n\n\nShow the code for more details\ndef generate_autocorrelated_sample(original, rho):\n    \"\"\"\n    Generates an autocorrelated sample from original.\n\n    Parameters:\n    ----------\n    sample: numpy array,\n        The original sample\n    rho: float,\n        Desired autocorrelation value\n\n    Returns:\n    --------\n    new_sample: numpy array, autocorrelated sample\n    \"\"\"\n    n = len(original)\n    y = np.copy(original)\n    mean = np.mean(original)\n    for i in range(1, n):\n        y[i] += rho * (y[i - 1] - mean) + np.random.randn()\n\n    return y\n\n\nmean = 0\nlag = 30\nsize = 300\n\niid_samples = pz.Normal(mean, 1).rvs((1000, size))\n\nrhos = np.linspace(0, 0.90, 7)\nN = len(rhos)\n\nfig, ax = plt.subplots(figsize=(12, 4))\n\nfor k, rho in enumerate(rhos):\n    auto_samples = np.stack(\n        [generate_autocorrelated_sample(iid_sample, rho) for iid_sample in iid_samples]\n    )\n    auto_error = []\n    for i in range(1, size):\n        auto_error.append(\n            np.mean(((np.mean(auto_samples[:, :i] - mean, 1) ** 2) ** 0.5))\n        )\n\n    ax.plot(auto_error[lag:], color=plt.cm.viridis_r(k / N))\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r)\ncbar = plt.colorbar(sm, ax=ax, label=\"Autocorrelation\", ticks=[0, 1])\ncbar.ax.set_yticklabels([\"Low\", \"High\"])\ncbar.ax.tick_params(length=0)\n\nax.set(\n    yticks=[],\n    ylabel=\"Error\",\n    xticks=[],\n    xlabel=\"Sample size\",\n    ylim=(0, None),\n    xlim=(-2, None),\n)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.plot(-2, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n\n\n\n\n\n\n\n\nFigure 4.9: Average error as a function of the sample size for different autocorrelation values.\n\n\n\n\n\nAs for MCMC samples, the sample size can be misleading, we instead estimate the effective sample size (ESS) (Vehtari et al. 2021), that is, the size of a sample with the equivalent amount of information but without autocorrelation. Figure 4.10 shows how when the sample size increases the ESS tends to increase too, and more importantly, it shows that the slope is higher for lower autocorrelation values.\n\n\nShow the code for more details\nmean = 0\nsize = 300\n\niid_samples = pz.Normal(mean, 1).rvs((500, size))\n\nrhos = np.linspace(0, 0.90, 7)\nN = len(rhos)\n\nfig, ax = plt.subplots(figsize=(12, 4))\n\nfor k, rho in enumerate(rhos):\n    auto_samples = np.stack(\n        [generate_autocorrelated_sample(iid_sample, rho) for iid_sample in iid_samples]\n    )\n    auto_error = []\n\n    for i in range(50, size, 10):\n        auto_error.append(azp.ess(auto_samples[:, :i]) / 500)\n\n    ax.plot(range(50, size, 10), auto_error, color=plt.cm.viridis_r(k / N))\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r)\ncbar = plt.colorbar(sm, ax=ax, label=\"Autocorrelation\", ticks=[0, 1])\ncbar.ax.set_yticklabels([\"Low\", \"High\"])\ncbar.ax.tick_params(length=0)\n\nax.set(\n    yticks=[],\n    ylabel=\"Effective sample size\",\n    xticks=[],\n    xlabel=\"Sample size\",\n    ylim=(0, None),\n    xlim=(48, None),\n)\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.plot(48, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n\n\n\n\n\n\n\n\nFigure 4.10: Effective sample size as a function of the sample size for different autocorrelation values.\n\n\n\n\n\nWith ArviZ we can get azp.ess(⋅), azp.summary(⋅) and azp.plot_forest(⋅, ess=True)\n\nazp.ess(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 4.184e+03\n    bad_sample_0  float64 8B 4.465\n    bad_sample_1  float64 8B 317.2xarray.DatasetDimensions:Data variables: (3)good_sample()float644.184e+03array(4184.08315356)bad_sample_0()float644.465array(4.46475517)bad_sample_1()float64317.2array(317.18292127)\n\n\nOne way to use the ESS is as a minimum requirement for trustworthy MCMC samples. It is recommended the ESS to be larger than 100 per chain. That is, for 4 chains we want a minimum of 400 effective samples.\n\n\n\n\n\n\nNote\n\n\n\nThe ESS can also be used as a metric of the efficiency of MCMC sampling methods. For instance, we may want to measure the ESS per sample (ESS/n), a sampler that generates an ESS/n closer to 1 is more efficient than a sampler that generates values closer to 0. Other common metrics are the ESS per second, and the ESS per likelihood evaluation.\n\n\nWe see that azp.summary(⋅) returns two ESS values, ess_bulk and ess_tail. This is because different regions of the parameter space may have different ESS values since not all regions are sampled with the same efficiency. Intuitively, one may think that when sampling a distribution like a Gaussian it is easier to obtain better sample quality around the mean than around the tails, simply because we have more samples from that region. For some models, it could be the other way around, but the take-home message remains, not all regions are necessarily sampled with the same efficiency\n\nazp.summary(sample, kind=\"diagnostics\")\n\n\n\n\n\n\n\n\ness_bulk\ness_tail\nr_hat\nmcse_mean\nmcse_sd\n\n\n\n\ngood_sample\n4184.08\n3580.03\n1.00\n0.00\n0.00\n\n\nbad_sample_0\n4.46\n4.91\n3.24\n0.13\n0.15\n\n\nbad_sample_1\n317.18\n155.69\n1.02\n0.02\n0.02\n\n\n\n\n\n\n\nIf we are going to use the MCMC samples to calculate central values such as means or medians then we have to make sure that the ess_bulk is sufficiently large, however, if we want to calculate intervals such as an HDI 94% we have to make sure that ess_tail be appropriate.\nArviZ offers several functions linked to the ESS. For example, if we want to evaluate the performance of the sampler for several regions at the same time we can use azp.plot_ess.\n\nazp.plot_ess(\n    sample,\n    figure_kwargs={\"figsize\": (10, 8)},\n    col_wrap=1,\n)\n\n\n\n\n\n\n\n\nA simple way to increase the ESS is to increase the number of samples, but it could be the case that the ESS grows very slowly with the number of samples, so even if we increased the number of samples 10 times we could still be very far from our target value. One way to estimate “how many more samples do we need” is to use azp.plot_ess_evolution(.). This graph shows us how the ESS changed with each iteration, which allows us to make predictions.\nFrom Figure 4.11 we can see that the ESS grows linearly with the number of samples for good_sample, and it does not grow at all for bad_sample_0. In the latter case, this is an indication that there is virtually no hope of improving the ESS simply by increasing the number of draws.\n\nazp.plot_ess_evolution(\n    sample,\n    var_names=[\"good_sample\", \"bad_sample_0\"],\n    col_wrap=1,\n);\n\n\n\n\n\n\n\nFigure 4.11: ESS evolution plot for good_sample and bad_sample_0.\n\n\n\n\n\n\n\n4.4.2 Monte Carlo standard error (MCSE)\nAn advantage of the ESS is that it is scale-free, it does not matter if one parameter varies between 0.1 and 0.2 and another between -2000 and 0, an ESS of 400 has the same meaning for both parameters. In models with many parameters, we can quickly identify which parameters are most problematic. However, when reporting results it is not very informative to know whether the ESS was 1372 or 1501. Instead, we would like to know the order of the errors we are making when approximating the posterior. This information is given by the Monte Carlo standard error (MCSE). Like the ESS, the MCSE takes into account the autocorrelation of the samples. This error should be below the desired precision in our results. That is, if for a parameter the MCSE is 0.1, it does not make sense to report that the mean of that parameter is 3.15. Since the correct value could easily be between 3.4 and 2.8.\nWith ArviZ we can get the MCSE with azp.mcse(⋅) or azp.summary(⋅).\n\nazp.mcse(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 0.004348\n    bad_sample_0  float64 8B 0.1339\n    bad_sample_1  float64 8B 0.01921xarray.DatasetDimensions:Data variables: (3)good_sample()float640.004348array(0.00434842)bad_sample_0()float640.1339array(0.13389772)bad_sample_1()float640.01921array(0.01920924)\n\n\n\n\n4.4.3 Thinning\nOne way to reduce autocorrelation in an MCMC chain is through thinning, where we retain only every \\(n\\)-th sample. While this method is straightforward, it has the drawback of discarding useful information. Research generally suggests that it’s better to keep all the samples when calculating estimates (MacEachern and Berliner 1994; Link and Eaton 2012). Provided the variance is finite, the central limit theorem applies even to correlated samples. Then if higher accuracy is needed, it’s more effective to increase the number of draws rather than to perform thinning. Still, there are situations where thinning might be useful, such as:\n\nReducing the size of stored data, which is especially important when dealing with a large number of models or when the postprocessing of the samples is expensive, for instance when we need to run expensive computations on every draw.\nAddressing bias in extreme ordered statistics, which may affect diagnostics like rank-plots (see Section 4.3) and uniformity tests typically done for posterior predictive checks, as shown in Figure 5.4 and Figure 7.2 or as part of Simulation Based Calibration (Talts et al. 2020).\n\nTo determine an appropriate thinning factor, we can use the effective sample size (ESS). For instance, if you have 2,000 samples and an ESS of 1,000, you would thin by a factor of 2, keeping every other sample. The higher the ESS, the lower the thinning factor required. A more refined approach is to calculate both ESS-tail and ESS-bulk, then use the smaller value, which better accounts for differences in sampling efficiency between the central 90% quantile and the 5% tail quantiles (Säilynoja, Bürkner, and Vehtari 2022).\nIn ArviZ we have the thin function, which allows us to perform thinning automatically.\n\nazp.thin(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 448B\nDimensions:       (chain: 4, draw: 4)\nCoordinates:\n  * chain         (chain) int64 32B 0 1 2 3\n  * draw          (draw) int64 32B 0 308 616 924\nData variables:\n    good_sample   (chain, draw) float64 128B 0.3661 0.7864 ... 0.1519 0.4031\n    bad_sample_0  (chain, draw) float64 128B 0.0714 0.08674 ... 0.7214 1.108\n    bad_sample_1  (chain, draw) float64 128B 0.3661 0.7864 ... 0.1519 0.4031xarray.DatasetDimensions:chain: 4draw: 4Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 308 616 924array([  0, 308, 616, 924])Data variables: (3)good_sample(chain, draw)float640.3661 0.7864 ... 0.1519 0.4031array([[0.36610475, 0.78636751, 0.51138167, 0.15025618],\n       [0.22672028, 0.09945281, 0.19787423, 0.64621791],\n       [0.40832387, 0.21469846, 0.17243945, 0.48192411],\n       [0.73288515, 0.2556624 , 0.15193088, 0.40308445]])bad_sample_0(chain, draw)float640.0714 0.08674 ... 0.7214 1.108array([[0.07139604, 0.08673756, 0.24009538, 0.18281916],\n       [0.15437162, 0.2389048 , 0.305509  , 0.34947182],\n       [0.31445014, 0.37375893, 0.46626187, 0.53174305],\n       [0.48259863, 0.6178953 , 0.72137822, 1.10821872]])bad_sample_1(chain, draw)float640.3661 0.7864 ... 0.1519 0.4031array([[0.36610475, 0.78636751, 0.79897638, 0.15025618],\n       [0.22672028, 0.09945281, 0.19787423, 0.64621791],\n       [0.40832387, 0.21469846, 0.17243945, 0.48192411],\n       [0.73288515, 0.2556624 , 0.15193088, 0.40308445]])\n\n\nAdditionally, if needed, we can specify the thinning factor manually by passing an integer to the argument factor.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "href": "Chapters/MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "title": "4  MCMC Diagnostics",
    "section": "4.5 Diagnosis of gradient-based algorithms",
    "text": "4.5 Diagnosis of gradient-based algorithms\nDue to its internal workings, algorithms like NUTS offer some specific tests that are not available to other methods. These tests are generally very sensitive.\nTo exemplify this we are going to load two InferenceData from pre-calculated models. The details of how these data were generated are not relevant at the moment. We will only say that they are two models that are mathematically equivalent but parameterized in different ways. In this case, the parameterization affects the efficiency of the sampler. The centered model is sampled more efficiently than the non_centered model.\n\nidata_cm = azp.load_arviz_data(\"centered_eight\")\nidata_ncm = azp.load_arviz_data(\"non_centered_eight\")\n\n\n4.5.1 Transition energy vs marginal energy\nWe can think of a Hamiltonian Monte Carlo as a two-step process\n\nDeterministic sampling (following the Hamiltonian)\nA random walk in momentum space\n\nIf the transition energy distribution is similar to the marginal energy distribution, then NUTS can generate samples of the marginal energy distribution that are almost independent between transitions. We can evaluate this visually as shown in Figure 4.12 for the centered_eight model and Figure 4.13 for the non_centered_eight. \n\nazp.plot_energy(idata_cm)\n\n\n\n\n\n\n\nFigure 4.12: Energy plot for centered_eight model.\n\n\n\n\n\n\nazp.plot_energy(idata_ncm)\n\n\n\n\n\n\n\nFigure 4.13: Energy plot for non_centered_eight model.\n\n\n\n\n\n\n\n4.5.2 Divergences\nOne advantage of NUTS is that it fails with style. This happens, for example, when trying to go from regions of low curvature to regions of high curvature. In these cases, the numerical trajectories may diverge. Essentially this happens because in these cases there is no single set of hyper-parameters that allows efficient sampling of both regions. So one region is sampled properly and when the sampler moves to the other region it fails. Divergent numerical trajectories are extremely sensitive identifiers of pathological neighborhoods.\nThe following example shows two things; the non_centered model shows several divergences (red circles) grouped in one region. In the centered model, which has no divergence, you can see that around that same region, there are samples for smaller values of tau. That is to say, the non_centered model fails to sample a region, but at least it warns that it is having problems sampling that region!\n\nfor data in [idata_cm, idata_ncm]:\n    azp.plot_pair(\n        data,\n        var_names=[\"tau\", \"theta\"],\n        coords={\"school\": \"Choate\"},\n        visuals={\"divergence\": True},\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother plot that can be useful to diagnose divergences is azp.plot_parallel(), which shows the samples in parallel coordinates. A parallel coordinates plot is a way to visualize high-dimensional data by plotting each dimension as a vertical line and connecting the points across dimensions with lines.\nFrom Figure 4.14 we can see that the centered model has divergences in the region where tau takes small values.\n\nazp.plot_parallel(idata_cm, var_names=['theta', 'tau'])\n\n\n\n\n\n\n\nFigure 4.14: Parallel coordinates plot for centereed model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "href": "Chapters/MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "title": "4  MCMC Diagnostics",
    "section": "4.6 What to do when the diagnoses are wrong?",
    "text": "4.6 What to do when the diagnoses are wrong?\n\nMore samples or more tuning steps. This is usually only useful when the problems are minor\nBurn-in. Modern software like PyMC uses several samples to tune the hyper-parameters of the sampling methods. By default, these samples are eliminated, so in general, it is not necessary to do Burn-in manually.\nChange sampling method!\nReparameterize the model\nImprove priors\n\nThe folk theorem of computational statistics: When you have computational problems, there is often a problem with your model. The recommendation is NOT to change the priors to improve sampling quality. The recommendation is that if the sampling is bad, perhaps the model is too. In that case, we can think about improving the model, one way to improve it is to use prior knowledge to improve the priors.\n\nSome models can be expressed in more than one way, all mathematically equivalent. In those cases, some parameterizations may be more efficient than others. For example, as we will see later with hierarchical linear models.\nIn the case of divergences, these are usually eliminated by increasing the acceptance rate, for instance in PyMC you can do pm.sample(., target_accept=x) where x is 0.8 by default and the maximum value is 1. If you reach 0.99 you should probably do something else.\nModern probabilistic programming languages, usually provide useful warning messages and tips if they detect issues with sampling, paying attention to those messages can save you a lot of time.\n\n\n\n\n\nBrooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng, eds. 2011. Handbook of Markov Chain Monte Carlo. 1 edition. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/b10905.\n\n\nLink, William A., and Mitchell J. Eaton. 2012. “On Thinning of Chains in MCMC.” Methods in Ecology and Evolution 3 (1): 112–15. https://doi.org/10.1111/j.2041-210X.2011.00131.x.\n\n\nMacEachern, Steven N., and L. Mark Berliner. 1994. “Subsampling the Gibbs Sampler.” The American Statistician 48 (3): 188–90. https://doi.org/10.2307/2684714.\n\n\nSäilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022. “Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison.” Statistics and Computing 32 (2): 32. https://doi.org/10.1007/s11222-022-10090-6.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” https://arxiv.org/abs/1804.06788.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\widehat{R}\\) for Assessing Convergence of MCMC (with Discussion).” Bayesian Analysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_posterior_predictive_checks.html",
    "href": "Chapters/Prior_posterior_predictive_checks.html",
    "title": "5  Prior and Posterior predictive checks",
    "section": "",
    "text": "5.1 Prior predictive checks\nModels are simplifications of reality, sometimes even very crude simplifications. Thus, we can never fully trust them. While hoping they are good enough is an option, we should try to do better. One general approach to criticizing model is to judge them by their predictions. If a model is robust, its predictions should align well with observations, domain knowledge, or some other benchmark. There are at least four avenues to explore:\nAs we can see there are plenty of options to evaluate models. But we still have one additional ingredient to add to the mix, we have omitted the fact that we have different types of predictions. An attractive feature of the Bayesian model is that they are generative. This means that we can simulate synthetic data from models as long as the parameters are assigned a proper probability distribution, computationally we need a distribution from which we can generate random samples. We can take advantage of this feature to check models before or after fitting the data:\nAdditionally, for models like linear regression where we have a set of covariates, we can generate synthetic data evaluated at the observed covariates (our “Xs”) or at different values (“X_new”). If we do the first we call it in-sample predictions, and if we do the second we call it out-of-sample predictions.\nWith so many options we can feel overwhelmed. Which ones we should use will depend on what we want to evaluate. We can use a combination of the previous options to evaluate models for different purposes. In the next sections, we will see how to implement some of these checks.\nThe idea behind prior predictive checks is very general and simple: if a model is good it should be able generate data resembling our prior knowledge. We call these checks, prior predictive because we are generating synthetic data before we have seen the actual data.\nThe general algorithm for prior predictive checks is:\nNotice that in step 4 we use domain knowledge, NOT observed data!\nIn steps 1 and 2 what we are doing is approximating this integral: \\[\np(y^\\ast) = \\int_{\\Theta} p(y^\\ast \\mid \\theta) \\; p(\\theta) \\; d\\theta\n\\]\nwhere \\(y^\\ast\\) represents unobserved but potentially observable data. Notice that to compute \\(y^\\ast\\) we are evaluating the likelihood over all possible values ​​of the prior. Thus we are effectively marginalizing out the values of \\(\\theta\\), the parameters.\nTo exemplify a prior predictive check, let’s try with a super simple example. Let’s say we want to model the height of humans. We know that the heights are positive numbers, so we should use a distribution that assigns zero mass to negative values. But we also know that at least for adults using a normal distribution could be a good approximation. So we create the following model, without too much thought, and then draw 500 samples from the prior predictive distribution.\nThe figure below displays samples from the prior predictive distribution (shown as solid blue lines). To aid interpretation, we have included two reference values: the average length/height of a newborn (approximately 50 cm) and the average height of adult males in the Netherlands (around 182 cm). Reference values are meaningful benchmarks derived from domain knowledge—not from the observed data—and help assess whether predictions are on a reasonable scale. While there are no strict rules for selecting reference values, different analysts might choose different benchmarks based on context\npc = azp.plot_ppc_dist(dt, group=\"prior_predictive\")\nazp.add_lines(pc, values=(50, 182))\n\n\n\n\n\n\n\nFigure 5.1: The prior predictive check for the model of heights. We can see that the bulk of the samples are outside the reference values.\nWe can see that our model is bananas the bulk of the prior predictive distribution is outside of our reference values and the model is predicting values below 0. This is a clear indication that the model is not a good representation of our prior knowledge.\nIn many cases, data will be informative enough to overcome poorly selected priors, but this isn’t guaranteed. To address this, we can tighten our priors. While there’s no universal rule for doing so, a good guideline is to choose priors that concentrate most of the prior predictive distribution’s mass within a plausible range—such as between our reference values.\nSuch priors are often called weakly informative priors. Though not strictly defined, these priors produce a prior predictive distribution with little to no probability mass in unrealistic or impossible regions. For example, a normal distribution with a mean of 160 and a standard deviation of 10 assigns negligible weight to negative values while still accommodating a wide range of plausible heights.\nWe repeat the prior predictive checks with the new prior predictive distribution. We can see that the bulk of the prior predictive distribution is within the reference values.\npc = azp.plot_ppc_dist(dt, group=\"prior_predictive\")\nazp.add_lines(pc, values=(50, 182))\n\n\n\n\n\n\n\nFigure 5.2: The prior predictive check for the model of heights with a more narrower prior than Figure 5.1. Predictions are closer to our domain knowledge about human heights.\nYou are free to pick other priors and other reference values and make new prior predictive checks. Maybe you can use the historical record for the taller and shorter persons in the world as reference values.\nWhen plotting many distributions, where each one spans a narrow range of values compared to the range spanned but the entire collection of distributions, it is usually a good idea to plot the cumulative distribution instead of KDEs, histograms, or quantile dot plots.\npc = azp.plot_ppc_dist(dt, group=\"prior_predictive\", kind=\"ecdf\")\nazp.add_lines(pc, values=(50, 182))\n\n\n\n\n\n\n\nFigure 5.3: The prior predictive check for the model of heights. Same as Figure 5.2 but using empirical CDFs instead of KDEs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prior and Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_posterior_predictive_checks.html#prior-predictive-checks",
    "href": "Chapters/Prior_posterior_predictive_checks.html#prior-predictive-checks",
    "title": "5  Prior and Posterior predictive checks",
    "section": "",
    "text": "Draw \\(N\\) realizations from a prior distribution.\nFor each draw, simulate new data from the likelihood.\nPlot the results.\nUse domain knowledge to assess whether simulated values reflect prior knowledge.\nIf simulated values do not reflect prior knowledge, change the prior distribution, likelihood, or both and repeat the simulation from step 1.\nIf simulated values reflect prior knowledge, compute the posterior.\n\n\n\n\n\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model() as model: \n    # Priors for unknown model parameters\n    mu = pm.Normal('mu', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n    # Likelihood (sampling distribution) of observations\n    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n    # draw 500 samples from the prior predictive\n    dt = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n\n\n\nstan_code = \"\"\"\ndata {\n    int&lt;lower=0&gt; N;\n    array[N] real y;\n}\nparameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n}\nmodel {\n    // Priors\n    mu ~ normal(0, 10);\n    sigma ~ normal(0, 10);\n    \n    // Likelihood\n    y ~ normal(mu, sigma);\n}\ngenerated quantities {\n    real prior_mu = normal_rng(0, 10);\n    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));\n    array[N] real y_prior_pred;\n    for (i in 1:N) {\n        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);\n    }\n}\n\"\"\"\n\nwith open(\"./stan_code.stan\", \"w\") as f:\n    print(stan_code, file=f)\n\n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file=\"./stan_code.stan\")\n\n# Prepare the data\nstan_data = {\n    'N': len(y),\n    'y': y\n}\n\n# Sample from the prior predictive distribution\nprior_samples = model.sample(\n    eata=stan_data,\n    fixed_param=True,\n    iter_sampling=500,\n    seed=SEED\n)\n\n# Convert to ArviZ\ndt = azp.from_cmdstanpy(\n    prior=prior_samples,\n    prior_predictive=\"y_prior_pred\",\n    observed_data={\"y\": y},\n)\n\n\n\n\n\n\n\n\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model() as model: \n    # Priors for unknown model parameters\n    mu = pm.Normal('mu', mu=160, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n    # Likelihood (sampling distribution) of observations\n    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n    # draw 500 samples from the prior predictive\n    dt = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n\n\n\nstan_code = \"\"\"\ndata {\n    int&lt;lower=0&gt; N;\n    array[N] real y;\n}\nparameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n}\nmodel {\n    // Priors\n    mu ~ normal(160, 10);\n    sigma ~ normal(0, 10);\n    \n    // Likelihood\n    y ~ normal(mu, sigma);\n}\ngenerated quantities {\n    real prior_mu = normal_rng(160, 10);\n    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));\n    array[N] real y_prior_pred;\n    for (i in 1:N) {\n        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);\n    }\n}\n\"\"\"\n\nwith open(\"./stan_code.stan\", \"w\") as f:\n    print(stan_code, file=f)\n\n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file=\"./stan_code.stan\")\n\n# Prepare the data\nstan_data = {\n    'N': len(y),\n    'y': y\n}\n\n# Sample from the prior predictive distribution\nprior_samples = model.sample(\n    data=stan_data,\n    fixed_param=True,  # Sample from priors only\n    eter_sampling=500,\n    seed=SEED\n)\n\n# Convert to ArviZ\ndt = azp.from_cmdstanpy(\n    prior=prior_samples,\n    prior_predictive=\"y_prior_pred\",\n    observed_data={\"y\": y},\n)\n\n\n\n\n\n\n\n\n\n5.1.1 A final note about priors\nBefore moving on to the next section, we would like to share one last thought on priors. If you have access to reliable prior knowledge, you should use it, there’s no good reason to discard valid information. But in many real-world scenarios, turning that knowledge into informative priors often require considerable effort and time. And in some cases, they may lead to results that are nearly indistinguishable from those produced using less carefully chosen priors.\nIn practice weakly informative priors can offer meaningful advantages over both vague and informative priors. Even a modest amount of prior information is often better than none at all, as it helps guard against implausible or misleading results and it could provide computational benefits, such as improved sampling efficiency while being usually easier and less time-consuming to elicit than fully informative priors.\nFinally, one benefit that’s often underappreciated is that running prior predictive checks and playing around with different priors can give you valuable insights into your model and the problem you’re trying to solve, regardless of their impact on their direct impact on the posterior. To learn more about prior elicitation, check out Chapter 12.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prior and Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_posterior_predictive_checks.html#posterior-predictive-checks",
    "href": "Chapters/Prior_posterior_predictive_checks.html#posterior-predictive-checks",
    "title": "5  Prior and Posterior predictive checks",
    "section": "5.2 Posterior predictive checks",
    "text": "5.2 Posterior predictive checks\nThe idea behind posterior predictive checks is very general and simple: if a model is good it should be able generate data resembling the observed data (Gelman et al. 2013). We call these checks, posterior predictive because we are generating synthetic data after seeing the data.\nThe general algorithm for posterior predictive checks is:\n\nDraw \\(N\\) realizations from the posterior distribution.\nFor each draw, simulate new data from the likelihood.\nPlot the results.\nUse observed data to assess whether simulated values agree with observed values.\nIf simulated values do not agree with observations, change the prior distribution, likelihood, or both and repeat the simulation from step 1.\nIf simulated values reflect prior knowledge, compute the posterior.\n\nNotice that in contrast with prior predictive checks, we use observations here. Of course, we can also include domain knowledge to assess whether the simulated values are reasonable, but because we are using observations we do more stringent evaluations.\nIn steps 1 and 2 what we are doing is approximating this integral: \\[\np(\\tilde y) = \\int_{\\Theta} p(\\tilde y \\mid \\theta) \\; p(\\theta \\mid y) \\; d\\theta\n\\]\nwhere \\(\\tilde y\\) represents new observations, according to our model. The data generated is predictive since it is the data that the model expects to see.\nNotice that what we are doing is marginalizing the likelihood by integrating all possible values ​​of the posterior. Therefore, from the perspective of our model, we are describing the marginal distribution of data, that is, regardless of the values of the parameters.\nContinuing with our height example, we can generate synthetic data from the posterior predictive distribution.\n\nPyMCCmdStanPy\n\n\n\nwith model: \n    dt = pm.sample(idata_kwargs={\"log_likelihood\": True}, random_seed=SEED)\n    pm.sample_posterior_predictive(dt, random_seed=SEED, extend_inferencedata=True)\n\n\n\nstan_code = \"\"\"\ndata {\n    int&lt;lower=0&gt; N;\n    array[N] real y;\n}\nparameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n}\nmodel {\n    // Priors\n    mu ~ normal(0, 10);\n    sigma ~ normal(0, 10);\n    \n    // Likelihood\n    y ~ normal(mu, sigma);\n}\ngenerated quantities {\n    real prior_mu = normal_rng(0, 10);\n    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));\n    array[N] real y_prior_pred;\n    for (i in 1:N) {\n        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);\n    }\n\n    array[N] real log_lik;\n    for (i in 1:N) {\n        log_lik[i] = normal_lpdf(y[i] | mu, sigma);\n    }\n\n    array[N] real y_rep;\n    for (i in 1:N) {\n        y_rep[i] = normal_rng(mu, sigma);\n    }\n}\n\"\"\"\n\nwith open(\"./stan_code.stan\", \"w\") as f:\n    print(stan_code, file=f)\n\n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file=\"./stan_code.stan\")\n\n\n# Prepare the data\nstan_data = {\n    'N': len(y),\n    'y': y\n}\n\n\nprior_samples = model.sample(\n    data=stan_data,\n    fixed_param=True,  # Sample from priors only\n    iter_sampling=500,\n    seed=SEED\n)\n\nposterior = model.sample(data=stan_data)\n\n# Convert to ArviZ\ndt = azp.from_cmdstanpy(\n    posterior=posterior,\n    prior=prior_samples,\n    prior_predictive={\"y\":\"y_prior_pred\"},\n    posterior_predictive={\"y\": \"y_rep\"},\n    log_likelihood={\"y\": \"log_lik\"},\n    observed_data={\"y\": y},\n)\n\n\n\nAnd then we use ArviZ to render the comparison. We can see that the model is doing a good job at predicting the data. The observed data (black line) is within the bulk of the posterior predictive distribution (blue lines).\n\nazp.plot_ppc_dist(dt, num_samples=200);\n\n\n\n\n\n\n\nFigure 5.4: Posterior predictive check for the model of heights.\n\n\n\n\n\nOther common visualizations to compare observed and predictive values are empirical CDFs, histograms and less often quantile dotplots. Like with other types of visualizations, you may want to try different options, to be sure visualizations are not misleading and you may also want to adapt the visualization to your audience.\n\n5.2.1 Using summary statistics\nBesides directly comparing observations and predictions in terms of their densities, we can do comparisons in terms of summary statistics, like the median, the interquartile range, the standard deviation etc. Which ones we decide to use can vary from one data-analysis problem to another, and ideally they should be informed by the data-analysis goals. As in posterior predictive checks we use the data twice, first for fitting the model and then for checking it. It is advisable to select test statistics that are orthogonal to the model parameters (Gabry et al. 2019). For example, in a Normal model with a location parameter, the mean should be easy to recover, so a posterior predictive check using the mean as a test statistic would not be a particularly stringent test. As in many common models there is a location parameter, then the mean is usually not a good test statistic.\nWe can use the plot_ppc_tstat function to display the posterior predictive distribution of a test statistic. The function takes as input a DataTree with a posterior_predictive and observed_data groups and a name of the test statistic (custom function are also allowed), and it will compute the posterior predictive distribution of that statistic. The function also computes the observed value of the test statistic and plots it as a dot at the bottom of each subplot.\nThe following figure shows a comparison in terms of the mean, median and interquartile range (IQR). The dots at the bottom of each subplots corresponds to the summary statistics computed for the observed data and the KDE is for the model’s predictions.\n\npc = azp.combine_plots(dt,\n                 plots=[\n                  (azp.plot_ppc_tstat, {\"t_stat\":\"median\"}),\n                  (azp.plot_ppc_tstat, {\"t_stat\":\"mad\"}),\n                  (azp.plot_ppc_tstat, {\"t_stat\":\"iqr\"}),                   \n                 ],\n                group=\"posterior_predictive\",\n)\n\n\n\n\n\n\n\nFigure 5.5: Posterior predictive check for the model of heights using summary statistics.\n\n\n\n\n\nIf we want a numerical summary of a posterior predictive checks using test statistics we can compute the proportion of simulated data that is less than or equal to the observed data: \\[\np(T_{\\text{sim}} \\le T_{\\text{obs}} \\mid \\tilde y)\n\\]\nWhere \\(T\\) is the summary statistic of our choice, computed for both the observed data \\(T_{\\text{obs}}\\) and the simulated data \\(T_{\\text{sim}}\\).\nThis is known as a posterior predictive p-value (or Bayesian p-value) (Gelman et al. 2013). This is similar to the frequentist p-value, but computed with respect to the posterior predictive distribution, instead of the sample distribution under the null hypothesis.\nA posterior predictive p-value of 0.5 indicates that half of the predictions are below the observed values and half above. Posterior predictive p-values do not in general have uniform distributions under the null hypothesis but instead tend to have distributions more concentrated near 0.5 (Gelman 2013). For instance, we already mentioned that the mean is easy to recover for many models and thus the posterior predictive p-value for the mean is often concentrated around 0.5.\nThe term “Bayesian p-values” may sound like an oxymoron or paradoxical (Meng 1994). The Bayesian p-values are defined similar to their frequentist cousins and hence the name. But they are used in a very different way. We use posterior predictive p-values as a diagnostic tool to asses potential mismatches between model and data rather than as a measure of “statistical significance” or as a dichotomy decision tool. The null hypothesis is that the predictions from the model and the observed data are drawn from the same data-generating process, but in practice we are not interested in rejecting this hypothesis. We already know is not true! Instead, we are interested in understanding how well the model is doing at predicting the data, detecting potential problems, an if possible or desirable improving the model.\n\n\n5.2.2 PIT-ECDFs\nInstead of using a summary statistics, as before, we can directly compare observations and predictions by computing: \\[\np(\\tilde y_i \\le y_i \\mid y)\n\\]\nThis is often called the marginal p-value and the ideal distribution is the standard uniform distribution. The intuition is that if the model can generate predictions from the same distribution as the observed data, then the observed data can be thought of as just one random sample from the posterior predictive distribution. In this case, the observed data point is equally likely to appear anywhere within the range of the predicted values. This means there’s no systematic bias in where the observation falls, and the p-values derived from comparing the observed data to the predictions will be uniformly distributed.\nA more formal justification for this result is provided by the Probability Integral Transform (PIT). This property, also known as the universality of the Uniform distribution, states that if \\(Y\\) is a random variable with a continuous distribution and cumulative distribution function (CDF) \\(F_Y\\), then the transformed variable\n\\[\nU = F_Y(Y)\n\\]\nfollows a standard Uniform distribution. A proof of this result can be found in The Book of Statistical Proofs (Soch et al. 2024).\nIn other words if we apply the CDF of any continuous distribution to a random variable with that distribution, the result will be a random variable with a standard uniform distribution. This is a very powerful result, as it allows us to use the standard uniform distribution as a reference distribution for many statistical tests, including posterior predictive checks.\nAs mentioned earlier, the marginal p-value is given by\n\\[\np(\\tilde y_i \\leq y_i \\mid y).\n\\]\nIf the observed data and predictions are drawn from the same distribution, this expression is then equivalent to the definition of the CDF:\n\\[\nF_Y(y) = \\mathrm{Pr}(Y \\leq y).\n\\]\nThus, we can see the computation of the marginal p-value as an application of the Probability Integral Transform.\nIn practice we don’t have the CDF, but this is no problem as we have samples from the posterior predictive and hence we can compute the empirical CDF (ECDF). The CDF of the standard Uniform distribution is a diagonal line that goes from (0, 0) to (1,1), as shown in Figure 5.6. Deviations from this line may indicate problems with the model. This is a very simple to interpret visualization.\n\npz.Uniform(0, 1).plot_cdf()\n\n\n\n\n\n\n\nFigure 5.6: The CDF of the standard Uniform distribution.\n\n\n\n\n\nThe disadvantage of such visualization is that all the “action” is close to the diagonal line and most of the plot is just blank space. A simple trick to improve the data-ink ratio is to render the difference between the observed and expected cumulative distribution functions, the \\(\\Delta\\)-ECDF, as shown in Figure 5.7. The last ingredient to improve this visual diagnostic is to add a confidence band. Due to finite sample size we should expect deviations from uniformity, so a confidence band gives us an idea of how much deviation is expected by chance.\n\nazp.plot_ppc_pit(dt);\n\n\n\n\n\n\n\nFigure 5.7: Posterior predictive check for the model of heights using marginal Bayesian p-values, also know as u-values.\n\n\n\n\n\nIn ArviZ, we use the simultaneous confidence bands described by Säilynoja, Bürkner, and Vehtari (2022). The simultaneous confidence bands take into account the probability of observing deviations of the entire curve, as opposed to independent pointwise deviations. The band or envelope has an oval shape because the probability of observing a deviation is null at 0 and 1, all ECDFs must start at 0 and end at 1, and is higher in the middle of the curve.\nTo build intuition on how to interpret the PIT-ECDF plots we are going to explore four common patterns using synthetic data. The following three figures show four different scenarios, where the observed data follows a standard normal distribution (\\(\\mu=0, \\sigma^2=1\\)). In each case, we compare the observed data to predictions where:\n\nThe mean of the predictions is shifted to the right. The model is overpredicting the data.\nThe mean of the predictions is shifted to the left. The model is underpredicting the data.\nThe predictions have a wider spread. The predictions are too uncertain.\nThe predictions have a narrower spread. The predictions are too certain.\n\nFirst we show the KDEs of the observed data and the predictions.\n\nobserved = pz.Normal(0, 1).rvs(500)\n\npredictions = {}\nfor i, (mu, sigma) in enumerate([\n                                (0.5, 1),  # shifted to the right\n                                (-0.5, 1), # shifted to the left\n                                (0, 2),    # wider \n                                (0, 0.5),  # narrower\n                                ]):\n    predictions[f\"y{i}\"] =  pz.Normal(mu, sigma).rvs((4, 500, 100))\n\ndt_i = azp.from_dict({\n    \"posterior_predictive\":predictions,\n    \"observed_data\": {f\"y{i}\": observed for i in range(len(predictions))}\n})\n\nazp.plot_ppc_dist(dt_i,\n                  kind=\"kde\",  \n                  visuals={\"remove_axis\":False},\n                  figure_kwargs={\"sharey\":True},             \n                 );\n\n\n\n\n\n\n\nFigure 5.8: Posterior predictive check with KDEs showing four alternative scenarios.\n\n\n\n\n\nThen we show the ECDFs of the observed data and the predictions.\n\nazp.plot_ppc_dist(dt_i,\n                  kind=\"ecdf\",\n                  figure_kwargs={\"sharey\":True},        \n                 );\n\n\n\n\n\n\n\nFigure 5.9: Posterior predictive check with ECDFs showing four alternative scenarios.\n\n\n\n\n\nFinally, we show the PIT-ECDFs.\n\nazp.plot_ppc_pit(dt_i,\n                 visuals={\"ylabel\":False},\n                 figure_kwargs={\"sharey\":True},        \n                 );\n\n\n\n\n\n\n\nFigure 5.10: Posterior predictive check with PIT-ECDFs showing four alternative scenarios.\n\n\n\n\n\n\n\n5.2.3 Coverage\nThe coverage is the proportion of true values that fall within a given prediction interval. For a well-calibrated model, the coverage should match the intended interval width. For example, a 95% credible interval should contain the true value 95% of the time.\nFor equal-tailed intervals (ETI), the coverage can be obtained by transforming the PIT values, we just need to replace the PIT with two times the absolute difference between the PIT values and 0.5. As with the PIT-ECDFs we saw in the previous section, for a well calibrated model, we should expect the coverage to be uniform and within the confidence envelope.\nUsing ArviZ we can visualize the ETI coverage by setting coverage=True in the plot_ppc_pit function.\n\nazp.plot_ppc_pit(dt_i,\n                 coverage=True,\n                 visuals={\"ylabel\":False},\n                 figure_kwargs={\"sharey\":True},\n                 );\n\n\n\n\n\n\n\nFigure 5.11: Coverage check showing four alternative scenarios.\n\n\n\n\n\n\nIf the difference is positive, the model is under-confident: the predictions have a wider spread than the data – they are too uncertain.\nIf the difference is negative, the model is over-confident: the predictions have a narrower spread than the data – they are too certain.\n\n\n\n5.2.4 Avoiding double-dipping\nSo far we have being using the data twice, first to fit the model and then to evaluate it. This is a common practice in Bayesian data analysis and it is not a problem as long as we are aware of it. The main goal is to understand how well the model is doing at predicting the data, detecting potential problems, and if possible or desirable improving the model (Gelman et al. 2013).\nStill, we may want to avoid double-dipping. So instead of computing:\n\\[\np(\\tilde y_i \\leq y_i \\mid y)\n\\]\nWe may want to compute:\n\\[\np(\\tilde y_i \\leq y_i \\mid y_{-i})\n\\]\nwhere \\(y_{-i}\\) is the observed data without the \\(i\\)-th observation.\nThis is a more stringent test, as we are not using the \\(i\\)-th observation to compute the posterior predictive distribution. This is known as the leave-one-out cross-validation (LOO-CV) and it is a very popular method to assess the predictive performance of a model.\nIn principle computing this will be too costly, as we need to compute the posterior predictive distribution \\(n\\) times, where \\(n\\) is the number of observations. However, we can use a method called Pareto-smoothed importance sampling (PSIS) to approximate the LOO-CV from a single posterior computation. This is a topic we will discuss in more detail in Chapter 7. ArviZ offers many functions based on this method, one of them is loo_pit.\n\nazp.plot_loo_pit(dt);\n\n\n\n\n\n\n\nFigure 5.12: Posterior predictive check with LOO-PIT-ECDF.\n\n\n\n\n\n\n\n5.2.5 Hypothetical Outcome Plots\nAnother strategy that can be useful for posterior predictive plots is to use animations. For example instead of showing all draws from the posterior predictive at the same time Hypothetical Outcome Plots (HOPs) shows them animated with one just a few draws per frame. HOPs enable a user to experience uncertainty in terms of countable events, just like we experience probability in our day to day lives. You can read more about HOPs here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prior and Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_posterior_predictive_checks.html#posterior-predictive-checks-for-discrete-data",
    "href": "Chapters/Prior_posterior_predictive_checks.html#posterior-predictive-checks-for-discrete-data",
    "title": "5  Prior and Posterior predictive checks",
    "section": "5.3 Posterior predictive checks for discrete data",
    "text": "5.3 Posterior predictive checks for discrete data\nSo far we have show examples with continuous data. Many of the tools can still be used for discrete data, while KDEs are not useful for discrete data (unless the number of discrete values is large enough to assume continuity), histograms with properly specified bins (like one bin per discrete value) and ECDFs can be used for discrete data. Still, there are some posterior predictive plots that has been specifically designed for discrete data. In the next sections we discuss some of them.\n\n5.3.1 Posterior predictive checks for count data\nCount data is a type of discrete data that is very common in many fields. For instance, the number of iguanas per square meter in a rainforest, the number of bikes in a bike-sharing station, the number of calls to a call center, the number of emails you got last year, etc. When assessing the fit of a model to count data we need to consider the discreetness of the data and that we usually care about the amount of (over/under-)dispersion.\nRootograms are a graphical tool to assess the fit of count data models (Tukey 1977; Kleiber and Zeileis 2016). There are a few variations of rootograms, but traditionally rootograms use bars for the predicted data, and lines (plus markers) for the observed data. Finally, instead of plotting the raw data, they show the square root of the observed and predicted counts, which explain the name of the plots. The reason to square root the data is to make easier to compare observed and expected frequencies even for low frequencies. Often the uncertainty in the predictions is omitted.\nHere we are going to discuss the rootograms presented by Säilynoja et al. (2025). These rootograms emphasises the discreteness of the data and and predictions by using points. The uncertainty in the predictions is encoded using intervals. Instead of square-rooting the data, it set the y-axis on the square root scale, this makes easier to interpret the data, because we can directly read the frecuencies from the y-axis (instead of reading the square roots) while keeping the advantage of being able to discriminate details at lower frecuencies.\nTo illustrate rootograms we are going to use the Horseshoe crabs dataset (Brockmann 1996). Very briefly, horseshoe crabs arrive at the beach in pairs for their spawning ritual. Solitary males gather around the nesting couples and vying to fertilize the eggs. These individuals, known as satellite males, often congregate near certain nesting pairs while disregarding others. We used Bambi to create two models a poisson model and a hurdle-negative binomial model for the number of male satellites as a function of the carapace width and color of the female.\nWe are going to omit the modelling details, and just upload prefitted models.\n\ncrabs_poisson = azp.load_arviz_data('crabs_poisson')\ncrabs_hurdle_nb = azp.load_arviz_data('crabs_hurdle_nb')\n\nLet’s first check the Poisson model. We can see that the overall fit is not that bad, but the zeros are underpredicted, and counts 1 to 4 are overpredicted. Most counts from 6 onward are also underpredicted. This pattern is an indication of overdispersion in the data, and the huge difference for 0 indicates an excess of zeros.\n\npc = azp.plot_ppc_rootogram(crabs_poisson)\npc.viz.plot[\"satellite\"].item().set_xlim(-0.5, 20)\n\n\n\n\n\n\n\nFigure 5.13: Rootogram showing the uncertainty in the predictions for a Poisson model.\n\n\n\n\n\nNow we will check the fit for the hurdle model. As expected for a hurdle model we get a perfect fit for the zeros. For the positive values, we still get some deviations, but the fit is better than with the Poisson model.\n\npc = azp.plot_ppc_rootogram(crabs_hurdle_nb)\npc.viz.plot[\"satellite\"].item().set_xlim(-0.5, 20)\n\n\n\n\n\n\n\nFigure 5.14: Rootogram showing the uncertainty in the predictions for a Hurdle Negative Binomial model.\n\n\n\n\n\nBoth models predict more values in the tail than observed, even if with low probability. For both plots, we restrict the x-range to (0, 20).\n\n\n5.3.2 Posterior predictive checks for binary data\nBinary data is a common form of discrete data, often used to represent outcomes like yes/no, success/failure, or 0/1. We may be tempted to asses the fit of a binary model using a bar plot, or a plot similar to the rootogram we showed in the previous section, but this is not a good idea. The reason is that even a very simple model with one parameter for the proportion of one class (like an intercept), can perfectly model that proportion (Säilynoja et al. 2025). Then a bar plot will not be able to tell much about the quality of our model.\nOne solution to this challenge is to use the so call calibration or reliability plots. To create this kind of plot we first bin the predicted probabilities (e.g., [0.0–0.1], [0.1–0.2], …, [0.9–1.0]) and then for each bin we compute the fraction of observed positive outcomes. In this way we can compare the predicted probabilities to the observed frequencies per bin. The ideal calibration plot is a diagonal line, where the predicted probabilities are equal to the observed frequencies. The problem with this approach is that in practice we don’t have good rules to select the bins and different bins can result in plots that look drastically different.\nA more robust and simple to use method, that does not rely on binning the data has been proposed by Dimitriadis, Gneiting, and Jordan (2021). Figure 5.15 shows one example of this method. As previously mentioned, the ideal calibration plot is a diagonal line, where the predicted probabilities are equal to the observed frequencies. If the line is above the diagonal, the model is underestimating the probabilities, and if the line is below the diagonal, the model is overestimating the probabilities.The confidence bands are computed using the method proposed by Dimitriadis, Gneiting, and Jordan (2021).\n\ndt = azp.load_arviz_data('anes')\n\nazp.plot_ppc_pava(dt)\n\n\n\n\n\n\n\nFigure 5.15: PAV-adjusted calibration plot for a logistic regression model.\n\n\n\n\n\nThe y-axis of Figure 5.15 is labeled as CEP, short for “conditional event probabilities.” A CEP represents the probability that an event occurs, given that the classifier assigned a particular predicted probability. These probabilities are computed using the pool adjacent violators algorithm (Ayer et al. 1955), also known as the PAV-adjusted method—hence the name of the corresponding function in ArviZ. This algorithm ensures that CEPs are monotonic: they either increase or remain constant as the predicted probabilities increase, but never decrease. This monotonicity assumption is reasonable for calibrated models, where higher predicted probabilities should correspond to higher actual event probabilities.\n\n\n5.3.3 Posterior predictive checks for categorical data\nA categorical variable can take on one of a limited, and usually fixed, number of possible values. For categorical data, we can use the same approach used for binary data if we follow a “one vs others” strategy. That is, we select one category and we compare it to all the other categories. This will give as many plots as categories, which can be fine when the number of categories is relatively low, but unwieldy otherwise. In those other cases, we can first use a confusion matrix or some scalar miscallibration summary per category to help us filter out the cases that we want to inspect visually (Säilynoja et al. 2025).\nWe can use plot_ppc_pava for categorical data, but not directly. We need to tell it that we have categorical data by passing data_type=\"categorical\".\n\nazp.plot_ppc_pava(dt_cat, data_type=\"categorical\")\n\n\n\n\n\n\n\nFigure 5.16: PAV-adjusted calibration plot for a categorical regression model.\n\n\n\n\n\n\n\n5.3.4 Posterior predictive checks for ordinal data\nOrdinal data is a categorical, statistical data type where the variables have natural, ordered categories and the distances between the categories are not known. For ordinal data, we must account for the order when evaluating the calibration. Then we can compute the cumulative conditional event probabilities (Säilynoja et al. 2025), which will get us one plot less than the number of categories.\nWith ordinal data, we can not directly use plot_ppc_pava; we have to pass data_type=\"ordinal\".\n\nazp.plot_ppc_pava(dt_ord, data_type=\"ordinal\")\n\n\n\n\n\n\n\nFigure 5.17: PAV-adjusted calibration plot for an ordinal regression model.\n\n\n\n\n\n\n\n5.3.5 Posterior predictive checks for censored data\nSurvival models, also known as time-to-event models, are statistical methods used to study the time until a specific event occurs. A key challenge in survival analysis is properly accounting for the fact that only some events of interest are observed, while others occur outside the observation period.\nTo illustrate, imagine analyzing a group of recent graduates who all start searching for jobs at the same time. After three months, half of them have found employment. Using that data, we want to estimate the average time it takes to find a job. If we compute the estimate using only those who have already found jobs and discard those still searching, our estimate will be biased, as we would incorrectly conclude that everyone finds a job within three months or less. Instead, we need to account for the graduates who haven’t yet found jobs (but eventually will), as this correctly reflects that some people take longer than three months to get hired.\nFor a full example of how to work with this type of data, you can check Bambi’s documentation. Even after correctly modeling this data, we still have a challenge when doing posterior predictive checks with censored data. Comparing predictions directly to the raw observed data can be misleading. Instead, we replace the raw observations with Kaplan–Meier estimates; a simple, non-parametric method for estimating the probability of survival (in this case, still searching for a job) over time.\nThe following example is from a model of cat adoption. We record how many months pass until each cat is adopted, but at the time of data collection, some cats have not yet been adopted.\n\ndt_cens = azp.load_arviz_data('censored_cats')\nazp.plot_ppc_censored(dt_cens)\n\n\n\n\n\n\n\nFigure 5.18: Posterior predictive check for survival/censored data using Kaplan-Meier curves.\n\n\n\n\n\nTo keep predictions within a meaningful range ArviZ limits the predictive data relative to the maximum observed duration, preventing the survival curves from extending too far beyond the observed data. You can change this with the argument extrapolation_factor, which defaults to 1.2, meaning up to a 20% larger that the maximum observation. For more recommendations on how to perform predictive assessment for survival model you can read (Suorsa and Vehtari 2026).\n\n\n\n\nAyer, Miriam, H. D. Brunk, G. M. Ewing, W. T. Reid, and Edward Silverman. 1955. “An Empirical Distribution Function for Sampling with Incomplete Information.” The Annals of Mathematical Statistics 26 (4): 641–47. https://doi.org/10.1214/aoms/1177728423.\n\n\nBrockmann, H. Jane. 1996. “Satellite Male Groups in Horseshoe Crabs, Limulus Polyphemus.” Ethology 102 (1): 1–21. https://doi.org/10.1111/j.1439-0310.1996.tb01099.x.\n\n\nDimitriadis, Timo, Tilmann Gneiting, and Alexander I. Jordan. 2021. “Stable Reliability Diagrams for Probabilistic Classifiers.” Proceedings of the National Academy of Sciences 118 (8): e2016191118. https://doi.org/10.1073/pnas.2016191118.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society Series A: Statistics in Society 182 (2): 389–402. https://doi.org/10.1111/rssa.12378.\n\n\nGelman, Andrew. 2013. “Two simple examples for understanding posterior p-values whose distributions are far from uniform.” Electronic Journal of Statistics 7 (none): 2595–2602. https://doi.org/10.1214/13-EJS854.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. Boca Raton. https://doi.org/10.1201/b16018.\n\n\nKleiber, Christian, and Achim Zeileis. 2016. “Visualizing Count Data Regressions Using Rootograms.” The American Statistician 70 (3): 296–303. https://doi.org/10.1080/00031305.2016.1173590.\n\n\nMeng, Xiao-Li. 1994. “Posterior Predictive \\(p\\)-Values.” The Annals of Statistics 22 (3): 1142–60. https://doi.org/10.1214/aos/1176325622.\n\n\nSäilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022. “Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison.” Statistics and Computing 32 (2): 32. https://doi.org/10.1007/s11222-022-10090-6.\n\n\nSäilynoja, Teemu, Andrew R. Johnson, Osvaldo A. Martin, and Aki Vehtari. 2025. “Recommendations for Visual Predictive Checks in Bayesian Workflow.” https://arxiv.org/abs/2503.01509.\n\n\nSoch, Joram, Thomas J Faulkenberry, Kenneth Petrykowski, and Carsten Allefeld. 2024. “The Book of Statistical Proofs.” https://doi.org/10.5281/ZENODO.4305949.\n\n\nSuorsa, Saku, and Aki Vehtari. 2026. “Predictive Assessment and Comparison of Bayesian Survival Models for Cancer Recurrence.” https://arxiv.org/abs/2601.01662.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. 1 edition. Pearson.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prior and Posterior predictive checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html",
    "href": "Chapters/Sensitivity_checks.html",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "",
    "text": "6.1 Prior and likelihood sensitivity via power-scaling\nDetermining the sensitivity of the posterior to perturbations of the prior is an important part of building Bayesian models. The traditional way to assess the prior sensitivity is to compare the posterior obtained using a reference prior against the posterior obtained using one or more alternative priors. The reference prior can be the default prior in packages like Bambi or some other “template” prior from literature or previous analysis of a similar dataset/problem. But it can also be the prior obtained after a more careful elicitation process. The “alternative” priors can be prior that for some reason we also consider relevant, or deviations from the reference prior that we consider important to evaluate.\nTo perform the comparison we can use visual and numerical summaries as we already explained in Chapter 5. For instance, when working with ArviZ, we can use plot_forest or plot_density functions to compare multiple posteriors (or actually their marginals) in the same plot. Ideally, we should summarize and report the results of this analysis, so that others can also understand how robust the model is to different prior choices.\nThe same procedure we use to assess the sensitivity of the prior can be used to compare how different likelihoods affect the posterior. So we can discuss the sensitivity of the posterior to the likelihood, the prior or both.\nIn summary, the traditional approach to assess prior and likelihood sensitivity is to fit multiple models with different priors and/or likelihoods, and then compare the results. This approach is simple to understand, and should be straightforward to implement. However, in practice thinking about alternative priors, building alternative models and fitting them can be very time-consuming. In the chapter we discuss a more automated way to do this kind of analysis.\nThe method we will discuss was presented by Kallioinen et al. (2023), and offers visual and numerical diagnostics that can alert us of potential prior-data conflict or likelihood noninformativity. What makes this approach very practical is that we only need to fit a single model once, and without explicitly modifying the prior or likelihood, we can assess the effect of changing them. If you are eager to see how this method can be applied in practice, you can jump to the example section. If you want to understand the method in more detail, keep reading.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html#prior-and-likelihood-sensitivity-via-power-scaling",
    "href": "Chapters/Sensitivity_checks.html#prior-and-likelihood-sensitivity-via-power-scaling",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "",
    "text": "6.1.1 Power-scaling\nA key idea of the method is the concept of power-scaling. By power-scaling we mean raising a distribution to a power \\(\\alpha\\). Why is this interesting? Because by power-scaling a distribution we can “stretch” it or “compress” it. Figure 6.1 shows the effect of power-scaling on a Beta and a Normal distribution. Power-scaling is a very general transformation that work for any distribution, except for the uniform distribution. Notice this restriction also applies to a Beta(1, 1), which is uniform. This is a minor restriction, because the uniform distribution is hardly a useful prior, unless we are dealing with a very special case.\n\n\n\n\n\n\n\n\nFigure 6.1: The effect of power-scaling on Beta and Normal distributions. α values larger than 1 ‘compress’ the distribution while values of α below 1 ‘stretch’ the distribution.\n\n\n\n\n\nIn the context of Bayes Theorem we can use power-scaling to modulate the relative weights of the likelihood and prior. For instance we can write a power-scaled version of Bayes’ theorem as follows:\n\\[\np(\\theta \\mid y)_{\\alpha} \\propto p(y \\mid \\theta)^\\alpha \\; p(\\theta)\n\\]\nLet’s take a moment to understand this expression. If we set \\(\\alpha=1\\) we recover the usual expression for Bayes’ theorem, the one that you may have seen many times. Setting \\(\\alpha=0\\) is equivalent to ignoring the likelihood, thus \\(p(\\theta \\mid y)_{\\alpha}\\) will be equal to the prior. We can then conclude that any number between 0 and 1 will have the effect of “weakening” the likelihood with respect to the prior. By the same token any number greater than 1 will “strength” the likelihood with respect to the prior.\nWe can repeat this exercise for the prior and then we will be able to modulate the relative weights of the prior relative to the likelihood. Figure 6.2 shows the effect on the posterior of power-scaling the prior for a fixed likelihood. The prior is a Beta distribution with parameters \\(a=2\\) and \\(b=5\\) and the likelihood is a Binomial distribution with parameters \\(n=10\\) and \\(p=0.7\\). The posterior is computed using the prior and likelihood.\n\n\n\n\n\n\n\n\nFigure 6.2: The effect on the posterior when power-scaling the prior for a fixed likelihood.\n\n\n\n\n\n\n\n6.1.2 Power-scaling and importance sampling\nWe can analytically compute the effect of power scaling a distribution. For instance, for a Normal distribution, \\(\\mathcal{N}(\\theta \\mid \\mu, \\sigma)\\) and power-scaling factor \\(\\alpha\\) we get \\(\\mathcal{N}(\\theta \\mid \\mu, \\sigma \\; \\alpha^{-1/2})\\). In practice we usually work with MCMC samples and arbitrary distributions, thus a more convenient way to perform power-scaling is to use importance sampling. As discussed in Chapter 7 importance sampling is a method to estimate the expected value of a function under a distribution different from the one we have samples from. Kallioinen et al. (2023) proposed to use Pareto smoothed importance sampling (PSIS), a method we also discussed in Chapter 7 to estimate the ELPD.\nIn the context of power-scaling perturbations, the proposal distribution is the posterior we computed using MCMC (\\(p(\\theta \\mid y)\\)) and the target distribution is the perturbed posterior (\\(p(\\theta \\mid y)_{\\alpha}\\)). If the proposal and target distributions are expressed as the products of the prior \\(p(\\theta)\\) and likelihood \\(p(y \\mid \\theta)\\), with the prior raised to the power of \\(\\alpha\\), then we can write the importance weights for the prior as:\n\\[\nw_{\\alpha} = \\frac{p(\\theta)^{\\alpha} \\; p(y \\mid \\theta)}{p(\\theta) \\;\\; p(y \\mid \\theta)}\n\\]\nWhich simplifies to:\n\\[\nw_{\\alpha} = \\frac{p(\\theta)^{\\alpha}}{p(\\theta)}\n\\]\nwhich is the same as:\n\\[\nw_{\\alpha} = p(\\theta)^{\\alpha-1}\n\\]\nThis resulta means that the importance sampling weights for the prior only depend on the density of the prior being power-scaled. This simplifies the computation of the weights. By the same logic the same applies to the likelihood.\nTo summarize, the method proposed by Kallioinen et al. (2023) combines two key ideas, importance sampling and power-scaling. This combination allow us to approximate the sensitivity of the posterior to the prior and likelihood without the need to explicitly build and fit multiple models.\n\n\n6.1.3 Diagnosing sensitivity\nOnce we have power-scaled the prior and likelihood we can visually inspect the effect of the perturbations, by plotting the marginal posterior distributions for the parameter of interest. KDEs, ECDFs and point-intervals allows easy comparison of several distributions. If the perturbations are small, the distributions should overlap. If the perturbations are large, the distributions will be separated.\nWe can also compute numerical values based on distances (or divergences) between the unperturbed and perturbed posteriors. In Chapter 7 we discussed the use of the Kullback-Leibler divergence to compare how similar two distributions are. Kallioinen et al. (2023) instead suggest using the cumulative Jensen–Shannon divergence (CJS) (Nguyen and Vreeken 2015). This divergence has two good features, its symmetrised form is upper-bounded, which aids interpretation, and it can be computed from the ECDFs, which can be efficiently estimated from MCMC samples. Other methods requieres the estimation of the density of the distributions, which can be computationally expensive and sensible to the choice of the kernel and bandwidth.\nThe authors suggest a cut-off value of 0.05 for the CJS. For a standard Normal this value corresponds to the mean being shifted by more than \\(\\approx 0.3\\) standard deviations, or the standard deviation differing by a factor greater than \\(\\approx 0.3\\). This value is not set in stone, but it can be used as a reference to interpret the results of the sensitivity analysis.\nIf the value of CJS when power-scaling the prior and likelihood is larger than the threshold, we have a potential prior-data conflict. If the value of CJS is larger than the threshold but the value for the likelihood is smaller than the threshold, we potentially have a strong prior and weak likelihood.\nIn the next sections we are going to show two examples of on how to use this method in practice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html#body-fat-example",
    "href": "Chapters/Sensitivity_checks.html#body-fat-example",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "6.2 Body fat example",
    "text": "6.2 Body fat example\nLet’s see an example of prior-data conflict in a linear regression model. We will use the bodyfat data set (Johnson 1996). The aim of the analysis is to predict an expensive and cumbersome water immersion measurement of body fat percentage (observed variable named siri) from a set of thirteen easier to measure characteristics, including age, height, weight, and circumferences of various body parts.\nLet’s start by loading the data and creating a siri variable for the response and a body_fat variable for the predictors. We center the predictors.\n\nbody_fat = pd.read_csv(\"../data/body_fat.csv\")\nsiri = body_fat.pop(\"siri\")\nbody_fat = body_fat - body_fat.mean()\n\nWe are going to start by assuming that a resonable prior for all coefficients is Normal(0, 1). With that in mind, we will fit the following linear regression model:\n\\[\n\\begin{aligned}\n    \\beta_0 &\\sim t_3(0, 9.2) \\\\\n    \\beta_k &\\sim \\mathcal{N}(0, 1) \\\\\n    \\sigma &\\sim t_{3}^{+}(9.2) \\\\\n    \\mu &= β_0 + β X^T \\\\\n    \\text{siri} &\\sim \\mathcal{N}(\\mu, \\sigma)\n\\end{aligned}\n\\]\nFor the prior (and likelihood) sensitivity method to work we need to ensure that the DataTree object includes the groups log-prior and log-likelihood.\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model(coords={\"covariates\": body_fat.columns}) as model_bf_00:\n    β_0 = pm.StudentT(\"β_0\", nu=3, mu=0, sigma=9.2)\n    β = pm.Normal(\"β\", 0, 1, dims=\"covariates\")\n    σ = pm.HalfStudentT(\"σ\", nu=3, sigma=9.2)\n    μ = pm.Deterministic(\"μ\", β_0 + β @ body_fat.T)\n    y = pm.Normal(\"y\", μ, σ, observed=siri)\n    dt_bf_00 = pm.sample(random_seed=SEED,\n                            idata_kwargs={\"log_likelihood\": True, \"log_prior\": True})\n\n\n\n## coming soon\n\n\n\nWe can now use the psense_summary function from ArviZ to perform a numerical sensitivity analysis. This function returns a four-column DataFrame. The first column is the name of the variables, the second is the value of CJS when power-scaling the prior, then the value of CJS when power-scaling the likelihood and the last one is a textual description of the results.\nFor the \\(\\beta\\) coefficient wrist we can see that we may have prior-sensitivity (CJS&gt;0.05) and likelihood-sensitivity (CJS&gt;0.05). Thus, we may have a case of prior-data conflict for the wrist variable.\n\nazp.psense_summary(dt_bf_00, var_names=[\"~μ\"], round_to=2)\n\nWe detected potential issues. For more information on how to interpret the results, please check\nhttps://arviz-devs.github.io/EABM/Chapters/Sensitivity_checks.html#interpreting-sensitivity-diagnostics-summary\nor read original paper https://doi.org/10.1007/s11222-023-10366-5\n\n\n\n\n\n\n\n\n\nprior\nlikelihood\ndiagnosis\n\n\n\n\nβ[age]\n0.03\n0.12\n✓\n\n\nβ[weight]\n0.02\n0.08\n✓\n\n\nβ[height]\n0.01\n0.09\n✓\n\n\nβ[neck]\n0.01\n0.08\n✓\n\n\nβ[chest]\n0.02\n0.09\n✓\n\n\nβ[abdomen]\n0.00\n0.09\n✓\n\n\nβ[hip]\n0.01\n0.09\n✓\n\n\nβ[thigh]\n0.01\n0.10\n✓\n\n\nβ[knee]\n0.00\n0.10\n✓\n\n\nβ[ankle]\n0.03\n0.09\n✓\n\n\nβ[biceps]\n0.01\n0.08\n✓\n\n\nβ[forearm]\n0.02\n0.10\n✓\n\n\nβ[wrist]\n0.11\n0.16\npotential prior-data conflict\n\n\nβ_0\n0.01\n0.12\n✓\n\n\nσ\n0.01\n0.18\n✓\n\n\n\n\n\n\n\nWe can also evaluate the effect of power-scaling visually with the function plot_psense_dist. This function displays the posterior marginal for 3 values of \\(\\alpha\\). By default 0.8 (red), 1 (black) and 1.25 (blue). plot_psense_dist allow as to choose different representations including KDEs and ECDFs. From Figure 6.3 we can see that the posterior is sensitive to both prior and likelihood power-scaling, and that it shifts right (towards zero) as the prior is strengthened, and left (away from zero) as the likelihood is strengthened.\n\nazp.plot_psense_dist(dt_bf_00, \n                     var_names=[\"β\"], \n                     coords={\"covariates\": [\"wrist\"]},\n                     kind=\"ecdf\",\n                     )\n\n\n\n\n\n\n\nFigure 6.3: Posterior density estimates depending on amount of power-scaling (alpha). Overlapping lines indicate low sensitivity. Wider gaps between lines indicate greater sensitivity.\n\n\n\n\n\nWe can also use the plot_psense_quantities function to display quantities of interest like the mean, median, standard deviation, etc. We can then evaluate how much the quantities of interest change when we perturb the prior and likelihood. To get a reference for the relative effect size of the perturbations the function also shows the ±2 Monte Carlo standard errors (MCSE) of the base posterior quantity. From Figure 6.4 we can see that power-scaling the prior and likelihood pulls the posterior mean in opposite directions. This suggests there might be a potential mismatch or conflict between the prior and the likelihood.\n\nazp.plot_psense_quantities(dt_bf_00,\n                           var_names=[\"β\"],\n                           coords={\"covariates\": [\"wrist\"]})\n\n\n\n\n\n\n\nFigure 6.4: Posterior quantities of βwrist as a function of power-scaling of the prior and the likelihood. Dashed lines are ± 2 Monte Carlo standard errors (MCSE) of the base posterior quantity\n\n\n\n\n\n\n6.2.1 Adjusting the prior\nAs there is prior sensitivity arising from prior-data conflict, which is unexpected and unintentional as our priors were chosen to be weakly informative, we consider modifying the priors. On inspecting the raw data, we see that although the predictor variables are all measured on similar scales, the variances of the variables differ substantially. For example, the variance of wrist circumference is 0.83, while the variance of abdomen is 102.65. This leads to our chosen prior to be unintentionally informative for some of the regression coefficients, including wrist, while being weakly informative for others\nNow, let try with a different prior. We will use empirically scaled to the data, \\(βk \\sim \\mathcal{N}(0, 2.5 \\frac{s_y}{s_{xk}})\\), where \\(s_y\\) is the standard deviation of \\(y\\) and \\(s_{xk}\\) is the standard deviation of predictor variable \\(xk\\).\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model(coords={\"covariates\": body_fat.columns}) as model_bf_01:\n    β_0 = pm.StudentT(\"β_0\", nu=3, mu=0, sigma=9.2)\n    β = pm.Normal(\"β\", 0, 2.5 * siri.std() / body_fat.std(), dims=\"covariates\")\n    σ = pm.HalfStudentT(\"σ\", nu=3, sigma=9.2)\n    μ = pm.Deterministic(\"μ\", β_0 + β @ body_fat.T)\n    y = pm.Normal(\"y\", μ, σ, observed=siri)\n    dt_bf_01 = pm.sample(tune=1000,\n                      draws=2000, random_seed=SEED,\n                      idata_kwargs={\"log_likelihood\": True, \"log_prior\": True})\n    pm.sample_posterior_predictive(dt_bf_01, extend_inferencedata=True)\n\n\n\n## coming soon\n\n\n\nNow we can perform the sensitivity analysis again. We can see that we managed to remove the warning!\n\nazp.psense_summary(dt_bf_01, var_names=[\"~μ\"])\n\n\n\n\n\n\n\n\nprior\nlikelihood\ndiagnosis\n\n\n\n\nβ[age]\n0.001\n0.087\n✓\n\n\nβ[weight]\n0.001\n0.080\n✓\n\n\nβ[height]\n0.001\n0.082\n✓\n\n\nβ[neck]\n0.000\n0.095\n✓\n\n\nβ[chest]\n0.001\n0.083\n✓\n\n\nβ[abdomen]\n0.003\n0.096\n✓\n\n\nβ[hip]\n0.001\n0.081\n✓\n\n\nβ[thigh]\n0.000\n0.097\n✓\n\n\nβ[knee]\n0.000\n0.097\n✓\n\n\nβ[ankle]\n0.000\n0.092\n✓\n\n\nβ[biceps]\n0.001\n0.091\n✓\n\n\nβ[forearm]\n0.000\n0.100\n✓\n\n\nβ[wrist]\n0.000\n0.087\n✓\n\n\nβ_0\n0.005\n0.101\n✓\n\n\nσ\n0.002\n0.189\n✓\n\n\n\n\n\n\n\nWe can see that after changing the prior, the posterior mean for wrist changes from -1.45 to -1.86, indicating that the base prior was indeed unintentionally informative and in conflict with the data, pulling the estimate towards zero.\nAs previously, we can also evaluate the effect of power-scaling visually with the function plot_psense_dist. From Figure 6.5 we can see a better overlap between the posterior distributions, compared to the original prior.\n\nazp.plot_psense_dist(dt_bf_01, \n                     var_names=[\"β\"], \n                     coords={\"covariates\": [\"wrist\"]},\n                     kind=\"ecdf\",\n                     )\n\n\n\n\n\n\n\nFigure 6.5: Posterior density estimates depending on amount of power-scaling (alpha). Overlapping lines indicate low sensitivity. Wider gaps between lines indicate greater sensitivity.\n\n\n\n\n\nIn terms or the mean for βwrist, we can see that there is no longer prior or likelihood sensitivity, indicating no prior-data conflict. For the standard deviation, likelihood sensitivity remains, indicating that the likelihood is informative.\n\nazp.plot_psense_quantities(dt_bf_01, var_names=[\"β\"], coords={\"covariates\": [\"wrist\"]})\n\n\n\n\n\n\n\nFigure 6.6: Posterior quantities of βwrist as a function of power-scaling of the prior (purple line and circle markers) and the likelihood (green line and square markers). Dashed lines are ± 2 Monte Carlo standard errors (MCSE) of the base posterior quantity\n\n\n\n\n\nOverall, the power-scaling sensitivity analysis on the adjusted prior shows that there is no longer prior sensitivity, and there is appropriate likelihood sensitivity.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html#non-interpretable-parameters-and-derived-quantities",
    "href": "Chapters/Sensitivity_checks.html#non-interpretable-parameters-and-derived-quantities",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "6.3 Non-interpretable parameters and derived quantities",
    "text": "6.3 Non-interpretable parameters and derived quantities\nIn many cases, instead of looking at the posterior distribution of the model parameters, we will be interested in derived quantities. Some examples of derived quantities include, predictive performance measures, such as the Bayesian R² or the joint log-likelihood. Another example are predictions evaluated at specific values of the predictors, like the median of one or more predictors.\nEvaluating the sensitivity of derived quantities can be useful in at least two scenarios:\n\nWhen the model parameters are not directly interpretable. For example, the coefficient for splines are not only not directly interpretable, they are not relevant per se, they are just an “intermediate step” to get a smooth and flexible function. The same applies to Gaussian processes (GPs), Bayesian additive regression trees (BART), polynomials, just to name a few. If the parameters are not directly interpretable, we can exclude them from these sensitivity checks, and instead focus on the derived quantities.\nWhen we are interested in a quantity that is more relevant or easier to explain to our audience. Sometimes we don’t care about the parameters of a model, irrespective of whether they are interpretable or not. This could happen if we are focused on a quantity that is of interest or easier to interpret for our audience, like the R². This could also be the case when our focus is on predictions. If those cases, we may decide to focus on the sensitivity checks for the derived quantities and, if they are ok, ignore the ones for the model parameters, even if they are interpretable. If the checks for the derived quantities are not ok, we may need to go back and check the model parameters, in particular if they are interpretable.\n\n\n6.3.1 Computing derived quantities\nLet’s illustrate this with an example. We will use the same model we used in the previous section, but instead of looking at the posterior distribution of the parameters, we will focus on the following derived quantities:\n\nThe Bayesian R².\nThe joint log-likelihood. This is the sum of the log-likelihood over all observations.\n\nWe are going to start by computing predictions at new_data, as you may already know how to do this from other examples. To do that we need to create a new DataFrame with the values of the predictors we want to use for prediction. In this case, we will use the median, min, and max values of each covariate. For your particular use case, you may want to use different values, like quantiles of any other specific values of interest.\nWe can ask ArviZ, to compute the Bayesian R² for us, we need to specify which variables represent the posterior mean (or location parameter) and standard deviation (or variance).\n\nr2_da = azp.ndarray_to_dataarray(azp.bayesian_r2(dt_bf_01, pred_mean=\"μ\", scale=\"σ\", summary=False).reshape(4, 2000), var_name=\"r2\")\ndt_bf_01.posterior[\"r2\"] = r2_da\n\nTo compute the joint log-likelihood we just sum the poin-twise log-likelihood evaluations along the observations. In other words we compute one log-likelihood value per MCMC step.\n\ndt_bf_01.posterior[\"log_score\"] = dt_bf_01.log_likelihood.sum(\"y_dim_0\")[\"y\"]\n\nOnce we have added the derived quantities we just need to call psense_summary (or the others psense_* functions) as usual:\n\nazp.psense_summary(dt_bf_01, var_names=[\"r2\", \"log_score\"])\n\n\n\n\n\n\n\n\nprior\nlikelihood\ndiagnosis\n\n\n\n\nr2\n0.001\n0.158\n✓\n\n\nlog_score\n0.001\n0.619\n✓\n\n\n\n\n\n\n\nWe see no signs of data-conflict or likelihood noninformativity, we can visually check this as we did before, but given these values we should expect the distributions of the derived quantities to very similar across the different priors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html#bacteria-treatment",
    "href": "Chapters/Sensitivity_checks.html#bacteria-treatment",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "6.4 Bacteria treatment",
    "text": "6.4 Bacteria treatment\nNow we discuss and example of power-scaling sensitivity analysis for hierarchical models. The main motivation for this example is to show that for certain models we should selectively power-scaled the priors. To illustrate this, consider two forms of prior, a non-hierarchical prior with two independent parameters \\(p(\\theta)\\) and \\(p(\\phi)\\) and a hierarchical prior of the form \\(p(\\theta \\mid \\psi) p(\\psi)\\). In the first case, the appropriate power-scaling for the prior is \\(p(\\theta)^{\\alpha} p(\\phi)^{\\alpha}\\). This is what we did in the previous example. In the second case, for the hierarchical model, we only want to power-scale the top level prior, that is, \\(p(\\theta) p(\\phi)^{\\alpha}\\).\nFor this example we are going to use the bacteria data set (Venables and Ripley 2002).\n\nbacteria = pd.read_csv(\"../data/bacteria.csv\")\nbacteria[\"y\"] = bacteria[\"y\"].astype(\"category\").cat.codes\nbacteria[\"ID\"] = bacteria[\"ID\"].astype(\"category\").cat.codes\nbacteria[\"trtDrugP\"] = bacteria[\"trt\"] == \"drug+\"\nbacteria[\"trtDrug\"] = bacteria[\"trt\"] == \"drug\"\nK = len(bacteria[\"ID\"].unique())\n\nLet’s start by fitting a hierarchical model. The model is as follows:\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model() as model_bt:\n    μ = pm.Normal('μ', mu=0, sigma=10)\n    β_week = pm.Normal('β_week', mu=0, sigma=10)\n    β_trtDrug = pm.Normal('β_trtDrug', mu=0, sigma=10)\n    β_trtDrugP = pm.Normal('β_trtDrugP', mu=0, sigma=10)    \n\n    σ = pm.HalfNormal('σ', sigma=5)\n    b_Intercept = pm.Normal('b_Intercept', mu=0, sigma=σ, shape=K)\n\n    theta = μ + b_Intercept[bacteria.ID] + β_week * bacteria.week + β_trtDrug * bacteria.trtDrug + β_trtDrugP * bacteria.trtDrugP\n    \n    y_obs = pm.Bernoulli('y_obs', logit_p=theta, observed=bacteria.y)\n    \n    dt_bt = pm.sample()\n    pm.compute_log_prior(dt_bt, var_names=[\"μ\", \"β_week\", \"β_trtDrug\", \"β_trtDrugP\", \"σ\"])\n    pm.compute_log_likelihood(dt_bt)\n\n\n\n## coming soon\n\n\n\nFrom the power-scaling sensitivity analysis perspective the key element in the previous code-block is that we are specifying the variables we want to use for the prior-powerscaling var_names=[\"μ\", \"β_week\", \"β_trtDrug\", \"β_trtDrugP\", \"σ\"] i.e. we are omitting the b_Intercept variable. This is because we are only interested in power-scaling the top level prior. There are two way to specify the variables for power-scaling, the first is to use the var_names argument when computing the log_prior and/or log_likelihood, as we just did. The second is to use the prior_varnames and likelihood_varnames arguments in the psense-related functions.\nLet’s compute sensitivity diagnostics for all variables except ~b_Intercept, if we want to check the sensitivity of all of them we can do it. The key point with hierarchical models is to not power-scale the lower level priors.\n\nazp.psense_summary(dt_bt, var_names=[\"~b_Intercept\"])\n\n\n\n\n\n\n\n\nprior\nlikelihood\ndiagnosis\n\n\n\n\nβ_trtDrug\n0.007\n0.156\n✓\n\n\nβ_trtDrugP\n0.008\n0.166\n✓\n\n\nβ_week\n0.005\n0.196\n✓\n\n\nμ\n0.013\n0.367\n✓\n\n\nσ\n0.012\n0.585\n✓\n\n\n\n\n\n\n\nWe see that everything looks fine. If you like to get potentials issues you could try running the model again with a prior like σ = pm.HalfNormal('σ', sigma=1).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Sensitivity_checks.html#interpreting-sensitivity-diagnostics-summary",
    "href": "Chapters/Sensitivity_checks.html#interpreting-sensitivity-diagnostics-summary",
    "title": "6  Prior and likelihood sensitivity checks",
    "section": "6.5 Interpreting sensitivity diagnostics: Summary",
    "text": "6.5 Interpreting sensitivity diagnostics: Summary\nAs with other diagnostics we need to interpret the results of the sensitivity analysis in the context of the model, the data and the problem we are trying to solve. Context and modelling purpose should always be part of an analysis.\n\n6.5.1 Diagnostic value\nThe sensitivity diagnostic value given by psense and psense_summary is based on a measure of how much the posterior would change if the prior or likelihood is changed. This value is provided for each marginal posterior specified in the var_name argument. In simple models with few parameters, it is reasonable to look at sensitivity for all the parameters. But as model complexity increases, and there are more parameters or strong posterior dependencies, it is better to focus on sensitivity of specific parameters with meaningful interpretations or on derived quantities of interest.\n\n\n6.5.2 Diagnostic messages\nSensitivity diagnostic values are given for both prior and likelihood sensitivity. These values should be considered and interpreted together. Based on the values, a diagnosis is also given. Currently, this is either “strong prior / weak likelihood” (if the prior sensitivity is higher than a threshold and the likelihood sensitivity is lower than a threshold) or “prior-data conflict” (if both types of sensitivity are higher than the threshold).\nThese diagnostic messages do not necessarily indicate problems with the model. They are informative messages that describe the interplay between the chosen prior and likelihood. If your prior is meant to be informative, influence on the posterior is desired and prior-data conflict may not be an issue. However, if you did not put much effort into choosing the priors, these messages can let you know if you should be more deliberate in your prior specification.\n\nStrong prior / weak likelihood. This can occur when:\n\nThe prior is completely dominating the likelihood such that changing the likelihood strength has little to no impact on the posterior. The prior may be extremely informative and using a weaker prior may remove this domination.\nThe likelihood is uninformative and no information is gained by increasing the strength of the likelihood. The prior will always have an effect in this case.\n\nPrior-data conflict. This can occur when:\n\nThe posterior is sensitive to changes to both the prior and the likelihood. This indicates that the prior and likelihood are both influencing the posterior and may be in conflict with one-another.\n\n\n\n\n6.5.3 What to do with the diagnostics\nSensitivity analysis should be conducted with care, avoiding the repeated adjustment of priors solely to resolve discrepancies or diagnostic warnings. If a prior is modified to address a warning, the change must be justified based on domain expertise, data properties, or model assumptions. Conversely, choosing not to alter the model despite warnings can also be valid if there are compelling reasons to believe the model is sound. In such cases, it is essential to be transparent: report the diagnostic results and provide a clear rationale for disregarding the sensitivity diagnostics.\n\n\n\n\nJohnson, Roger W. 1996. “Fitting Percentage of Body Fat to Simple Body Measurements.” Journal of Statistics Education 4 (1). https://doi.org/10.1080/10691898.1996.11910505.\n\n\nKallioinen, Noa, Topi Paananen, Paul-Christian Bürkner, and Aki Vehtari. 2023. “Detecting and Diagnosing Prior and Likelihood Sensitivity with Power-Scaling.” Statistics and Computing 34 (1): 57. https://doi.org/10.1007/s11222-023-10366-5.\n\n\nNguyen, Hoang-Vu, and Jilles Vreeken. 2015. “Non-Parametric Jensen-Shannon Divergence.” In Machine Learning and Knowledge Discovery in Databases, edited by Annalisa Appice, Pedro Pereira Rodrigues, Vítor Santos Costa, João Gama, Alípio Jorge, and Carlos Soares, 173–89. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-23525-7_11.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th edition. New York: Springer. https://doi.org/10.1007/978-0-387-21706-2.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior and likelihood sensitivity checks</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html",
    "href": "Chapters/Model_comparison.html",
    "title": "7  Model Comparison",
    "section": "",
    "text": "7.1 Overview\nModels are designed as approximations to help us understand a specific problem or a related class of problems. They are not intended to be exact replicas of the real world. In this sense, all models are “wrong.” However, not all models are equally wrong, some are better suited to particular aspects of a problem. For example, a model that is good for high-temperature observations may perform poorly for low-temperature observations.\nIn a typical data analysis, it’s common to develop multiple models that fit the same data. These models may differ in aspects such as priors, likelihoods, linear versus nonlinear terms, interactions terms, etc. When several plausible models are available, a key question arises: how should we choose between them?\nUnderstanding the goals of the analysis and the problem at hand often provides valuable guidance. Models rarely incorporate all of our prior knowledge, so this additional information can help us evaluate and compare them. Prior and posterior predictive checks (see Chapter 5) and computational diagnostics (see Chapter 4) can further inform model choice (a model we can not fit well is a model we can not trust!). Even convention and tradition may influence selection, although scientific reasoning should ideally prevail over tradition.\nIn this chapter, we provided a theoretical, yet accesible, foundation for a series of methods for efficient model comparison. Later in Chapter 8, Chapter 10, and Chapter 9 we provided applied examples of these methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#the-balance-between-simplicity-and-accuracy",
    "href": "Chapters/Model_comparison.html#the-balance-between-simplicity-and-accuracy",
    "title": "7  Model Comparison",
    "section": "7.2 The balance between simplicity and accuracy",
    "text": "7.2 The balance between simplicity and accuracy\nWhen comparing models, we seek a balance between simplicity and accuracy. Occam’s razor suggests that, among equally plausible explanations, the simplest should be preferred. In modeling, simplicity is often measured by the number of parameters, while accuracy reflects how well a model fits the data. Ideally, we want models that fit the data well without overfitting.\nOverfitting occurs when a model captures not only the underlying patterns but also random noise. Such models perform well on the data used for fitting (within-sample error) but poorly on new, unseen data. Overfitting is more likely in highly flexible models with many parameters or when the model structure is overly complex. Detecting it requires evaluating performance on data not used for fitting, motivating out-of-sample measures such as cross-validation or information criteria.\nBayesian models are generally less prone to overfitting than many alternative approaches. This is because priors constrain the parameter space, and computing the posterior involves averaging over these priors rather than committing to a single point estimate. As a result, even highly flexible Bayesian models often avoid the extreme overfitting seen in classical settings. A typical text book example of overfitting is a polynomial with as many degrees of freedom as data points. This will indeed badly overfit under optimization-based methods, but a Bayesian model with even quite vague priors will provide a reasonable solution. Priors can even stabilize models with more parameters than observations, and for decades we have known that Bayesian methods can handle models with infinitely many parameters, such as Gaussian processes. However, this protection is not automatic. Bayesian models with very vague priors are more prone to overfitting, while models whose priors induce a prior predictive distribution consistent with domain knowledge tend to be more resistant to it.\n\n7.2.1 Predictive accuracy measures\nEvaluating a model only on the data used to fit it usually gives an overly optimistic view of its performance, a problem known as overfitting. Instead we prefer to evaluate a model in terms of its predictive accuracy, i.e. it’s capacity to predicte unobserved data.\nThe simplest way to do this is to use separate training and test datasets. But in practice, data is often limited, and setting aside part of it can be wasteful. Because this situation is so common, many methods have been developed to estimate predictive accuracy without wasting data.\nWe will focus on two families of methods:\n\nCross-validation: Divides the data into subsets that are alternately used for fitting and evaluation, effectively simulating a hold-out set while still using all data for inference.\nInformation criteria: Approximate out-of-sample accuracy by combining in-sample fit with a penalty for model complexity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#cross-validation",
    "href": "Chapters/Model_comparison.html#cross-validation",
    "title": "7  Model Comparison",
    "section": "7.3 Cross validation",
    "text": "7.3 Cross validation\nCross-validation is a simple and, in most cases, effective solution for comparing models. We take our data and divide it into \\(K\\) slices. We try to keep the portions more or less the same (in size and sometimes also in other characteristics, such as an equal number of classes). We then use \\(K-1\\) portions to train the model and the rest to evaluate it. This process is systematically repeated, leaving, for each iteration, a different portion out of the training set and using that portion as the evaluation set. This is repeated until we have completed \\(K\\) rounds of adjustment–evaluation. The accuracy of the model will be the average over the \\(K\\) rounds. This is known as \\(K\\)-fold cross-validation. Finally, once we have cross-validated, we use all the data to fit our model, and this is the model that is used to make predictions or for any other purpose.\n\nWhen \\(K\\) is equal to the number of data points, we get what is known as leave-one-out cross-validation (LOO-CV).\nCross validation is a routine practice in machine learning. And we have barely described the most essential aspects of this practice. For more information you can read The Hundred-Page Machine Learning Book or Python Machine Learning, by Sebastian Raschka.\nOne downside of cross-validation is that it is computationally expensive. We need to fit the model \\(K\\) times, and if we have a large dataset, this can be very expensive. But lucky us, by being Bayesian we can approximately compute LOO-CV in a very fast way, as discussed later in Section 7.5. But before that we are going to discuss information criteria, because as we will see LOO-CV is actually related to these methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#information-criteria",
    "href": "Chapters/Model_comparison.html#information-criteria",
    "title": "7  Model Comparison",
    "section": "7.4 Information criteria",
    "text": "7.4 Information criteria\nInformation criteria are a collection of closely related tools used to compare models in terms of goodness of fit and model complexity. In other words, information criteria formalize the intuition we developed at the beginning of the chapter. The exact way these quantities are derived has to do with a field known as Information Theory.\nAn intuitive way to measure how well a model fits the data is to calculate the root mean square error between the data and the predictions made by the model:\n\\[\n\\frac{1}{N} \\sum _{i}^{N} (y_i - \\operatorname{E} (y_i \\mid \\theta))^2\n\\]\n\\(\\operatorname{E} (y_i \\mid \\theta)\\) is the predicted value given the estimated parameters. It is important to note that this is essentially the average of the difference between the observed and predicted data. Taking the square of the errors ensures that differences do not cancel out and emphasizes larger errors compared to other alternatives such as calculating the absolute value.\nThe mean square error may be familiar to us since it is very popular. But if we stop and reflect on this quantity we will see that in principle there is nothing special about it and we could well come up with more general expressions. In order to do that we are going to first discuss to two useful concepts, entropy and the Kulback-Leibler divergence.\n\n7.4.1 Entropy\nFor a probability distribution with \\(N\\) possible different events which each possible event having probability \\(p_i\\), the entropy is defined as:\n\\[\nH(p) = - \\mathbb{E}[\\log{p}] = -\\sum_i^N p_i \\log{p_i}\n\\]\nEntropy is a measure of the uncertainty of a distribution. In this sense we can say that the uncertainty contained in a distribution is the logarithm of the average probability of an event. If only one event is possible the entropy will be 0, if all events have the same probability the entropy will be maximum. The concept of entropy can be extended to continuous distributions, but we will not go into those details. Figure 7.1 shows the entropy of a Bernoulli distribution for four different values of the probability of success. We can see that the entropy is maximum when the probability of success is 0.5, and minimum when the probability of success is 0.\n\n\nCode\n_, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n\nfor p, ax in zip([0.5, 0.1, 0.9, 0.0001], axes.ravel()):\n    dist = pz.Bernoulli(p=p)\n    dist.plot_pdf(ax=ax, legend=False)\n    ax.set_title(f\"Entropy={dist.entropy():.2f}\")\n    ax.set_ylim(-0.05, 1.05)\n\n\n\n\n\n\n\n\nFigure 7.1: Entropy of a Bernoulli distribution as a function of the probability of success\n\n\n\n\n\nThe concept of entropy appears many times in statistics. It can be useful, for example when defining priors. In general we want to use a prior that has maximum entropy given our knowledge (see for example PreliZ’s maxent function). And also when comparing models as we will see in the next section.\n\n\n7.4.2 KL divergence\nThe Kulback-Leibler divergence is a measure of how one probability distribution diverges from a second expected probability distribution. Suppose we have a target distribution \\(p\\), with which we cannot work directly and we only have access to a different distribution that we will call \\(q\\). We want to evaluate how well \\(q\\) approximates \\(p\\). One way to do this is to measure the Kulback-Leibler divergence between \\(p\\) and \\(q\\). If \\(q\\) is a parametric family we can find the parameters making \\(q\\) as close to \\(p\\) as possible by minimizing the KL divergence. The KL divergence is defined as:\n\\[\n\\mathbb{KL}(p \\parallel q) =  \\overbrace{-\\sum_i^N p_i \\log{q_i}}^{H(p, q)} -  \\overbrace{\\left(-\\sum_{i}^n p_i \\log{p_i}\\right)}^{H(p)}\n\\]\nNotice that it has two components, the entropy of \\(p\\), \\(H(p)\\) and the cross entropy \\(H(p, q)\\), that is, the entropy of \\(q\\) but evaluated according to \\(p\\). This may seem somewhat abstract, but if we think that we have \\(N\\) samples that we assume come from an unknown distribution \\(p\\) and we have a model described by \\(q(y \\mid \\theta)\\), then we will see that we are describing a typical situation in data analysis.\nAccording to this expression, the KL divergence represents the “extra” entropy that we introduce when approximating \\(p\\) by \\(q\\). It is common to find it written in other ways, such as:\n\\[\n\\mathbb{KL}(p \\parallel q) \\quad=\\quad- \\sum_i^N p_i (\\log{q_i} - \\log{p_i}) \\quad=\\quad \\mathbb{E}_p[\\log{p}] - \\mathbb{E}_p[\\log{q}] \\quad=\\quad \\sum_i^N p_i \\log{\\frac{p_i}{q_i}}\n\\]\nIf \\(p\\) represents the data generating process or the population or the true distribution, and \\(q\\) represents our model. It may seems that this expressions are all useless because we don’t know \\(p\\). That the reason we are trying to fit a model in the first place. But, if our goal is to compare \\(m\\) models represented with \\(q_0, q_1, \\cdots, q_m\\), we can can still use the KL divergence to compare them! The reason is that even when we do not know \\(p\\), its entropy is a constant term for all comparisons.\n\\[\n\\begin{split}\n        \\mathbb{KL}(p \\parallel q_0) =&\\; \\mathbb{E}[\\log{p}] - \\mathbb{E}[\\log{q(y \\mid \\theta_0)}] \\\\\n        \\mathbb{KL}(p \\parallel q_1) =&\\; \\mathbb{E}[\\log{p}] - \\mathbb{E}[\\log{q(y \\mid \\theta_1)}] \\\\\n        &\\cdots \\\\\n        \\mathbb{KL}(p \\parallel q_2) =&\\; \\mathbb{E}[\\log{p}] - \\mathbb{E}[\\log{q(y \\mid \\theta_2)}]\n\\end{split}\n\\]\nThis tell us that when comparing models the best model, from the set of compared models, will be the one that has the larger (log-)likelihood value. In other words, minimizing the KL divergence is proportional to maximizing likelihood.\nThis result is very general and useful, but it has a catch. In practice, we don’t really have \\(\\mathbb{E}[\\log{q}]\\), and hence we need to estimate it. We can use the observed data to estimate the parameters of a model and then use those parameters to estimate \\(\\mathbb{E}[\\log{q}]\\), but that will introduce bias. We will be overconfident about the ability of our model to explain the data. Information criteria and cross-validation provide a way to reduce this bias without having to get a new dataset.\n\n\n7.4.3 Akaike information criterion\nThe Akaike information criterion (Akaike 1974) (AIC) is a very well-known and widely used information criterion and is defined as:\n\\[\nAIC = -2 \\sum_i^N \\log p(y_i \\mid \\hat{\\theta}_{mle}) + 2 k\n\\]\nWhere, \\(k\\) is the number of model parameters and \\(\\hat{\\theta}_{mle}\\) is the maximum likelihood estimate for \\(\\theta\\). For the rest of our discussion we will omit the constant -2 and write\n\\[\nAIC = \\sum_i^N \\log p(y_i \\mid \\hat{\\theta}_{mle}) - k\n\\]\nIn this way it is easier to see that the Akaike criterion is a penalized maximum likelihood, it becomes smaller the more parameters a model has. Furthermore, this version without the -2 has a clearer correspondence with other expressions which we will see below.\n\n\n\n\n\n\nNote\n\n\n\nThat the number of parameters is a valid penalty criterion follows our intuition, a model with a greater number of parameters is, in general, more flexible. But it is interesting to note that the Akaike’s criterion has a theoretical justification, it is not that Akaike simply thought that using \\(k\\) was a good idea.\n\n\nThe AIC criterion is very useful, but can be very limited for Bayesian models. One reason is that it uses a point estimate of \\(\\theta\\) and not the posterior distribution, hence it discards potentially useful information. Furthermore AIC, from a Bayesian perspective, assumes that priors are flat and therefore AIC is incompatible with informative and/or weakly informative priors. Furthermore, the number of parameters in a model is not always a good measure of its complexity. In general, a regularized model will be a model with less effective number of parameters. For example, when using informative priors or in hierarchical models, parameters becomes interrelated and thus the effective number of parameters can be smaller than the actual number of parameter. AIC has no way to account for this.\nCan we find something like the Bayesian version of AIC? Yes, we can.\n\n\n7.4.4 ELPD\nAs we already saw in the Akaike criterion, the goodness of fit is given by:\n\\[\n\\sum_i^N \\log p(y_i \\mid \\hat{\\theta}_{mle})\n\\]\nBut in Bayesian statistics, we do NOT have a point estimate of \\(\\theta\\). We have a distribution. To account for this we could do:\n\\[\n\\sum_i^N \\log \\int \\ p(y_i \\mid \\theta) \\; p(\\theta \\mid y) d\\theta\n\\]\nIn general we do not have an analytical expression for the posterior, \\(p(\\theta \\mid y)\\), instead we usually work with samples (such as those obtained by MCMC), then we can approximate the above integral by a sum over the \\(S\\) posterior samples:\n\\[\n\\sum_i^N \\log \\left(\\frac{1}{S} \\sum _{j}^S p(y_i \\mid \\theta^j) \\right)\n\\]\nWe will call this quantity the ELPD, which is short for expected log-predictive density. When the likelihood is discrete, we should use “probability” instead of “density”, but it is a common practice to avoid pedantry.\nThe ELPD is more Bayesian way to measure goodness of fit that the term used in AIC, but we are still missing one element, the penalization term.\n\n\n7.4.5 WAIC\nThe Widely applicable information criterion (WAIC) uses the ELPD plus a penalization term (Watanabe 2013).\n\\[\nWAIC = \\sum_i^N \\log \\left(\\frac{1}{S} \\sum _{s}^S p(y_i \\mid \\theta^j) \\right) - \\sum_i^N \\left( V_{j}^S \\log p(y_i \\mid \\theta^j) \\right)\n\\]\nWe can see that penalization term is given by the variance of the log-likelihoods over the \\(S\\) posterior samples. Justifying this term requieres a bit more work, but the intuition is that the variance of the log-likelihoods is a measure of how much variability there is in the predictions made by the model. The more variability, the more flexible the model is. And therefore, the more we should penalize it. Let’s look at a linear model as an example:\n\\[\nY = \\alpha + \\beta X\n\\]\nA model where \\(\\beta=0\\) will be less flexible, since it is equivalent to a model that only has one parameter, \\(\\alpha\\). In a slightly more subtle way, a model where \\(\\beta\\) varies in a narrow range will be less flexible (more regularized), than a model where \\(\\beta\\) can take any value. WAIC properly formalized this intuition.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#sec-efficient-loo-cv",
    "href": "Chapters/Model_comparison.html#sec-efficient-loo-cv",
    "title": "7  Model Comparison",
    "section": "7.5 Efficient LOO-CV",
    "text": "7.5 Efficient LOO-CV\nWe have seen that we can use information criteria to estimate the expected log-predictive density (ELPD) by introducing a penalization term to the log-likelihood, which corrects for the bias introduced when double-dipping our model into the data.\nAs already discussed cross-validation provides an alternative way to address this bias. The LOO-CV ELPD can be computed as:\n\\[\n\\sum_i^N  \\log \\left( \\frac{1}{S}\\sum_j^S \\mathbin{\\color{#E9692C}{p(y_i \\mid \\theta _{-i}^j)}} \\right)\n\\]\nwhere \\(_{-i}\\) means that we leave observation \\(i\\) out. A Naive implementation of this estimation requires that we estimate as many posterior distributions as observations we have, since for each of them we will eliminate one observation. However, this is not necessary since it is possible to estimate \\(\\color{#E9692C}{p(y_i \\mid \\theta _{-i}^j})\\) using Importance Sampling.\n\n7.5.1 Importance Sampling\nThis is a technique for estimating properties of a distribution of interest \\(f\\), given that we only have samples from a distribution \\(g\\). Using importance sampling makes sense, for example, when it is simpler to sample \\(g\\) than \\(f\\).\nIf we have a set of samples of the random variable \\(X\\) and we can evaluate \\(g\\) and \\(f\\) point-wise, we can calculate the importance weights as:\n\\[\\begin{equation}\nw_i = \\frac{f(x_i)}{g(x_i)}\n\\end{equation}\\]\nComputationally it looks like this:\n\nExtract \\(N\\) samples \\(x_i\\) from \\(g\\)\nCalculate the probability of each sample \\(g(x_i)\\)\nEvaluate \\(f\\) on the \\(N\\) samples \\(f(x_i)\\)\nCalculate the importance weights \\(w_i = \\frac{f(x_i)}{g(x_i)}\\)\n\nOnce the weights \\(w_i\\) are obtained, we can use them to estimate properties of \\(f\\), its density, moments, quantiles, etc.\nIn the code-block below \\(g\\) is a Normal distribution and \\(f\\) is a Gamma and we use importance sampling to estimate the PDF of \\(f\\). This is just a pedagogic example, since we actually have a very direct way to calculate the PDF of a Gamma. But in practice \\(f\\) can be a much more complex object.\n\ng = pz.Normal(0, 10)\nsamples = g.rvs(1000, random_state=SEED)\nf = pz.Gamma(mu=4, sigma=1.5)\n\nw = f.pdf(samples) / g.pdf(samples)\n\nax = f.plot_pdf()\nax.hist(samples, bins=100, density=True, weights=w, \n        alpha=0.6, color='C2', label='Weighted samples')\nax.set_xlim(0, 15);\n\n\n\n\n\n\n\n\nWhen doing importance sampling, the more similar \\(g\\) and \\(f\\) are, the better the results will be. In practice, inferences are more reliable when \\(g\\) has a larger support than \\(f\\), that is, when it is “wider”, intuitively we need the samples of \\(g\\) to cover the entire support of \\(f\\), or actually to ensure we are not missing any high-density regions.\n\n\n7.5.2 Importance sampling and LOO-CV\nThe distribution we know is the posterior distribution, and the one we want to approximate by importance sampling is the posterior distribution leaving one observation out \\(p(y_i \\mid \\theta_{-i}^j)\\). Therefore, the importance weights that we are interested in calculating are:\n\\[\nw_i^j \\propto \\frac{p(\\theta^j \\mid y_{-i} )}{p(\\theta^j \\mid y)} \\propto \\frac{p(\\theta) \\prod_{i\\not =-i}^n p(y_i \\mid \\theta)}{p(\\theta) \\prod_i^n p(y_i \\mid \\theta)} \\propto \\frac{1}{p(y_i \\mid \\theta^j) }\n\\]\nThe beauty of this expression is that all terms in the numerator and the denominator will cancel out except for only one! The likelihood for the observation we want to remove, that will remain in the denominator.\nThe weights computed in this way are are not normalized, so to use them we need to divide each weight by the total sum of the weights. Once the weights have been normalized, we can use them to estimate the ELPD as:\n\\[\n\\sum_i^N \\log \\left( \\frac{1}{S} \\sum_j^S w_i^j p(y_i \\mid \\theta^j) \\right)\n\\]\nThis result is fantastic news, it tells us that we can calculate the leave-one-out cross-validation ELPD, without having to refit the model \\(N\\) times.\nThe catch is that the expected \\(p(\\theta^j \\mid y_{-i})\\) will, in general, be wider than \\(p(\\theta^j \\mid y)\\), because it is a posterior distribution estimated with one fewer observation. This is the opposite of the ideal situation for importance sampling. Often the difference is small enough that it does not cause problems, but in some cases it can be large enough to make the approximation unreliable. When does this happen? The more influential the observation, the larger the change when we remove it.\nIs everything lost? Not yet! In terms of importance sampling this translates into weights with greater relative importance and which therefore tend to dominate the estimation. One way to correct this problem is to simply truncate the weights that are “too high”, this can be done but is sweeping the problem under the rug. We can do something similar but better.\nTheory indicates that, under very general conditions, the tail of the distribution of weights can be approximated by a generalized Pareto distribution. So instead of truncating them, we can fit a generalized Pareto distribution to the tail of the computed weights and then replace the original weights with draws from this fit. This is a form of smoothing that, within a certain range, allows stabilizing the importance sampling estimate by making some “very large” weights not so large.\nWhen we combine all these ideas we get a method called Pareto-Smooth Importance Sampling Leave-One-Out Cross Validation (Vehtari, Gelman, and Gabry 2017; Yao et al. 2018), which is abbreviated as PSIS-LOO-CV.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#psis-loo-cv-in-arviz",
    "href": "Chapters/Model_comparison.html#psis-loo-cv-in-arviz",
    "title": "7  Model Comparison",
    "section": "7.6 PSIS-LOO-CV in Arviz",
    "text": "7.6 PSIS-LOO-CV in Arviz\nSince the name and even acronym are too long and not that easy to pronounce, in ArviZ, we usually refer to this and related methods as LOO. For example to compute the ELPD, estimated using PSIS-LOO_CV, we call the loo function, we just need to pass DataTree object containing a log-likelihood group.\n\ndt_rugby = azp.load_arviz_data('rugby')\nelpd = azp.loo(dt_rugby, var_name=\"home_points\")\nelpd\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 2000 posterior samples and 60 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -282.09    26.49\np_loo       25.16        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)       57   95.0%\n   (0.70, 1]   (bad)         3    5.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nWe can see that we get the estimated ELPD value using LOO and its standard error. p_loo can be roughly interpreted as the effective number of parameters. For some models this number should be close to the actual number of parameters, for models with regularization, like hierarchical models, it should be less than the actual number of parameters. After all this introduction, actually computing LOO may seem somewhat disappointing.\nTo compare two or more models, we can use the compare function, which accepts a dictionary where the keys are the names of the models and the values are DataTree objects.\n\nPyMCCmdStanPy\n\n\n\ntarget = pz.StudentT(nu=4, mu=0, sigma=1).rvs(200, random_state=SEED)\n\nwith pm.Model() as model_n:\n    μ = pm.Normal(\"μ\", 0, 1)\n    σ = pm.HalfNormal(\"σ\", 1)\n    pm.Normal(\"y\", μ, σ, observed=target)\n    idata_n = pm.sample(idata_kwargs={\"log_likelihood\":True})\n    \nwith pm.Model() as model_t:\n    μ = pm.Normal(\"μ\", 0, 1)\n    σ = pm.HalfNormal(\"σ\", 1)\n    ν = pm.Exponential(\"ν\", scale=30)\n    pm.StudentT(\"y\", nu=ν, mu=μ, sigma=σ, observed=target)\n    idata_t = pm.sample(idata_kwargs={\"log_likelihood\":True})\n\n\n\n## coming soon\n\n\n\n\ncmp_df = azp.compare({'model_n':idata_n, 'model_t':idata_t})\ncmp_df\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nmodel_t\n0\n-327.519119\n3.406163\n0.000000\n1.0\n13.470384\n0.000000\nFalse\n\n\nmodel_n\n1\n-334.187731\n3.187292\n6.668612\n0.0\n15.124893\n4.510585\nFalse\n\n\n\n\n\n\n\nIn the rows we have the compared models and in the columns we have\n\nrank: the order of the models (from best to worst)\nelpd_loo: the point estimate of the ELPD using LOO\np_loo: the effective number of parameters\nelpd_diff: the difference between the ELPD of the best model and the other models\nweight: the relative weight of each model. If we wanted to make predictions by combining the different models, instead of choosing just one, this would be the weight we should assign to each model. In this case we see that model_t takes all the weight.\nse: the standard error of the ELPD\ndse: the standard error of the differences\nwarning: a warning about whether there is at least one high k value\nscale: the scale on which the ELPD is calculated\n\nWe can obtain similar information, but graphically, using the azp.plot_compare function\n\nazp.plot_compare(cmp_df);\n\n\n\n\n\n\n\n\n\nThe open circles represent the ELPD values ​​and black lines the standard error.\nThe highest ELPD value is indicated with a vertical dashed gray line for easy comparison with other values.\nFor all models except the best, we also obtain a triangle indicating the value of the ELPD difference between each model and the best model. The gray error bar indicating the standard error of the differences between the point estimates.\n\nThe simplest way to use information criteria is to choose a single model. Simply choose the model with the highest ELPD value. If we follow this rule we will have to accept that the quadratic model is the best. Even if we take into account the standard errors we can see that they do not overlap. Which gives us some security that the models are indeed different from each other. If, instead, the standard errors overlapped, we should provide a more nuanced answer.\n\n7.6.1 Pareto k and LOO diagnostics\nThen we see a table titled “Pareto k diagnostic values”. Earlier we said that we use a method involving a Pareto distribution to regularize the estimation of the importance weights. One of the parameters of that fit is called \\(k\\), and its estimate is often written as \\(\\hat{k}\\). Because we compute a Pareto adjustment per observation, we obtain one \\(\\hat{k}\\) value per observation. This parameter is useful because it tells us two related things: when an observation is “very influential” and when the LOO approximation may be failing for that observation.\nAs a general rule, if \\(\\hat{k}\\) is less than 0.7 there are no problems; if it is between 0.7 and 1 it is very likely that we are in trouble; and if it is greater than 1, we are doomed. The cut-off value 0.7 is not fixed, it can in principle be lower, and it depends on the total number of posterior draws (2000 in this example). But when the number of draws is about 2000, the recommended threshold is close to 0.7. In practice, it is common to use at least 2000 posterior draws. Increasing the number of samples from the posterior may reduce the value of \\(\\hat{k}\\) and thus remove some of these warnings, but in general the number needed will be too large to be practically useful.\nIt is possible to visualize the values ​​of \\(k\\), using plot_khat\n\n# coming soon\n#azp.plot_khat(elpd, threshold=0.7);\n\nWhile the main function of LOO is to compare models, the values ​​of \\(k\\) can be useful even if we only have only one model. For example, we could have extra knowledge that tells us why these observations are influential, perhaps there was a problem in data collection and the values ​​are incorrect. Or perhaps the values ​​are correct but from the perspective of our model they are influential, “strange”, “surprising”.\nIf \\(k &gt; 0.7\\), the value of p_loo can give us some more information, where \\(p\\) is the total number of parameters in a model.\n\nIf \\(p_{\\text{loo}} &lt;&lt; p\\) then the model must be misspecified. This should also be seen in post-hoc predictive testing. One solution is to use an overdispersed model (such as changing a Poisson for a NegativeBinomial or for a ZeroInflatedPoisson or HurdlePoisson, or changing a Normal for a Student’s T, etc.). Or it is likely that the model needs more structure or complexity, perhaps we need a non-linear term, etc.\nIf \\(p_{\\text{loo}} &lt; p\\) and the observations are relatively few compared to \\(p\\), (say \\(p&gt;N/5\\)). It is likely that we have a model that is too flexible and/or priors that are too vague. This can happen for hierarchical models with very few observations per group or for example for splines with many knots or Gaussian processes with very short scale values.\nIf \\(p_{\\text{loo}} &gt; p\\), then the model has very serious problems. If \\(p&lt;&lt;N\\), then posterior predictive tests should also report problems. If, however, p is relatively large (say \\(p&gt;N/5\\)). So post-hoc predictive testing may not reflect problems.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#sec-IWMM-theory",
    "href": "Chapters/Model_comparison.html#sec-IWMM-theory",
    "title": "7  Model Comparison",
    "section": "7.7 When Pareto k values are too high",
    "text": "7.7 When Pareto k values are too high\nThe efficiency of PSIS-LOO-CV depends critically on stable importance sampling, which breaks down when the Pareto-\\(k\\) diagnostic exceeds a threshold (typically around 0.7 for standard MCMC sample sizes), leading to unreliable estimates.\nWhen observations produce high Pareto-\\(k\\) values, we can use standard LOO-CV for those observations, i.e. refit the model while removing those problematic observations one at a time. This could be useful if the number of observations with high \\(\\hat k\\) is small, otherwise it can easily become too expensive for routine use. K-fold CV is another option but it can also be too expensive.\nBut those are not the only two solutions when PSIS-LOO-CV produces high \\(\\hat k\\) values. Instead of expensive model refitting, the method importance weighted moment matching (IWMM), presented by Paananen et al. (2020), works by improving the proposal distribution by iteratively transforming existing posterior draws to better match the moments of the target distribution.\nTo understand how moment matching improves these importance weights, we first need to understand which Monte Carlo estimator we are using.\n\n7.7.1 Monte Carlo estimators\nTwo of the most commonly used estimators are standard importance sampling (IS) and self-normalized importance sampling (SNIS). The choice between IS and SNIS determines which proposal distribution works best, which in turn affects how moment matching should adapt the draws.\nThe central problem we’re interested in is computing \\(\\mathbb{E}_p[h(\\theta)] = \\int h(\\theta) p(\\theta) \\, d\\theta\\) using draws \\(\\{\\theta^{(s)}\\}_{s=1}^S\\) from a proposal distribution \\(g(\\theta)\\).\n\n7.7.1.1 Standard Importance Sampling (IS)\nThe standard importance sampling estimator computes the expectation as \\[\n\\hat{\\mu}_{\\text{IS}} = \\frac{1}{S} \\sum_{s=1}^S w^{(s)} h(\\theta^{(s)}), \\quad w^{(s)} = \\frac{p(\\theta^{(s)})}{g(\\theta^{(s)})},\n\\]\nwhich is unbiased when normalizing constants are known, with variance depending on how well \\(g(\\theta)\\) matches the target in regions where \\(|h(\\theta)|\\) is large.\n\n\n7.7.1.2 Self-Normalized Importance Sampling (SNIS)\nThe SNIS estimator computes the ratio of weighted averages as \\[\n\\hat{\\mu}_{\\text{SNIS}} = \\frac{\\sum_{s=1}^S w^{(s)} h(\\theta^{(s)})}{\\sum_{s=1}^S w^{(s)}}, \\quad w^{(s)} = \\frac{p(\\theta^{(s)})}{g(\\theta^{(s)})}.\n\\]\nThis is a natural choice for PSIS-LOO-CV because the normalizing constants of \\(p(\\theta \\mid y_{-i})\\) and \\(p(\\theta \\mid y)\\) are typically unknown.\nSNIS introduces an additional consideration. We need to estimate both numerator and denominator accurately, which changes which regions of the proposal matter most. The optimal proposal for SNIS emphasizes where \\(|h(\\theta) - \\mathbb{E}_p[h(\\theta)]|\\) is large, whereas for standard IS it emphasizes where \\(|h(\\theta)|\\) is large. In practice, we use self-normalized weights \\(\\tilde{w}^{(s)} = w^{(s)} / \\sum_{r=1}^S w^{(r)}\\) for moment matching since normalizing constants are often unknown.\n\n\n\n7.7.2 Multiple importance sampling\nThe distinction between numerator and denominator adaptation for SNIS creates a challenge. As we will see later, the optimal proposal for the numerator differs from the optimal proposal for the denominator. Rather than choosing between these competing proposals, moment matching uses multiple importance sampling to combine them. MIS provides a principled framework for sampling from \\(J\\) different proposal distributions and combining the results into a single estimator with provably good properties.\nSuppose we independently draw \\(S_j\\) samples from each of \\(J\\) proposal distributions \\(g_1(\\theta), \\ldots, g_J(\\theta)\\), with \\(\\sum_{j=1}^J S_j = S\\) total draws. The multiple importance sampling estimator is\n\\[\\hat{\\mu}_{\\mathrm{MIS}} = \\sum_{j=1}^{J} \\frac{1}{S_j} \\sum_{s=1}^{S_j} \\beta_j(\\theta^{(j,s)}) \\frac{p(\\theta^{(j,s)})}{g_j(\\theta^{(j,s)})} h(\\theta^{(j,s)}),\\]\nwhere \\(\\{\\beta_j(\\theta)\\}_{j=1}^{J}\\) forms a partition of unity with \\(\\beta_j(\\theta) \\geq 0\\) and \\(\\sum_{j=1}^J \\beta_j(\\theta) = 1\\) for all \\(\\theta\\). The weighting functions \\(\\beta_j\\) determine how much each proposal contributes at different points in the parameter space.\nA natural choice for the weighting functions is the balance heuristic\n\\[\\beta_j(\\theta) = \\frac{S_j g_j(\\theta)}{\\sum_{k=1}^J S_k g_k(\\theta)}.\\]\nA critical property of this weighting function is that it can be shown that the variance of the MIS estimator using the balance heuristic is provably smaller than the variance of any single proposal distribution, plus a term that vanishes as the sample sizes grow.\nIntuitively, the balance heuristic computes importance weights as if all draws came from a mixture proposal \\(g_{\\alpha}(\\theta) = \\sum_{j=1}^J \\alpha_j g_j(\\theta)\\) where \\(\\alpha_j = S_j / S\\), which gives us the weights\n\\[w^{(j,s)} = \\frac{p(\\theta^{(j,s)})}{g_{\\alpha}(\\theta^{(j,s)})} = \\frac{p(\\theta^{(j,s)})}{\\sum_{j=1}^J \\alpha_j g_j(\\theta^{(j,s)})}.\\]\nThis framework becomes crucial for moment matching when adapting two separate proposal distributions, because rather than choosing between them, we can combine both using MIS and the balance heuristic.\n\n\n7.7.3 Importance weighted moment matching\nMoment matching extends the idea of PSIS-LOO-CV in a clever way. It implicitly updates the proposal without needing auxiliary parameters or resampling. Traditional adaptive importance sampling alternates between drawing samples, computing weights, and updating parameters. The key insight from Paananen et al. (2020) is simpler: skip generating fresh draws and instead repeatedly transform the existing draws from \\(p(\\theta \\mid y)\\) in the unconstrained parameter space. These transformations align the low-order moments of the draws with the moments implied by the leave-one-out weights.\nThis implicit adaptation is fully automated and requires only operations on already available posterior draws. It’s designed to plug into PSIS-LOO-CV pipelines that flag problematic cases using the Pareto-\\(k\\) diagnostic from Vehtari, Gelman, and Gabry (2017). Because we reuse the same Monte Carlo sample throughout, the computational cost stays close to that of a single PSIS-LOO-CV evaluation, even when several adaptation rounds are attempted. Importantly, this approach preserves the effective sample size accrued during the original posterior simulation and avoids delicate tuning choices.\nThe approach involves several key components:\n\nApply affine transformations to existing draws to better match the target distribution\nRecognize that different Monte Carlo estimators require different proposal strategies\nIdentify optimal proposal distributions for both standard and self-normalized importance sampling\nApproximate the challenging self-normalized optimal proposal using a split proposal that separates numerator and denominator adaptation\nCombine the adapted proposals using multiple importance sampling\n\nThe following sections develop each of these ideas in turn.\n\n7.7.3.1 Affine transformations and implicit adaptation\nThe core idea of importance weighted moment matching (IWMM) is to transform existing draws to better match the target distribution without resampling. We achieve this using affine transformations guided by importance-weighted moments.\nConsider a set of draws \\(\\{\\theta^{(s)}\\}_{s=1}^S\\) from the full-data posterior \\(p(\\theta \\mid y)\\). A generic affine transformation consists of a square matrix \\(\\mathbf{A}\\) and a translation vector \\(\\mathbf{b}\\) such that\n\\[\nT(\\theta^{(s)}) = \\mathbf{A}\\theta^{(s)} + \\mathbf{b} = \\theta^{*(s)}.\n\\]\nSince the transformations are affine and the same for all draws, the implicit density of the transformed draws is\n\\[\ng_T(\\theta^{*(s)}) = p(\\theta^{(s)} \\mid y) |\\mathbf{J}_T|^{-1},\n\\]\nwhere \\(|\\mathbf{J}_T|^{-1} = |\\det(\\mathbf{A})|^{-1}\\) is the inverse Jacobian determinant. Crucially, this allows us to evaluate the proposal density at the transformed draws without ever writing down an explicit functional form. The proposal is adapted implicitly through the transformation, which requires an invertible matrix \\(\\mathbf{A}\\) so that the Jacobian determinant is well defined.\nWhen moment matching is applied to leave-one-out cross-validation, the importance weights must be recomputed after each transformation. For a transformed draw \\(\\theta^{*(s)}\\), the importance weight becomes\n\\[\nw_{\\text{loo}, i}^{*(s)} = \\frac{p(\\theta^{*(s)} \\mid y)}{p(\\theta^{(s)} \\mid y) \\, p(y_i \\mid \\theta^{*(s)})} \\propto \\frac{p(\\theta^{*(s)} \\mid y_{-i})}{p(\\theta^{(s)} \\mid y)},\n\\]\nwhich is still proportional to the ratio of the LOO posterior to the original proposal. This reweighting accounts for both the density of the transformed draw under the full-data posterior and its likelihood for observation \\(i\\). The denominator term \\(p(\\theta^{(s)} \\mid y)\\) remains constant for each draw across all LOO folds, but the numerator terms must be evaluated at each transformed location.\n\n\n7.7.3.2 Targeting optimal proposal distributions\nThe affine transformations need some target to guide them. The choice of target determines whether moment matching successfully reduces the Pareto-\\(k\\) diagnostic. The theoretically optimal proposal density depends on both the function \\(h(\\theta)\\) being estimated and the weighting scheme used.\nFor standard importance sampling, the optimal proposal is\n\\[\ng_{\\mathrm{IS}}^{\\mathrm{opt}}(\\theta) \\propto p(\\theta)\\lvert h(\\theta)\\rvert,\n\\]\nwhich emphasizes regions where \\(|h(\\theta)|\\) is large. For self-normalized importance sampling, the optimal proposal is\n\\[\ng_{\\mathrm{SNIS}}^{\\mathrm{opt}}(\\theta) \\propto p(\\theta)\\left|h(\\theta)-\\mathbb{E}_{p}[h(\\theta)]\\right|,\n\\]\nwhich emphasizes regions where \\(h(\\theta)\\) deviates from its expectation.\nThe difference arises because SNIS must estimate both numerator and denominator accurately. When \\(h(\\theta)\\) varies substantially, we need draws where \\(h(\\theta)\\) deviates most from its mean. When \\(h(\\theta)\\) is roughly constant, we need draws emphasizing posterior mass. Moment matching uses the available importance weights to steer affine transformations toward these optimal shapes.\nTo make this work in practice, we use two types of weights:\n\nCommon weights \\(w^{(s)} = p(\\theta^{(s)}) / g(\\theta^{(s)})\\) quantify the mismatch between proposal and target independent of the function \\(h\\)\nExpectation-specific weights \\(v^{(s)} = h(\\theta^{(s)}) w^{(s)}\\) weight each draw by both the density ratio and the integrand value\n\nThe transformations are calibrated with \\(\\{w^{(s)}\\}\\) or with \\(\\{|v^{(s)}|\\}\\), depending on whether the denominator or numerator of the SNIS estimator needs improvement. We use absolute values \\(|v^{(s)}|\\) because both positive and negative values of \\(h(\\theta)\\) contribute equally to variance, so we want draws where the magnitude is large regardless of sign.\n\n\n7.7.3.3 The split proposal for SNIS\nThe SNIS optimal proposal from the previous section presents a practical challenge. It can easily be multimodal even when the expectation is defined over a unimodal distribution, making it difficult to approximate with simple affine transformations. It also depends on the unknown expectation \\(\\mathbb{E}_p[h(\\theta)]\\).\n\n\n7.7.3.4 The split proposal approximation\nTo combine the two adaptation strategies into an efficient proposal distribution, Paananen et al. (2020) develop a practical approximation called the split proposal density given by\n\\[\ng_{\\mathrm{SNIS}}^{\\mathrm{split}}(\\theta) \\propto |h(\\theta)|p(\\theta) + \\mathbb{E}_{p}[h(\\theta)]p(\\theta).\n\\]\nThis approximation splits the piecewise-defined optimal proposal into two clear components. The first component \\(|h(\\theta)|p(\\theta)\\) is proportional to the optimal proposal for standard importance sampling and uses the absolute expectation-specific weights \\(\\{|v^{(s)}|\\}\\). The second component \\(\\mathbb{E}_{p}[h(\\theta)]p(\\theta)\\) is proportional to the target distribution \\(p(\\theta)\\) itself and uses the common weights \\(\\{w^{(s)}\\}\\). This split form is a convenient approximation that has similar tail behavior to the SNIS optimal proposal while being simpler to work with. It avoids the potentially complex multimodal structure of \\(|h(\\theta) - \\mathbb{E}_p[h(\\theta)]|\\) by separating it into two interpretable components.\nThe trade-off is that this approximation places unnecessary probability mass in regions where \\(h(\\theta) \\approx \\mathbb{E}_p[h(\\theta)]\\), thus losing some efficiency. However, the approximation works best precisely when we need it most. When \\(p(\\theta)\\) is more distinct from \\(p(\\theta)|h(\\theta)|\\), the value of \\(\\mathbb{E}_p[h(\\theta)]\\) becomes smaller and the approximation becomes closer to the optimal form. These are exactly the cases when adaptive importance sampling is most beneficial.\n\n\n7.7.3.5 Double adaptation strategy\nThe split proposal naturally suggests a double adaptation strategy. We run two separate moment matching procedures in parallel: one uses the absolute expectation-specific weights \\(\\{|v^{(s)}|\\}\\) to approximate the first component, and the other uses the common weights \\(\\{w^{(s)}\\}\\) to approximate the second. These two adapted proposal distributions are then combined using multiple importance sampling with the balance heuristic.\nWhen \\(h(\\theta) \\geq 0\\), both components integrate to \\(\\mathbb{E}_p[h(\\theta)]\\). Paananen et al. (2020) show that equal allocation (50-50 split) is provably optimal in this case and more generally is a conservative choice that guarantees the asymptotic variance is never more than twice that of using the better component alone.\n\n\n7.7.3.6 Three moment matching transformations\nOur overall goal is to transform the existing posterior draws to match the optimal proposals. To do this, we match progressively more moments of the distribution to importance-weighted moments.\nPaananen et al. (2020) recommend a sequence of three affine transformations with increasing complexity. The transformations use importance weights to identify where the target distribution places its mass relative to the proposal. We start with the simplest transformation and progress to more complex ones only when simpler ones no longer improve the Pareto-\\(k\\) diagnostic.\nTransformation \\(T_1\\) (matching the mean). The simplest transformation shifts the sample mean to match the importance-weighted mean\n\\[\n\\theta^{*(s)} = T_1(\\theta^{(s)}) = \\theta^{(s)} - \\bar{\\theta} + \\bar{\\theta}_w,\n\\]\nwhere the unweighted mean is \\(\\bar{\\theta} = \\frac{1}{S} \\sum_{s=1}^S \\theta^{(s)}\\) and the self-normalized importance-weighted mean is\n\\[\n\\bar{\\theta}_w = \\frac{\\sum_{s=1}^S w^{(s)} \\theta^{(s)}}{\\sum_{s=1}^S w^{(s)}} = \\sum_{s=1}^S \\tilde{w}^{(s)} \\theta^{(s)},\n\\]\nwith \\(\\tilde{w}^{(s)} = w^{(s)} / \\sum_{r=1}^S w^{(r)}\\) denoting the self-normalized weights.\nTransformation \\(T_2\\) (matching marginal variances). The second transformation matches both the mean and the marginal variances\n\\[\n\\theta^{*(s)} = T_2(\\theta^{(s)}) = \\mathbf{v}_w^{1/2} \\circ \\mathbf{v}^{-1/2} \\circ (\\theta^{(s)} - \\bar{\\theta}) + \\bar{\\theta}_w,\n\\]\nwhere \\(\\circ\\) denotes element-wise multiplication, \\(\\mathbf{v}\\) is the vector of unweighted marginal variances, and \\(\\mathbf{v}_w\\) is the vector of importance-weighted marginal variances computed as\n\\[\n\\mathbf{v}_w = \\sum_{s=1}^S \\tilde{w}^{(s)} (\\theta^{(s)} - \\bar{\\theta}_w) \\circ (\\theta^{(s)} - \\bar{\\theta}_w) = \\frac{\\sum_{s=1}^S w^{(s)} (\\theta^{(s)} - \\bar{\\theta}_w) \\circ (\\theta^{(s)} - \\bar{\\theta}_w)}{\\sum_{s=1}^S w^{(s)}}.\n\\]\nTransformation \\(T_3\\) (matching the covariance). The most sophisticated transformation matches the full covariance structure\n\\[\n\\theta^{*(s)} = T_3(\\theta^{(s)}) = \\mathbf{L}_w \\mathbf{L}^{-1} (\\theta^{(s)} - \\bar{\\theta}) + \\bar{\\theta}_w,\n\\]\nwhere \\(\\mathbf{L}\\mathbf{L}^\\top = \\boldsymbol{\\Sigma}\\) and \\(\\mathbf{L}_w\\mathbf{L}_w^\\top = \\boldsymbol{\\Sigma}_w\\) are Cholesky decompositions of the sample and weighted covariance matrices, with the weighted covariance computed as\n\\[\n\\boldsymbol{\\Sigma}_w = \\sum_{s=1}^S \\tilde{w}^{(s)} (\\theta^{(s)} - \\bar{\\theta}_w)(\\theta^{(s)} - \\bar{\\theta}_w)^\\top = \\frac{\\sum_{s=1}^S w^{(s)} (\\theta^{(s)} - \\bar{\\theta}_w)(\\theta^{(s)} - \\bar{\\theta}_w)^\\top}{\\sum_{s=1}^S w^{(s)}}.\n\\]\n\n\n7.7.3.7 The moment matching algorithm\nThe ArviZ implementation uses a greedy search strategy for each observation with high Pareto-\\(k\\) values. For each problematic observation, the algorithm iteratively tries the three transformations in sequence. When the mean shift transformation (\\(T_1\\)) reduces the Pareto-\\(k\\) diagnostic, it is accepted and the algorithm restarts from \\(T_1\\) again. Only when \\(T_1\\) fails to improve the diagnostic does the algorithm try the variance transformation (\\(T_2\\)), and similarly for the covariance transformation (\\(T_3\\)) when enabled. This process continues until no transformation improves the diagnostic or the maximum number of iterations is reached.\nAfter the transformations converge, ArviZ applies an optional split proposal strategy by default. The cumulative transformation is applied to the first half of the posterior draws while the inverse transformation is applied to the second half. These two sets of transformed draws are then combined using multiple importance sampling with the balance heuristic. This split approach helps avoid potential bias from reusing the same draws and often improves the stability of the final estimates. The entire workflow is automated through the loo_moment_match() function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#sec-subsampledloo-theory",
    "href": "Chapters/Model_comparison.html#sec-subsampledloo-theory",
    "title": "7  Model Comparison",
    "section": "7.8 When the dataset is too large",
    "text": "7.8 When the dataset is too large\nComparing Bayesian models on large datasets presents a computational challenge. Full leave-one-out cross-validation (LOO) requires computing \\(n\\) posteriors and brute force K-fold cross-validation methods become prohibitively expensive as data size grows.\nTo overcome these challenges, we will use the approach described in Magnusson et al. (2020) to combine fast approximations with targeted, exact computations. The key idea is to use cheap approximations, called surrogates, for all \\(n\\) observations, and then correct these approximations using exact Pareto smoothed importance sampling (PSIS) LOO-CV on just a small random subsample with a difference estimator that corrects for the approximation error across the full dataset.\n\n7.8.1 From LOO to subsampled LOO\nAs mentioned earlier in this chapter, PSIS-LOO-CV uses importance sampling to approximate LOO-CV without refitting the model \\(n\\) times. The Pareto-\\(k\\) diagnostic flags unreliable importance sampling approximations when \\(k &gt; 0.7\\). For large \\(n\\), even this efficient computation becomes expensive.\nThe key insight for large data is to combine three elements:\n\nFast surrogates \\(\\tilde\\pi_i\\) computed for all \\(n\\) observations (e.g., point predictions)\nExact PSIS-LOO-CV computed only on a small random subsample \\(\\mathcal{S}\\) of size \\(m \\ll n\\)\nThe difference estimator to correct the surrogates using the subsample\n\nThese elements dramatically reduce computational cost while maintaining statistical accuracy. We can also extend the importance ratios with the correction term \\(p(\\theta_s \\mid y)/q(\\theta_s \\mid y)\\) when using approximate posteriors from variational inference or Laplace approximations to further reduce computational cost.\nA surrogate is simply a computationally inexpensive approximation of each observation’s LOO predictive density, such as evaluating the likelihood at the posterior mean, that doesn’t require refitting the model. The difference estimator then uses the subsample to correct for approximation errors across the full dataset. The result is accurate estimates of predictive performance with a fraction of the computational cost, making model comparison feasible even when \\(n\\) is very large.\nA critical component of the difference estimator is using simple random sampling to select the subsample, rather than model-specific importance sampling schemes. This allows the auxiliary information, e.g., the surrogates \\(\\tilde\\pi_i\\), to be used in the estimation stage rather than the sampling stage, meaning the same subsample can be efficiently reused for all models and all quantities of interest. Alternative approaches that incorporate model-specific information during sampling, such as the Hansen-Hurwitz estimator, which we will describe later, require drawing separate subsamples for each model and each comparison, multiplying computational costs.\nThe quality of surrogate approximations \\(\\tilde\\pi_i\\) plays a key role in determining the required subsample size. Surrogates that incorporate model complexity through the effective number of parameters \\(p_{\\text{eff}} = V_\\theta(\\log p(y_i \\mid \\theta))\\) tend to approximate the true LOO contributions better than simple point predictions, especially for hierarchical or weakly identified models. This improvement can reduce the required subsample size by orders of magnitude. However, computing better surrogates requires more computation per observation, which creates a fundamental trade-off between surrogate quality and subsample size that should be carefully considered in practice.\n\n\n7.8.2 Model comparison for large data\nWhen comparing models \\(A\\) and \\(B\\), we are interested in the difference in predictive performance\n\\[\n\\operatorname{elpd}_D = \\operatorname{elpd}_A - \\operatorname{elpd}_B,\n\\]\nalong with its variance\n\\[\nV(\\operatorname{elpd}_D) = V(\\operatorname{elpd}_A) + V(\\operatorname{elpd}_B) - 2\\operatorname{Cov}(\\operatorname{elpd}_A, \\operatorname{elpd}_B).\n\\]\nThe covariance term is crucial and often overlooked in practice. When models make similar predictions for most observations, which is common when comparing related models, the predictions are highly correlated, making \\(\\operatorname{Cov}(\\operatorname{elpd}_A, \\operatorname{elpd}_B)\\) large and positive. This correlation dramatically reduces the variance of the difference compared to naively summing individual variances.\nThis is why using a common subsample across models is essential. Different random subsamples lose this correlation and fail to capture the variance reduction, whereas the same subsample preserves the correlation structure and accurately estimates both \\(\\operatorname{elpd}_D\\) and its much smaller variance, for an exmple see ?sec-sec-model-comparison-large-data.\n\n\n7.8.3 Previous approaches and practical limitations\nBefore describing the central difference estimator used in LOO-CV for large data, it is important to understand the limitations of previous methods for scaling LOO to large data and model comparison. One previous approach is the Hansen-Hurwitz (HH) estimator (Hansen and Hurwitz 1943) which uses importance sampling with auxiliary information incorporated at the sampling stage. For estimating \\(\\operatorname{elpd}_{\\text{loo}}\\), observations are subsampled with probability proportional to an approximate LOO contribution \\(\\tilde{\\pi}_i\\) giving\n\\[\n\\widehat{\\operatorname{elpd}}_{\\text{HH}} = \\frac{1}{m} \\sum_{j \\in \\mathcal{S}} \\frac{1}{\\tilde{\\pi}_j} \\pi_j,\n\\]\nwhere \\(\\mathcal{S}\\) is the subsample of size \\(m\\). While the HH estimator works well for individual model evaluation, it has two key limitations for model comparison:\n\nSince the auxiliary information \\(\\tilde{\\pi}_i\\) is model-specific and used in the sampling step, each model requires a different subsample. Comparing models or estimating \\(V(\\operatorname{elpd}_D)\\) requires drawing additional subsamples for each quantity of interest, multiplying computational costs.\nUsing point estimates like \\(\\log p(y_i \\mid \\hat{\\theta})\\) as auxiliary information ignores the effective number of parameters \\(p_{\\text{eff}}\\) and can lead to poor approximations for complex models and requiring larger subsample sizes.\n\nThese practical limitations motivate the difference estimator approach described below, which uses the same subsample for all models and incorporates model complexity directly in the surrogate approximation.\n\n\n7.8.4 Difference estimator for large data\nThe key to subsampling LOO for large datasets is the difference estimator, which corrects the bias in fast approximations of the full-data LOO. Let \\(\\pi_i = \\log p(y_i\\mid y_{-i})\\) denote the exact LOO predictive density for observation \\(i\\), and let \\(\\tilde\\pi_i\\) be a computationally efficient approximation. For a simple random subsample \\(\\mathcal S\\) of size \\(m\\), the difference estimator is given by\n\\[\n\\widehat{\\operatorname{elpd}}_{\\text{diff, loo}}\n= \\sum_{i=1}^n \\tilde\\pi_i\n+ \\frac{n}{m} \\sum_{j\\in\\mathcal S} (\\pi_j - \\tilde\\pi_j).\n\\]\nIts subsampling variance is\n\\[\nV\\bigl(\\widehat{\\operatorname{elpd}}_{\\text{diff, loo}}\\bigr)\n= n^2\\!\\left(1-\\frac{m}{n}\\right) \\frac{s_e^2}{m},\n\\]\nwhere \\(s_e^2 = \\frac{1}{m-1} \\sum_{j\\in\\mathcal S} (e_j-\\bar e)^2\\) is the sample variance of the approximation error \\(e_j=\\pi_j-\\tilde\\pi_j\\), and \\(\\bar e = \\frac{1}{m} \\sum_{j\\in\\mathcal S} e_j\\).\nThe difference estimator has two key properties that make it ideal for large-data model comparison:\n\nThe variance decreases as the surrogate quality improves, e.g., as \\(\\tilde\\pi_i \\to \\pi_i\\), the approximation error \\(e_j \\to 0\\) and the subsampling variance vanishes.\nThe term \\((1-m/n)\\) is a finite population correction factor that ensures the variance also vanishes as the subsample size approaches the full dataset size. This means we can start with a small subsample and incrementally increase it until we achieve the desired precision, without wasting computational effort.\n\nCrucially, the same subsample can be reused across all models. To estimate model differences \\(\\operatorname{elpd}_D = \\operatorname{elpd}_A - \\operatorname{elpd}_B\\), we simply apply the difference estimator with difference surrogates \\(\\tilde\\pi_{i,D} = \\tilde\\pi_{i,A} - \\tilde\\pi_{i,B}\\) and exact differences \\(\\pi_{j,D} = \\pi_{j,A} - \\pi_{j,B}\\) on the common subsample.\nFor model comparison, we also need the variability of pointwise ELPD contributions, \\(\\sigma^2_{\\text{loo}} = \\tfrac{1}{n}\\sum_i (\\pi_i-\\bar\\pi)^2\\). Using the same subsample with squared surrogates \\(\\tilde\\pi_i^2\\) yields the unbiased estimator\n\\[\n\\begin{align}\n\\hat{\\sigma}^2_{\\text{diff, loo}}\n= \\sum_{i=1}^n \\tilde\\pi_i^2 &+ \\frac{n}{m} \\sum_{j\\in\\mathcal S} (\\pi_j^2 - \\tilde\\pi_j^2) \\\\\n&\\quad + \\frac{1}{n}\\biggl[\\Bigl( \\frac{n}{m}\\sum_{j\\in\\mathcal S} (\\pi_j-\\tilde\\pi_j) \\Bigr)^2 - V\\bigl(\\widehat{\\operatorname{elpd}}_{\\text{diff, loo}}\\bigr)\\biggr] \\\\\n&\\quad + \\frac{1}{n}\\biggl[ 2\\Bigl(\\sum_{i=1}^n \\tilde\\pi_i\\Bigr)\\widehat{\\operatorname{elpd}}_{\\text{diff, loo}} - \\Bigl(\\sum_{i=1}^n \\tilde\\pi_i\\Bigr)^2 \\biggr].\n\\end{align}\n\\]\n\n\n\n\n\n\nNoteUnbiasedness of the difference estimator\n\n\n\nThe estimators \\(\\widehat{\\operatorname{elpd}}_{\\text{diff, loo}}\\) and \\(\\hat{\\sigma}^2_{\\text{diff, loo}}\\) are unbiased for \\(\\operatorname{elpd}_{\\text{loo}}\\) and \\(\\sigma^2_{\\text{loo}}\\), respectively (Magnusson et al. 2020). In practice, \\(\\hat{\\sigma}^2_{\\text{diff, loo}}\\) tends to be optimistic since no general unbiased estimator of the true variability exists in cross-validation (Bengio and Grandvalet 2004).\n\n\n\n\n7.8.5 Fast LOO surrogates\nFor the difference estimator to be efficient, we need good approximations of the LOO contributions. We want surrogates \\(\\tilde\\pi_i\\) that\n\nApproximate \\(\\pi_i\\) well in finite samples\nAre computationally cheap\nConverge in mean, \\(\\mathbb{E}|\\pi_i - \\tilde\\pi_i| \\to 0\\) as \\(n \\to \\infty\\)\n\nNotably, the third property ensures favorable scaling characteristics as the data size grows.\n\n7.8.5.1 Practical surrogates: PLPD and LPD\nIn practice, the function loo_subsample in ArviZ implements two main surrogates.\nThe first is the point log predictive density (PLPD) and is given by\n\\[\n\\tilde\\pi_i^{\\text{PLPD}} = \\log p(y_i \\mid \\hat\\theta),\n\\]\nwhere \\(\\hat\\theta\\) is typically the posterior mean. This is computationally cheapest but ignores posterior uncertainty.\nThe second is the log predictive density (LPD) and is given by\n\\[\n\\tilde\\pi_i^{\\text{LPD}} = \\log \\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i \\mid \\theta_s)\\right),\n\\]\nwhich averages over posterior draws before taking the logarithm. The approximation error \\(e_j = \\pi_j - \\tilde\\pi_j\\) directly affects the subsampling variance, so better surrogates allow smaller subsamples for the same precision.\n\n\n\n7.8.6 Asymptotic guarantees and assumptions\nThe difference estimator has strong theoretical foundations, with convergence guarantees under mild regularity conditions. Remarkably, convergence accelerates as the data size grows, even when the subsample size and number of posterior draws are fixed, making the method more efficient precisely when it is needed most.\nThe required assumptions are standard regularity conditions satisfied by most common parametric models including regression, GLMs, and hierarchical models. For detailed theoretical treatment of the asymptotic properties and regularity conditions, see Magnusson et al. (2020).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#absolute-metrics",
    "href": "Chapters/Model_comparison.html#absolute-metrics",
    "title": "7  Model Comparison",
    "section": "7.9 Absolute metrics",
    "text": "7.9 Absolute metrics\nIn the previous sections, we have seen how to compare models using relative metrics. By relative we mean that we are comparing the models with respect to each other. For example, we can say that model A is better than model B because it has a higher ELPD value. But we can not, in general, judge a single model by its ELPD alone. In contrast, sometimes we are interested in absolute metrics. Some common absolute metrics are the root mean square error (RMSE), the mean absolute error (MAE), the coeﬀicient of determination (\\(R^2\\)), etc. Interestingly, we can use PSIS-LOO-CV procedure to compute the leave-one-out cross-validation version of these metrics.\n\n7.9.1 LOO expectations and metrics\nFrom the PSIS procedure, we obtain a set of weights. The loo function utilizes these weights to estimate the ELPD we should have obtained if we have performed leave-one-out cross-validation. Furthermore, these weights can be used to estimate other quantities, such as the mean, standard deviation, and quantiles, as if we had performed leave-one-out cross-validation. For example, to compute the 25th and 75th percentiles we use:\n\nazp.loo_expectations(dt_rugby, kind=\"quantile\", probs=[0.25, 0.75])\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n(&lt;xarray.DataArray 'home_points' (quantile: 2, match: 60)&gt; Size: 960B\n array([[44.        , 12.        , 25.        , 18.        , 14.        ,\n         32.        , 21.        , 15.        , 16.        , 38.        ,\n         16.        , 20.        , 10.        , 23.        , 10.        ,\n         18.        ,  9.        , 21.        , 45.        , 20.        ,\n         13.        , 33.        , 15.        , 17.        , 14.        ,\n         30.        , 14.        , 11.        , 10.        , 22.        ,\n         33.        , 13.        , 18.        , 11.        , 26.        ,\n         10.        , 22.        , 15.        , 16.        , 37.        ,\n         21.        , 14.        , 37.        , 24.        , 13.        ,\n         10.        , 24.        , 12.        ,  8.        , 18.        ,\n         20.        , 13.        , 20.        , 47.        , 14.        ,\n         12.        , 25.        , 31.        , 14.        , 17.        ],\n        [54.        , 18.        , 32.        , 24.        , 20.        ,\n         41.        , 28.        , 21.        , 23.        , 47.        ,\n         22.        , 28.        , 15.        , 31.        , 15.        ,\n         25.        , 13.        , 28.        , 56.        , 27.        ,\n         19.        , 42.        , 21.        , 23.        , 20.        ,\n         39.        , 19.        , 16.        , 16.        , 28.        ,\n         41.        , 19.        , 25.        , 16.        , 35.        ,\n         16.        , 29.        , 21.        , 22.        , 46.        ,\n         28.        , 20.        , 47.        , 31.        , 19.        ,\n         14.        , 31.        , 17.        , 13.        , 25.        ,\n         27.        , 18.98637825, 27.        , 57.        , 20.        ,\n         17.        , 33.11938331, 39.        , 20.        , 23.        ]])\n Coordinates:\n   * quantile   (quantile) float64 16B 0.25 0.75\n   * match      (match) &lt;U16 4kB 'Wales Italy' ... 'Ireland England'\n     home_team  (match) &lt;U8 2kB 'Wales' 'France' 'Ireland' ... 'France' 'Ireland'\n     away_team  (match) &lt;U8 2kB 'Italy' 'England' ... 'Wales' 'England',\n &lt;xarray.DataArray 'home_points' (match: 60)&gt; Size: 480B\n array([0.81019699, 0.28836737, 0.19853754, 0.13917616, 0.3967199 ,\n        0.23962001, 0.16479781, 0.14713066, 0.3616447 , 0.1013727 ,\n        0.09210699, 0.17722584, 0.09801876, 0.73061293, 0.17059836,\n        0.23174109, 0.25935113, 0.22608703, 0.11200082, 0.19183078,\n        0.347671  , 0.62174049, 0.00583348, 0.04696175, 0.30153295,\n        0.28010278, 0.3377815 , 0.09288675, 0.15444295, 0.74229683,\n        0.35880752, 0.19010797, 0.21098666, 0.22112489, 0.17530626,\n        0.12022844, 0.10181887, 0.14713066, 0.10800236, 0.23071062,\n        0.25231008, 0.30349302, 0.4710015 , 0.17825569, 0.18154059,\n        0.3013977 , 0.16174767, 0.23836638, 0.16261163, 0.23174109,\n        0.14606708, 0.50457581, 0.17899662, 0.20756025, 0.28074787,\n        0.14313128, 0.70480686, 0.41149501, 0.00931864, 0.08937109])\n Coordinates:\n   * match      (match) &lt;U16 4kB 'Wales Italy' ... 'Ireland England'\n     home_team  (match) &lt;U8 2kB 'Wales' 'France' 'Ireland' ... 'France' 'Ireland'\n     away_team  (match) &lt;U8 2kB 'Italy' 'England' ... 'Wales' 'England')\n\n\nSimilarly we may be interested in computing estimates of leave-one-out predictive metrics given a set of predictions and observations. For instance to compute the root mean square error we can do:\n\nazp.loo_metrics(dt_rugby, kind=\"rmse\", var_name=\"home_points\")\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nrmse(mean=11.0, se=1.2)\n\n\nAnoth\n\nazp.loo_r2(dt_rugby, var_name=\"home_points\")\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nloo_R2(mean=0.35, eti_lb=0.14, eti_ub=0.51)\n\n\nNotice that for loo_metrics and loo_r2 if we have more than one variable we must specify the variable that we care using the var_names argument, like in the previous examples.\n\n\n7.9.2 LOO-PIT\nAnother quantity of interest that we can obtain via PSIS-LOO-CV is the PIT values. As already mentioned in Section 5.2.4, we often are interested in computing:\n\\[\np(\\tilde y_i \\le y_i \\mid y_{-i})\n\\]\nThat is, we are evaluating the model’s ability to predict an observation when we remove that observation from the observed data. We can use PSIS-LOO_CV to estimate this from a single model fit.\n\nazp.plot_loo_pit(dt_rugby, var_names=\"home_points\")\n\n\n\n\n\n\n\nFigure 7.2: LOO-PIT plot for the rugby model",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#other-information-criteria",
    "href": "Chapters/Model_comparison.html#other-information-criteria",
    "title": "7  Model Comparison",
    "section": "7.10 Other information criteria",
    "text": "7.10 Other information criteria\nPSIS-LOO-CV and WAIC converge asymptotically and are based on the same set of assumptions, so theoretically they are equivalent. However, in practice PSIS-LOO-CV is more robust and also offers a diagnostic that indicates when it could be failing (thanks to the Pareto adjustment). For this reason, ArviZ only implements PSIS-LOO-CV. We now briefly discuss two other information criteria that are somewhat popular.\nDeviance Information Criteria, DIC, if we use the bayes-o-meter™, DIC is more Bayesian than AIC but less than WAIC. Although still popular, PSIS-LOO_CV (and also WAIC) have proven to be more useful both theoretically and empirically than DIC. Therefore we DO NOT recommend its use.\nBayesian Information Criterion (BIC) was proposed as a way to correct some of the problems with AIC, and the author provided a Bayesian rationale for it. But BIC is not really Bayesian in the sense that, like AIC, it assumes flat priors and uses maximum likelihood estimation. More importantly, BIC differs from AIC and WAIC in its objective. AIC and WAIC aim to reflect which model generalizes better to new data (predictive accuracy), while BIC tries to identify the correct model and is therefore more closely related to Bayes factors than to WAIC. In the next section, we discuss Bayes factors and how they differ from PSIS-LOO-CV and WAIC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#bayes-factors",
    "href": "Chapters/Model_comparison.html#bayes-factors",
    "title": "7  Model Comparison",
    "section": "7.11 Bayes factors",
    "text": "7.11 Bayes factors\nAn alternative to cross-validation, approximate cross-validation with LOO and information criteria is Bayes factors. It is common for Bayes factors to show up in the literature as a Bayesian alternative to frequentist hypothesis testing.\nWe can compare \\(K\\) models by computing their marginal likelihood, \\(p(y \\mid M_k)\\), i.e., the probability of the observed data \\(Y\\) given the model \\(M_K\\). The marginal likelihood is the normalization constant of Bayes’ theorem. We can see this if we write Bayes’ theorem and make explicit the fact that all inferences depend on the model.\n\\[\np (\\theta \\mid Y, M_k ) = \\frac{p(Y \\mid \\theta, M_k) p(\\theta \\mid M_k)}{p(Y \\mid M_k)}\n\\]\nwhere, \\(Y\\) is the data, \\(\\theta\\) is the parameters, and \\(M_K\\) is a model out of \\(K\\) competing models.\nIf our main objective is to choose only one model, the best from a set of models, we can choose the one with the largest value of \\(p(y \\mid M_k)\\). This is fine if we assume that all models have the same prior probability. Otherwise, we must calculate:\n\\[\np(M_k \\mid y) \\propto p(y \\mid M_k) p(M_k)\n\\]\nIf, instead, our main objective is to compare models to determine which are more likely and to what extent, this can be achieved using the Bayes factors:\n\\[\nBF_{01} = \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\n\\]\nThat is the ratio between the marginal likelihood of two models. The higher the value of \\(BF_{01}\\), the better the model in the numerator (\\(M_0\\) in this example). To facilitate the interpretation of the Bayes factors, and to put numbers into words, Harold Jeffreys proposed a scale for their interpretation, with levels of support or strength, see the following table.\n\n\n\nBayes Factor\nSupport\n\n\n\n\n1–3\nAnecdotal\n\n\n3–10\nModerate\n\n\n10–30\nStrong\n\n\n30–100\nVery Strong\n\n\n&gt;100\nExtreme\n\n\n\nKeep in mind that if you get numbers below 1, then the support is for \\(M_1\\), i.e., the model in the denominator. Tables are also available for those cases, but notice that you can simply take the inverse of the obtained value.\nIt is very important to remember that these rules are just conventions – simple guides at best. Results should always be put in the context of our problems and should be accompanied by enough detail so that others can assess for themselves whether they agree with our conclusions. The proof necessary to ensure something in particle physics, or in court, or to decide to carry out an evacuation in the face of a looming natural catastrophe is not the same.\n\n7.11.1 Some observations\nWe will now briefly discuss some key facts about the marginal likelihood:\n\nThe good: Occam’s razor included. Models with lots of parameters have a higher penalty than models with few parameters. The intuitive reason is that the greater the number of parameters, the more the prior extends with respect to the likelihood. An example where it is easy to see this is with nested models: for example, a polynomial of order 2 “contains” the models polynomial of order 1 and polynomial of order 0.\nThe bad: For many problems, the marginal likelihood cannot be calculated analytically. Also, approximating it numerically is usually a difficult task that in the best of cases requires specialized methods and, in the worst case, the estimates are either impractical or unreliable. In fact, the popularity of the MCMC methods is that they allow obtaining the posterior distribution without the need to calculate the marginal likelihood.\nThe ugly: The marginal likelihood depends very sensitively on the prior distribution of the parameters in each model \\(p(\\theta_k \\mid M_k)\\).\n\nIt is important to note that the good and the ugly points are related. Using marginal likelihood to compare models is a good idea because it already includes a penalty for complex models (which helps us prevent overfitting), and at the same time, a change in the prior will affect the marginal likelihood calculations. At first, this sounds a bit silly; we already know that priors affect calculations (otherwise we could just avoid them). But we are talking about changes in the prior that would have a small effect in the posterior but a great impact on the value of the marginal likelihood.\nThe use of Bayes factors is often a watershed among Bayesians. The difficulty of its calculation and the sensitivity to the priors are some of the arguments against it. Another reason is that, like p-values and hypothesis testing in general, Bayes factors favor dichotomous thinking over the estimation of the “effect size.” In other words, instead of asking ourselves questions like: How many more years of life can a cancer treatment provide? We end up asking if the difference between treating and not treating a patient is “statistically significant.” Note that this last question can be useful in some contexts. The point is that in many other contexts, this type of question is not the question that interests us; we’re only interested in the one that we were taught to answer.\n\n\n7.11.2 Calculation of Bayes factors\nThe marginal likelihood (and the Bayes factors derived from it) is generally not available in closed form, except for a few models. For this reason, many numerical methods have been devised for its calculation. Some of these methods are so simple and naive that they work very poorly in practice. We are going to discuss only one way to compute them, once that can be applied under some particular cases.\n\n\n7.11.3 Savage–Dickey ratio\nThere are times when we want to compare a null hypothesis \\(H_0\\) (or null model) against an alternative \\(H_1\\) hypothesis. For example, to answer the question “Is this coin biased?”, we could compare the value \\(\\theta = 0.5\\) (representing no bias) with the output of a model in which we allow \\(\\theta\\) to vary. For this type of comparison, the null model is nested within the alternative, which means that the null is a particular value of the model we are building. In those cases, calculating the Bayes factor is very easy and does not require any special methods. We only need to compare the prior and posterior evaluated at the null value (for example, \\(\\theta = 0.5\\)) under the alternative model. We can see that this is true from the following expression:\n\\[\nBF_{01} = \\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\frac{p(\\theta=0.5 \\mid y, H_1)}{p(\\theta=0.5 \\mid H_1)}\n\\]\nThis is true only when \\(H_0\\) is a particular case of \\(H_1\\), see The Book of Statistical Proofs (Soch et al. 2024).\nLet’s do it. We only need to sample the prior and posterior for a model. Let’s try the BetaBinomial model with a Uniform prior:\n\nPyMCCmdStanPy\n\n\n\ny = np.repeat([1, 0], [50, 50])  # 50 heads, 50 tails\nwith pm.Model() as model_uni:\n    a = pm.Beta(\"a\", 1, 1)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_uni = pm.sample(2000, random_seed=42)\n    idata_uni.extend(pm.sample_prior_predictive(8000))\n\n\n\n## coming soon\n\n\n\nAnd now we call azp.plot_bf\n\nazp.plot_bf(idata_uni, var_names=\"a\", ref_val=0.5);\n\n\n\n\n\n\n\n\nIn the previous Figure we can see one KDE for the prior (black) and one for the posterior (gray). The two black dots show that we evaluated both distributions at the value 0.5. We can see that the Bayes factor in favor of the null hypothesis, BF_01, is \\(\\approx 8\\), which we can interpret as moderate evidence in favor of the null hypothesis.\nAs we have already discussed, the Bayes factors measure which model, as a whole, is better at explaining the data. This includes the prior, even for models that the prior has a relatively low impact on the computation of the posterior. We can also see this prior effect by comparing a second model to the null model.\nIf, instead, our model were a BetaBinomial with a prior Beta(30, 30), the BF_01 would be lower ( on the Jeffrey scale). This is because, according to this model, the value of \\(\\theta=0.5\\) is much more likely a priori than for a Uniform prior, and therefore the prior and posterior will be much more similar. That is, it is not very to see that the posterior is concentrated around 0.5 after collecting data. Don’t just believe me, let’s calculate it:\n\nPyMCCmdStanPy\n\n\n\nwith pm.Model() as model_conc:\n    a = pm.Beta(\"a\", 30, 30)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_conc = pm.sample(2000, random_seed=42)\n    idata_conc.extend(pm.sample_prior_predictive(8000))\n\n\n\n## coming soon\n\n\n\n\nazp.plot_bf(idata_conc, var_names=[\"a\"], ref_val=0.5);\n\n\n\n\n\n\n\n\nWe can see that the BF_01 is \\(\\approx 1.6\\), which we can interpret as anecdotal evidence in favor of the null hypothesis (see the Jeffreys’ scale, discussed earlier).\n\n\n7.11.4 Bayes factors vs the alternatives\nWe could say that the Bayes factors measure which model, as a whole, is better at explaining the data. This includes the details of the prior, no matter how similar the model predictions are. In many scenarios, this is not what interests us when comparing models. For many real problems, priors are not intended to be an accurate description of the True prior distribution of parameters; instead, they are often chosen using partial information and with the goal of providing some regularization. In these cases, we prefer to evaluate models in terms of how similar their predictions are. For those cases, we can use LOO.\n\n\n\n\nAkaike, H. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2004. “No Unbiased Estimator of the Variance of k-Fold Cross-Validation.” Journal of Machine Learning Research 5: 1089–1105. https://jmlr.csail.mit.edu/papers/v5/grandvalet04a.html.\n\n\nHansen, Morris H., and William N. Hurwitz. 1943. “On the Theory of Sampling from Finite Populations.” The Annals of Mathematical Statistics 14 (4): 333–62. https://doi.org/10.1214/aoms/1177731360.\n\n\nMagnusson, Måns, Michael Riis Andersen, Johan Jonasson, and Aki Vehtari. 2020. “Leave-One-Out Cross-Validation for Model Comparison in Large Data.” In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics. Vol. 108. Proceedings of Machine Learning Research. PMLR. https://arxiv.org/abs/2001.00980.\n\n\nPaananen, Topi, Juho Piironen, Paul-Christian Bürkner, and Aki Vehtari. 2020. “Implicitly Adaptive Importance Sampling.” https://arxiv.org/abs/1906.08850.\n\n\nSoch, Joram, Thomas J Faulkenberry, Kenneth Petrykowski, and Carsten Allefeld. 2024. “The Book of Statistical Proofs.” https://doi.org/10.5281/ZENODO.4305949.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nWatanabe, Sumio. 2013. “A Widely Applicable Bayesian Information Criterion.” Journal of Machine Learning Research 14 (March): 867–97. https://dl.acm.org/doi/10.5555/2567709.2502609.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using Stacking to Average Bayesian Predictive Distributions (with Discussion).” Bayesian Analysis 13 (3): 917–1007. https://doi.org/10.1214/17-BA1091.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html",
    "href": "Chapters/Model_comparison_large_data.html",
    "title": "8  Model Comparison for Large Data",
    "section": "",
    "text": "8.1 Workflow for model comparison\nIn this chapter, we demonstrate how to efficiently compare Bayesian models on large datasets using subsampled PSIS-LOO-CV. We apply the methods introduced in Section 7.8 to a real dataset containing thousands of observations, showing how to balance computational efficiency with statistical accuracy. The workflow combines fast surrogate approximations with exact computations on a small subsample, and can be further accelerated using approximate posteriors from variational inference or Laplace approximations. For theoretical background and details of the method, we recommend you read Section 7.8 and references therein.\nThe workflow for applying subsampled PSIS-LOO-CV to model comparison is straightforward. We compute cheap surrogates for all observations, then refine our estimates using PSIS-LOO-CV on a small subsample, balancing computational efficiency with statistical accuracy:\nAgain, the key advantage is that the same subsample can be reused across all models, making the approach highly efficient for comparing multiple models. The difference \\(\\operatorname{elpd}_D = \\operatorname{elpd}_A - \\operatorname{elpd}_B\\) and its variance are estimated directly from the common subsample, properly accounting for the correlation between models’ predictions.\nThis reuse is what makes the difference estimator superior to the Hansen-Hurwitz approach for model comparison tasks. This is also the reason why we recommend using the same subsample for all models, rather than subsampling each model separately.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#workflow-for-model-comparison",
    "href": "Chapters/Model_comparison_large_data.html#workflow-for-model-comparison",
    "title": "8  Model Comparison for Large Data",
    "section": "",
    "text": "Compute posteriors \\(p_A(\\theta \\mid y)\\) and \\(p_B(\\theta \\mid y)\\) for models \\(A\\) and \\(B\\)\nCalculate \\(\\tilde\\pi_i\\) for all \\(n\\) observations using an appropriate approximation\nUse simple random sampling to select \\(m\\) observations (start with \\(m \\approx 100\\))\nCalculate \\(\\pi_j\\) for the subsample using PSIS-LOO-CV, checking Pareto-\\(k\\) diagnostics for unreliable importance sampling approximations\nEstimate \\(\\operatorname{elpd}_A\\), \\(\\operatorname{elpd}_B\\), and their difference\nIf subsampling SE is too large relative to the estimated difference, increase \\(m\\) using update_subsample",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#wells-data-and-logistic-model",
    "href": "Chapters/Model_comparison_large_data.html#wells-data-and-logistic-model",
    "title": "8  Model Comparison for Large Data",
    "section": "8.2 Wells data and logistic model",
    "text": "8.2 Wells data and logistic model\nTo demonstrate the PSIS-LOO-CV subsampling method, we use data from a household survey in a region of Bangladesh where drinking water was contaminated with arsenic. Households with unsafe arsenic levels in their wells were asked whether they would switch to using a neighbor’s safe well. The goal is to predict this binary switching decision using household characteristics.\nWe focus on a simple logistic regression with two key predictors: the arsenic concentration in the household’s well and the distance to the nearest safe well. The dataset contains \\(n = 3020\\) observations. While this is not massive by modern standards, it is large enough that efficient computational methods become valuable. The advantage of this sample size is that individual observations have limited influence on the overall model fit, which helps ensure stable importance sampling computations. The data is available from the R package loo.\nWe model the indicator of switching water wells using a logistic regression with intercept, arsenic concentration, and distance to the nearest safe well in hundreds of metres. The design matrix is \\(X = [1, \\text{dist}/100, \\text{arsenic}]\\) and the likelihood is given by\n\\[\n\\Pr(y_i = 1 \\mid \\beta) = \\operatorname{logit}^{-1}(X_i^{\\mathsf{T}} \\beta), \\qquad \\beta \\sim \\mathcal{N}(0, I_3).\n\\]\n\nimport pandas as pd\nimport numpy as np\n\nwells = pd.read_csv(\"../data/wells.csv\")\nwells[\"dist100\"] = wells[\"dist\"] / 100\n\nX = np.column_stack([\n    np.ones(len(wells)),        \n    wells[\"dist100\"].values,     \n    wells[\"arsenic\"].values \n])\ny = wells[\"switch\"].values\n\nWe can fit the model using PyMC or CmdStanPy where posterior draws are stored in a DataTree object.\n\nPyMCCmdStanPy\n\n\n\nimport pymc as pm\nimport xarray as xr\n\nwith pm.Model():\n    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=3)\n    logit_p = pm.math.dot(X, beta)\n    pm.Bernoulli(\"y\", logit_p=logit_p, observed=y)\n\n    idata = pm.sample(\n        draws=1000,\n        tune=1000,\n        chains=4,\n        random_seed=4711,\n        progressbar=True,\n        idata_kwargs={'log_likelihood': True}\n    )\n\ndata = azp.convert_to_datatree(idata)\ndata[\"constant_data\"] = xr.Dataset({\"X\": ([\"obs_id\", \"coef\"], X)})\n\n\n\n# CmdStanPy implementation will be added in future\n\n\n\n\n8.2.1 Constructing the log-likelihood helper\nUnlike regular loo(), the routine loo_subsample() expects a callable with signature log_lik_fn(observed, data) depending on the approximation method so that we can compute the log-likelihood for a subset of observations on demand rather than storing the log-likelihood for all observations, which can be computationally expensive for large datasets.\nThis callable signature is the baseline expectation for loo_subsample(), and we can always wrap auxiliary parameters with tools such as functools.partial so the routine still receives (observed, data).\nFor logistic regression, the per-draw log probability mass function is given by\n\\[\n\\log p(y_i \\mid \\beta_s, X_i) = y_i \\log \\pi_{is} + (1 - y_i) \\log (1 - \\pi_{is}), \\qquad \\pi_{is} = \\operatorname{logit}^{-1}(X_i^{\\mathsf{T}} \\beta_s).\n\\]\nA direct approach is to store the design matrix in the constant_data group so that the helper can directly compute the log-likelihood for whichever subset loo_subsample() hands to it.\n\nfrom scipy import stats\nfrom scipy.special import expit\n\ndef log_lik_fn(obs, data):\n    beta = data.posterior[\"beta\"].values\n    X = data.constant_data[\"X\"].values\n    logit_pred = X @ beta\n    prob = expit(logit_pred)\n    return stats.bernoulli.logpmf(obs, prob)\n\nThis log-likelihood helper evaluates the Bernoulli log-pmf for the selected subset and returns log probabilities for each posterior draw. No log-likelihood values are stored permanently, which keeps memory usage modest even when \\(N\\) is large.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#subsampled-psis-loo-cv-with-the-plpd-surrogate",
    "href": "Chapters/Model_comparison_large_data.html#subsampled-psis-loo-cv-with-the-plpd-surrogate",
    "title": "8  Model Comparison for Large Data",
    "section": "8.3 Subsampled PSIS-LOO-CV with the PLPD surrogate",
    "text": "8.3 Subsampled PSIS-LOO-CV with the PLPD surrogate\nAs we’ve discussed previously, a key component of the PSIS-LOO-CV subsampling method is the notion of a fast surrogate for the leave-one-out term \\(\\pi_i\\). One such surrogate is the point log predictive density (PLPD) which Magnusson et al. (2020) motivate by replacing the leave-one-out term \\(\\pi_i\\) with\n\\[\n\\tilde\\pi_i^{\\text{plpd}} = \\log p(y_i \\mid X_i, \\hat{\\beta}), \\quad \\hat{\\beta} = \\mathbb{E}[\\beta \\mid y].\n\\]\nThe approximation reduces the computational burden to a single evaluation per observation while retaining the shrinkage induced by the posterior mean. Inserting \\(\\tilde\\pi_i^{\\text{plpd}}\\) into the difference estimator converts the exact correction to a subsample average that only requires re-evaluating the full posterior draws on \\(m\\) observations.\nHere we use the method=\"plpd\" argument to loo_subsample() to use the PLPD surrogate.\n\nloo_plpd = azp.loo_subsample(\n    data=data,\n    observations=100,\n    var_name=\"y\",\n    method=\"plpd\",\n    log_lik_fn=log_lik_fn,\n    param_names=[\"beta\"],\n    pointwise=True,\n    seed=SEED,\n)\n\nloo_plpd\n\nComputed from 4000 by 100 subsampled log-likelihood\nvalues from 3020 total observations.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1968.4 15.6            0.3\np_loo          3.1\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      100  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nThe output inherits from the ELPDData interface with an additional column subsampling_se isolating the extra Monte Carlo error introduced by drawing only \\(100\\) of the \\(3020\\) observations. In our wells run with \\(m=100\\) we obtained \\(\\widehat{\\operatorname{elpd}}_{\\text{loo}}\\approx-1968.4\\) with \\(\\text{SE}\\approx15.6\\) and \\(\\text{subsampling SE} \\approx0.3\\). All Pareto‑\\(k\\) values on the subsample were \\(\\le 0.7\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#subsampled-psis-loo-cv-with-the-lpd-surrogate",
    "href": "Chapters/Model_comparison_large_data.html#subsampled-psis-loo-cv-with-the-lpd-surrogate",
    "title": "8  Model Comparison for Large Data",
    "section": "8.4 Subsampled PSIS-LOO-CV with the LPD surrogate",
    "text": "8.4 Subsampled PSIS-LOO-CV with the LPD surrogate\nWhen the posterior mass is diffuse or strongly correlated, evaluating the full posterior predictive density can improve the approximation. We can use the log predictive density (LPD) surrogate given by\n\\[\n\\tilde\\pi_i^{\\text{lpd}} = \\log \\left( \\frac{1}{S} \\sum_{s=1}^S p(y_i \\mid X_i, \\beta_s) \\right),\n\\]\nwhere \\(\\beta_s\\) are the draws stored in data.posterior. The additional sum over draws raises the cost and often reduces the approximation error. The same subsample size and random seed ensure that the estimator reuses the indices chosen for the PLPD computation, which makes comparisons straightforward.\n\nloo_lpd = azp.loo_subsample(\n    data=data,\n    observations=100,\n    var_name=\"y\",\n    method=\"lpd\",\n    pointwise=True,\n    seed=SEED,\n)\n\nloo_lpd\n\nComputed from 4000 by 100 subsampled log-likelihood\nvalues from 3020 total observations.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1968.5 15.6            0.4\np_loo          3.2\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      100  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nFrom the output, we can see that the estimated ELPD closely matches the PLPD result, both about \\(-1968.5\\) with \\(\\text{SE}\\approx15.6\\), and the subsampling SEs are very similar (PLPD \\(\\approx0.3\\) vs. LPD \\(\\approx0.4\\)).\nFor well‑identified models with tight posteriors, PLPD can match or even slightly outperform LPD. For more complex models, LPD often gains ground. Taken together, the parallel outputs validate the theoretical trade-off. PLPD minimises computation, whereas LPD expends extra work to tighten the estimator. For larger \\(m\\) the difference between the two surrogates typically diminishes, and the choice becomes a balance between runtime and stability.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#incremental-sampling-updates",
    "href": "Chapters/Model_comparison_large_data.html#incremental-sampling-updates",
    "title": "8  Model Comparison for Large Data",
    "section": "8.5 Incremental sampling updates",
    "text": "8.5 Incremental sampling updates\nMagnusson et al. (2020) recommend increasing \\(m\\) until the subsampling uncertainty is negligible for the task at hand. The update_subsample() routine efficiently extends an existing subsample without discarding work already done. Here we add 200 additional observations (from 100 to 300 total in the subsample) by using the previous subsample as a starting point.\n\nloo_update = azp.update_subsample(\n    loo_plpd,\n    data,\n    observations=200,\n    var_name=\"y\",\n    method=\"plpd\",\n    log_lik_fn=log_lik_fn,\n    param_names=[\"beta\"],\n    seed=SEED,\n)\n\nloo_update\n\nComputed from 4000 by 300 subsampled log-likelihood\nvalues from 3020 total observations.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1968.4 15.6            0.2\np_loo          3.1\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      300  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nThe updated result shows how the estimator stabilises as more observations are included. In our run, the subsampling SE decreased from about \\(0.3\\) to \\(0.2\\), while the estimated ELPD and \\(p_{\\text{loo}}\\) remained essentially unchanged.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#approximate-loo-cv-using-psis-loo-with-posterior-approximations",
    "href": "Chapters/Model_comparison_large_data.html#approximate-loo-cv-using-psis-loo-with-posterior-approximations",
    "title": "8  Model Comparison for Large Data",
    "section": "8.6 Approximate LOO-CV using PSIS-LOO with posterior approximations",
    "text": "8.6 Approximate LOO-CV using PSIS-LOO with posterior approximations\nUsing posterior approximations such as variational inference or Laplace approximations can further accelerate LOO-CV for large data. These methods avoid the computational cost of full MCMC while maintaining reasonable approximation quality for well-behaved models.\nWe will use the Laplace approximation which fits a multivariate normal distribution centered at the posterior mode \\(\\hat{\\theta}\\) with covariance equal to the inverse Hessian of the negative log posterior. For a model with likelihood \\(p(y \\mid \\theta)\\) and prior \\(p(\\theta)\\), the Laplace approximation is\n\\[\nq(\\theta \\mid y) = \\mathcal{N}(\\theta \\mid \\hat{\\theta}, \\Sigma),\n\\]\nwhere\n\\[\n\\hat{\\theta} = \\arg\\max_\\theta \\log p(\\theta \\mid y) \\quad \\text{ and } \\quad \\Sigma = [-\\nabla^2 \\log p(\\theta \\mid y)]^{-1}.\n\\]\nDraws from \\(q(\\theta \\mid y)\\) can be obtained efficiently by sampling from the fitted normal distribution.\nThe following fits the wells model using a Laplace approximation via pymc_extras.fit_laplace, which returns draws from the approximate posterior along with the mode and covariance estimates.\n\nPyMCCmdStanPy\n\n\n\nimport pymc as pm\nfrom pymc_extras import fit_laplace\n\nwith pm.Model() as model_laplace:\n    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=3)\n    logit_p = pm.math.dot(X, beta)\n    pm.Bernoulli(\"y\", logit_p=logit_p, observed=y)\n\n    idata_laplace = fit_laplace(\n        chains=4,\n        draws=2000,\n        random_seed=SEED,\n        progressbar=True\n    )\n\n\n\n# CmdStanPy implementation will be added in future\n\n\n\nThe fit_laplace function stores the posterior mode in the fit group as mean_vector and the covariance in covariance_matrix. Posterior draws are stored in the posterior group as usual. We convert the result to a DataTree and attach the observed data and design matrix for use with loo_approximate_posterior().\n\ndata_laplace = azp.convert_to_datatree(idata_laplace)\n\ndata_laplace[\"observed_data\"] = xr.Dataset({\n    \"y\": ([\"obs_id\"], y, {\"obs_id\": range(len(y))})\n})\ndata_laplace[\"constant_data\"] = xr.Dataset({\"X\": ([\"obs_id\", \"coef\"], X)})\n\n\n8.6.1 Computing log density corrections\nTo use loo_approximate_posterior(), we must supply log_p and log_q, which represent the log density of the true posterior and the approximate posterior, respectively. The importance ratios are then adjusted to account for the approximation quality with\n\\[\nr(\\theta_s) \\propto \\frac{1}{p(y_i \\mid \\theta_s)} \\frac{p(\\theta_s \\mid y)}{q(\\theta_s \\mid y)}.\n\\]\nFor the Laplace approximation, \\(\\log q(\\theta_s \\mid y)\\) is the log density of the fitted normal distribution, while \\(\\log p(\\theta_s \\mid y)\\) is the log prior plus the log likelihood. We require both of these quantities to be able to compute the importance ratios.\n\nfrom scipy.stats import multivariate_normal\n\nbeta_samples = data_laplace.posterior[\"beta\"]\nX_xr = data_laplace.constant_data[\"X\"]\ny_xr = data_laplace.observed_data[\"y\"]\n\nparam_dim = [d for d in beta_samples.dims if d not in [\"chain\", \"draw\"]][0]\nbeta_renamed = beta_samples.rename({param_dim: \"coef\"})\n\nlogit_pred = xr.dot(X_xr, beta_renamed, dims=[\"coef\"])\nprob = 1 / (1 + xr.ufuncs.exp(-logit_pred))\nlog_lik_laplace = y_xr * xr.ufuncs.log(prob) + (1 - y_xr) * xr.ufuncs.log(1 - prob)\n\ndata_laplace[\"log_likelihood\"] = xr.Dataset({\"y\": log_lik_laplace})\n\nmean_vals = data_laplace.fit[\"mean_vector\"].values\ncov_matrix = data_laplace.fit[\"covariance_matrix\"].values\n\n# This applies the multivariate normal log density function to each draw along the last axis\n# which is the parameter dimension\nlog_q_vals = np.apply_along_axis(\n    lambda b: multivariate_normal.logpdf(b, mean=mean_vals, cov=cov_matrix),\n    axis=-1,\n    arr=beta_renamed.values\n)\nlog_q_da = xr.DataArray(\n    log_q_vals,\n    dims=[\"chain\", \"draw\"],\n    coords={\"chain\": beta_renamed.chain, \"draw\": beta_renamed.draw}\n)\n\nlog_prior = -0.5 * (beta_renamed ** 2).sum(\"coef\") - 1.5 * np.log(2 * np.pi)\nlog_lik_sum = log_lik_laplace.sum(\"obs_id\")\nlog_p_da = log_prior + log_lik_sum\n\nThe log prior uses the standard normal density \\(\\mathcal{N}(0, I_3)\\) specified in the model. The log likelihood sums over all observations to obtain the joint log density. These quantities are then passed to loo_approximate_posterior() to compute corrected LOO estimates.\n\n\n8.6.2 Running LOO-CV with approximate posterior\nWith log_p and log_q in hand, we can compute LOO-CV using the approximate posterior. The loo_approximate_posterior() function adjusts the importance ratios to account for the discrepancy between the true posterior and the Laplace approximation.\n\nloo_ap = azp.loo_approximate_posterior(\n    data=data_laplace,\n    log_p=log_p_da,\n    log_q=log_q_da,\n    var_name=\"y\",\n    pointwise=True\n)\n\nloo_ap\n\nComputed from 8000 posterior samples and 3020 observations log-likelihood matrix.\nPosterior approximation correction used.\n\n         Estimate       SE\nelpd_loo -1968.42    15.59\np_loo        3.17        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)     3020  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nThe output shows how the approximate posterior performs relative to MCMC. The Pareto-\\(k\\) diagnostics remain available and indicate whether the importance sampling correction is reliable. You can also see in the output the line “Posterior approximation correction used”, which indicates that the approximate posterior correction was used.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#combining-approximate-posterior-with-subsampling",
    "href": "Chapters/Model_comparison_large_data.html#combining-approximate-posterior-with-subsampling",
    "title": "8  Model Comparison for Large Data",
    "section": "8.7 Combining approximate posterior with subsampling",
    "text": "8.7 Combining approximate posterior with subsampling\nThe approximate posterior correction can be combined with subsampling to further reduce computational cost.\nWhen using loo_subsample() with an approximate posterior, we simply pass log_p and log_q along with the subsample specification. The difference estimator then corrects for both the approximation error and the subsampling uncertainty.\n\nloo_ap_ss = azp.loo_subsample(\n    data=data_laplace,\n    observations=100,\n    var_name=\"y\",\n    method=\"plpd\",\n    log_lik_fn=log_lik_fn,\n    param_names=[\"beta\"],\n    log_p=log_p_da,\n    log_q=log_q_da,\n    pointwise=True,\n    seed=SEED\n)\n\nloo_ap_ss\n\nComputed from 8000 by 100 subsampled log-likelihood\nvalues from 3020 total observations. Posterior approximation correction used.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1968.6 15.6            0.4\np_loo          3.3\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      100  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nThe combined approach leverages the strengths of both methods. The Laplace approximation avoids expensive MCMC sampling, while subsampling reduces the cost of evaluating the full dataset. The correction terms ensure the estimates remain unbiased despite the double approximation.\nIn our wells example, the subsampled approximate posterior LOO produces estimates very close to both the full MCMC and the full approximate posterior results, with negligible subsampling SE. This demonstrates that for well-behaved models, aggressive approximation can yield accurate predictive performance estimates at minimal computational cost.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#comparing-models-with-subsampled-loo",
    "href": "Chapters/Model_comparison_large_data.html#comparing-models-with-subsampled-loo",
    "title": "8  Model Comparison for Large Data",
    "section": "8.8 Comparing models with subsampled LOO",
    "text": "8.8 Comparing models with subsampled LOO\nWe can compare models that are estimated via subsampling just like we would for regular LOO using the compare() function. We will fit a second model using log(arsenic) instead of arsenic as a predictor and compare the two models.\n\nPyMCCmdStanPy\n\n\n\nX_log = X.copy()\nX_log[:, 2] = np.log(X[:, 2])\n\nwith pm.Model():\n    beta2 = pm.Normal(\"beta\", mu=0, sigma=1, shape=3)\n    logit_p2 = pm.math.dot(X_log, beta2)\n    pm.Bernoulli(\"y\", logit_p=logit_p2, observed=y)\n\n    idata2 = pm.sample(\n        draws=1000,\n        tune=1000,\n        chains=4,\n        random_seed=SEED,\n        progressbar=True,\n        idata_kwargs={'log_likelihood': True}\n    )\n\ndata2 = azp.convert_to_datatree(idata2)\ndata2[\"constant_data\"] = xr.Dataset({\"X\": ([\"obs_id\", \"coef\"], X_log)})\n\n\n\n# CmdStanPy implementation will be added in future\n\n\n\n\n8.8.1 Comparison with different subsamples\nWe first compute subsampled LOO for the second model using a different random seed than the first model. This means the two models will be evaluated on different subsets of observations.\n\nloo_ss_2_diff = azp.loo_subsample(\n    data=data2,\n    observations=100,\n    var_name=\"y\",\n    method=\"plpd\",\n    log_lik_fn=log_lik_fn,\n    param_names=[\"beta\"],\n    pointwise=True,\n    seed=315\n)\n\nloo_ss_2_diff\n\nComputed from 4000 by 100 subsampled log-likelihood\nvalues from 3020 total observations.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1952.3 16.2            0.3\np_loo          3.1\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      100  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nNow we compare the models using different subsamples.\n\ncomparison_diff = azp.compare({\n    \"model1_arsenic\": loo_plpd,\n    \"model2_log_arsenic\": loo_ss_2_diff,\n}).round(1)\n\ncomparison_diff\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/compare.py:292: UserWarning: Different subsamples used in 'model2_log_arsenic' and 'model1_arsenic'. Naive diff SE is used.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\nsubsampling_dse\n\n\n\n\nmodel2_log_arsenic\n0\n-1952.3\n3.1\n0.0\n0.5\n16.2\n0.0\nFalse\n0.0\n\n\nmodel1_arsenic\n1\n-1968.4\n3.1\n16.1\n0.5\n15.6\n22.5\nFalse\n0.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNote on dse and subsampling_dse\n\n\n\nThe output is very similar to regular compare() output with an additional column for subsampling uncertainty, subsampling_dse. Note that dse already accounts for the subsampling uncertainty, so we cannot add subsampling_dse to it. We report subsampling_dse to show the exact amount of subsampling uncertainty.\n\n\nWhen using different subsamples, notice we get a warning message indicating that the subsamples are different. This warning indicates that the comparison cannot account for the correlation between models’ predictions. In this case, the standard error of the difference dse is computed naively by treating the two models as independent, which substantially inflates the uncertainty because we are losing the benefit of the covariance term \\(\\operatorname{Cov}(\\operatorname{elpd}_A, \\operatorname{elpd}_B)\\) discussed earlier.\n\n\n8.8.2 Comparison with the same subsample\nTo properly compare models, we reuse the exact same observations by passing the first LOO subsample object to the second loo_subsample() call. This ensures both models are evaluated on the same subset of observations, which is critical for accurate model comparison.\n\nloo_ss_2 = azp.loo_subsample(\n    data=data2,\n    observations=loo_plpd.loo_subsample_observations,\n    var_name=\"y\",\n    method=\"plpd\",\n    log_lik_fn=log_lik_fn,\n    param_names=[\"beta\"],\n    pointwise=True,\n)\n\nloo_ss_2\n\nComputed from 4000 by 100 subsampled log-likelihood\nvalues from 3020 total observations.\n\n         Estimate   SE subsampling SE\nelpd_loo   -1952.2 16.2            0.2\np_loo          3.0\n\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      100  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nNow we compare the models using the same subsample.\n\ncomparison_same = azp.compare({\n    \"model1_arsenic\": loo_plpd,\n    \"model2_log_arsenic\": loo_ss_2,\n}).round(1)\n\ncomparison_same\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\nsubsampling_dse\n\n\n\n\nmodel2_log_arsenic\n0\n-1952.2\n3.0\n0.0\n0.5\n16.2\n0.0\nFalse\n0.0\n\n\nmodel1_arsenic\n1\n-1968.4\n3.1\n16.2\n0.5\n15.6\n4.4\nFalse\n0.1\n\n\n\n\n\n\n\nWhen using the same subsample, we can see that the standard error of the difference is dramatically reduced compared to the previous comparison. This is because the correlation between models’ predictions on the common subsample is properly accounted for in the variance calculation. The SE reduction is substantial, making it much easier to detect meaningful performance differences.\nIn this example, the second model (with log-transformed arsenic) has higher ELPD than the first model, indicating better predictive performance. The subsampling SE is very small relative to the difference in ELPD, indicating that the subsample size of 100 is sufficient for accurate comparison. If the subsampling SE were large, we could increase the subsample size using update_subsample() as demonstrated earlier.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#comparing-full-loo-to-subsampled-loo",
    "href": "Chapters/Model_comparison_large_data.html#comparing-full-loo-to-subsampled-loo",
    "title": "8  Model Comparison for Large Data",
    "section": "8.9 Comparing full LOO to subsampled LOO",
    "text": "8.9 Comparing full LOO to subsampled LOO\nWe can also compare a subsampled LOO object to a full LOO object that was computed on all \\(N = 3020\\) observations.\nThe following computes full PSIS-LOO-CV for the first model using the loo() function.\n\nloo_full = azp.loo(data, var_name=\"y\", pointwise=True)\nloo_full\n\nComputed from 4000 posterior samples and 3020 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1968.47    15.60\np_loo        3.24        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)     3020  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nWhen comparing a full LOO calculation to a subsampled calculation, the compare() function automatically uses only the observations included in both calculations.\n\ncomparison_mixed = azp.compare({\n    \"model1_full_loo\": loo_full,\n    \"model2_subsampled_loo\": loo_ss_2\n}).round(1)\n\ncomparison_mixed\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/compare.py:338: UserWarning: Estimated elpd_diff using observations included in loo calculations for all models.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\nsubsampling_dse\n\n\n\n\nmodel2_subsampled_loo\n0\n-1952.2\n3.0\n0.0\n0.5\n16.2\n0.0\nFalse\n0.0\n\n\nmodel1_full_loo\n1\n-1968.5\n3.2\n16.3\n0.5\n15.6\n4.4\nFalse\n0.2\n\n\n\n\n\n\n\nNotice that this comparison triggers a warning message indicating that only observations included in both LOO calculations are used for the difference estimate. Since the subsampled LOO only evaluated 100 observations, the comparison is based on those 100 observations rather than the full dataset. We can also see an increase in subsampling_dse compared to the previous comparison which is due to a technical detail that we omit from this example.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison_large_data.html#practical-considerations",
    "href": "Chapters/Model_comparison_large_data.html#practical-considerations",
    "title": "8  Model Comparison for Large Data",
    "section": "8.10 Practical considerations",
    "text": "8.10 Practical considerations\nFor moderately sized models, the PLPD surrogate typically suffices because the posterior mean encapsulates most of the predictive information. When the model has hierarchical components or weakly identified parameters, Magnusson et al. (2020) recommend richer surrogates such as the LPD or diagnostics-driven truncation of importance weights to reduce the approximation error \\(e_j\\) and therefore the subsampling variance. Truncated importance sampling plays a similar role by clipping large \\(r_{is}\\) values before applying the Pareto smoothing step. Regardless of the surrogate, increasing \\(m\\) under the difference estimator guarantees convergence because the finite population correction shrinks the variance as \\(m\\) approaches \\(N\\).\nWhen comparing multiple models via subsampling, it is recommended to reuse a single subsample across all fits. In this case, the difference estimator produces \\(\\widehat{\\operatorname{ELPD}}_{\\text{diff}}\\) for each model and the estimated difference \\(\\widehat{\\operatorname{ELPD}}_{A} - \\widehat{\\operatorname{ELPD}}_{B}\\) inherits the same subsampling indices. Consistency results in Magnusson et al. (2020) ensure that even with fixed \\(m\\) and a finite number of posterior draws, the estimator converges as the quality of the surrogate improves.\n\n\n\n\nMagnusson, Måns, Michael Riis Andersen, Johan Jonasson, and Aki Vehtari. 2020. “Leave-One-Out Cross-Validation for Model Comparison in Large Data.” In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics. Vol. 108. Proceedings of Machine Learning Research. PMLR. https://arxiv.org/abs/2001.00980.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Comparison for Large Data</span>"
    ]
  },
  {
    "objectID": "Chapters/Moment_Matching.html",
    "href": "Chapters/Moment_Matching.html",
    "title": "9  Moment matching for improved PSIS-LOO-CV",
    "section": "",
    "text": "9.1 Roaches data and Poisson regression model\nIn this chapter, we demonstrate how to apply importance weighted moment matching (IWMM) to improve PSIS-LOO-CV estimates when influential observations produce high Pareto-\\(k\\) values. We walk through a complete applied example using the roaches dataset from Gelman and Hill (2007), showing how to specify the required functions and apply moment matching to resolve problematic importance sampling approximations without expensive model refitting. The workflow demonstrates ArviZ’s loo_moment_match() function with a deliberately misspecified Poisson regression model where multiple observations are flagged as problematic. For theoretical background and details of how moment matching works, we recommend you read Section 7.7 and references therein.\nLet’s now walk through a concrete example using the roaches dataset to see this algorithm in action. The roaches dataset from Gelman and Hill (2007) examines the efficacy of a pest management system at reducing cockroach infestations in urban apartments. The study followed 264 apartments over several months, recording the number of roaches caught during follow-up (y), pre-treatment roach counts (roach1, square root transformed), treatment status (treatment), whether the building is restricted to elderly residents (senior), and trap exposure time in days (exposure2).\nWe intentionally use a Poisson model rather than negative binomial regression to demonstrate how moment matching handles misspecified models. The exposure time varies across apartments, so we include log(exposure2) as an offset term.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moment matching for improved PSIS-LOO-CV</span>"
    ]
  },
  {
    "objectID": "Chapters/Moment_Matching.html#roaches-data-and-poisson-regression-model",
    "href": "Chapters/Moment_Matching.html#roaches-data-and-poisson-regression-model",
    "title": "9  Moment matching for improved PSIS-LOO-CV",
    "section": "",
    "text": "9.1.1 Loading and preparing the data\nWe start by loading the roaches dataset and examining its basic structure.\n\nimport pandas as pd\n\nroaches = pd.read_csv(\"../data/roaches.csv\", index_col=0)\nroaches['log_exposure2'] = np.log(roaches['exposure2'])\n\nroaches.describe()\n\n\n\n\n\n\n\n\ny\nroach1\ntreatment\nsenior\nexposure2\nlog_exposure2\n\n\n\n\ncount\n262.000000\n262.000000\n262.000000\n262.000000\n262.000000\n262.000000\n\n\nmean\n25.648855\n4.419861\n0.603053\n0.305344\n1.021047\n-0.012538\n\n\nstd\n50.846539\n4.769184\n0.490201\n0.461434\n0.320757\n0.249755\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.200000\n-1.609438\n\n\n25%\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n50%\n3.000000\n2.645751\n1.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n24.000000\n7.106071\n1.000000\n1.000000\n1.000000\n0.000000\n\n\nmax\n357.000000\n21.213203\n1.000000\n1.000000\n4.285714\n1.455287\n\n\n\n\n\n\n\nThe dataset exhibits substantial overdispersion with mean 25.6 and standard deviation 50.8. Approximately 36% of apartments had zero roaches while the maximum reached 357. This combination of high variability, many zeros, and extreme counts makes certain observations potentially influential, which is precisely where moment matching becomes valuable for improving LOO-CV estimates.\n\n\n9.1.2 Model specification and fitting\nWe fit a Poisson regression model using Bambi, which provides a high-level interface for Bayesian modeling. The model includes the pre-treatment roach count, treatment indicator, and senior building indicator as predictors, with log exposure as an offset.\n\nBambiCmdStanPy\n\n\n\nimport bambi as bmb\n\nmodel = bmb.Model(\n    'y ~ roach1 + treatment + senior + offset(log_exposure2)',\n    data=roaches,\n    family='poisson',\n    priors={\n        'roach1': bmb.Prior('Normal', mu=0, sigma=2.5),\n        'treatment': bmb.Prior('Normal', mu=0, sigma=2.5),\n        'senior': bmb.Prior('Normal', mu=0, sigma=2.5),\n        'Intercept': bmb.Prior('Normal', mu=0, sigma=5.0)\n    }\n)\n\nidata = model.fit(\n    draws=1000,\n    tune=1000,\n    chains=4,\n    random_seed=SEED,\n    idata_kwargs={'log_likelihood': True}\n)\n\n\n\n# CmdStanPy implementation will be added in future\n\n\n\n\nazp.summary(idata.posterior.ds, var_names=['roach1', 'treatment', 'senior', 'Intercept'])\n\n\n\n\n\n\n\n\nmean\nsd\neti89_lb\neti89_ub\ness_bulk\ness_tail\nr_hat\nmcse_mean\nmcse_sd\n\n\n\n\nroach1\n0.16\n0.00\n0.16\n0.16\n3269.45\n3057.47\n1.0\n0.0\n0.0\n\n\ntreatment\n-0.57\n0.03\n-0.61\n-0.53\n3412.93\n2837.12\n1.0\n0.0\n0.0\n\n\nsenior\n-0.32\n0.03\n-0.37\n-0.26\n3535.11\n2733.84\n1.0\n0.0\n0.0\n\n\nIntercept\n2.53\n0.03\n2.49\n2.57\n3294.58\n2853.49\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n9.1.3 Initial PSIS-LOO-CV evaluation\nWith the posterior draws in hand, we compute the PSIS-LOO-CV estimate of the model’s predictive performance.\n\nloo_result = azp.loo(idata, pointwise=True, var_name=\"y\")\nloo_result\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 4000 posterior samples and 262 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -5461.14   693.85\np_loo      258.87        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      249   95.0%\n   (0.70, 1]   (bad)         6    2.3%\n    (1, Inf)   (very bad)    7    2.7%\n\n\nThe output shows that 13 observations have Pareto-\\(k\\) values exceeding 0.7, indicating that the importance sampling approximation is unreliable for these cases. For these problematic observations, the most accurate (but computationally expensive) approach would be to refit the model, leaving out each observation in turn. However, this quickly becomes impractical as the number of flagged observations grows. Moment matching offers an efficient solution by improving the reliability of the PSIS-LOO-CV estimates for exactly those cases where brute-force refitting would otherwise be required, reducing computational burden without sacrificing much accuracy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moment matching for improved PSIS-LOO-CV</span>"
    ]
  },
  {
    "objectID": "Chapters/Moment_Matching.html#applying-moment-matching",
    "href": "Chapters/Moment_Matching.html#applying-moment-matching",
    "title": "9  Moment matching for improved PSIS-LOO-CV",
    "section": "9.2 Applying moment matching",
    "text": "9.2 Applying moment matching\nNow we apply importance weighted moment matching to improve the PSIS-LOO-CV estimates for the problematic observations. ArviZ implements this through the loo_moment_match() function, which requires us to provide the unconstrained parameter draws and functions for evaluating the model’s log posterior and log likelihood.\n\n9.2.1 Function specification and implementation\nTo apply moment matching, we need to provide functions that compute the log posterior \\(\\log p(\\theta \\mid y)\\) and the log likelihood for a single observation \\(\\log p(y_i \\mid \\theta)\\) in the unconstrained parameter space, where all parameters are real-valued without constraints.\nOur Poisson regression model is\n\\[\ny_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = \\exp(\\eta_i),\n\\]\nwhere the linear predictor is\n\\[\n\\eta_i = \\alpha + \\sum_{j=1}^3 \\beta_j x_{ij} + \\log(\\text{exposure2}_i),\n\\]\nwith \\(x_i = (\\text{roach1}_i, \\text{treatment}_i, \\text{senior}_i)\\). We use independent normal priors \\(\\beta_j \\sim N(0, 2.5^2)\\) and \\(\\alpha \\sim N(0, 5^2)\\).\nThe log posterior combines the log likelihood with the log prior\n\\[\n\\log p(\\theta \\mid y) = \\sum_{i=1}^n \\log p(y_i \\mid \\theta) + \\log p(\\theta),\n\\]\nwhere the log likelihood for observation \\(i\\) is\n\\[\n\\log p(y_i \\mid \\theta) = y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!).\n\\]\nWe start by converting our data into DataArrays with properly labeled dimensions to use xarray’s dimension-aware operations for the computations.\n\nimport xarray as xr\nfrom scipy.special import gammaln\nfrom functools import partial\n\nX = roaches[['roach1', 'treatment', 'senior']].values\ny = roaches['y'].values\nlog_offset = roaches['log_exposure2'].values\n\nn_obs = len(roaches)\ncoef_names = ['roach1', 'treatment', 'senior']\n\ndesign_matrix = xr.DataArray(\n    X,\n    dims=['obs_id', 'coef'],\n    coords={'obs_id': range(n_obs), 'coef': coef_names}\n)\ny_da = xr.DataArray(y, dims=['obs_id'], coords={'obs_id': range(n_obs)})\noffset_da = xr.DataArray(log_offset, dims=['obs_id'], coords={'obs_id': range(n_obs)})\nfactorial_term = xr.DataArray(gammaln(y + 1), dims=['obs_id'], coords={'obs_id': range(n_obs)})\n\nbeta_prior_scale = 2.5\nalpha_prior_scale = 5.0\n\nFirst, we construct an array of unconstrained parameters from the posterior draws. All of the parameters are unconstrained in this model, so we can simply concatenate the posterior draws along the uparam dimension.\n\nposterior = idata.posterior\nupars = xr.concat([\n    posterior.ds['roach1'].expand_dims({'uparam': ['roach1']}),\n    posterior.ds['treatment'].expand_dims({'uparam': ['treatment']}),\n    posterior.ds['senior'].expand_dims({'uparam': ['senior']}),\n    posterior.ds['Intercept'].expand_dims({'uparam': ['Intercept']})\n], dim='uparam').transpose('chain', 'draw', 'uparam')\n\nWe can now define the log posterior function and the leave-one-out log likelihood function. The log posterior function computes the log posterior probability for a given set of unconstrained parameters, and the log likelihood function computes the log likelihood for a given set of unconstrained parameters and a given observation.\n\ndef log_prob_upars_fn(upars, design_matrix, y_da, offset_da, factorial_term,\n                       coef_names, beta_prior_scale, alpha_prior_scale):\n    \"\"\"Compute log posterior for unconstrained parameters.\"\"\"\n    beta = upars.sel(uparam=coef_names).rename({'uparam': 'coef'})\n    intercept = upars.sel(uparam='Intercept')\n\n    lin = xr.dot(design_matrix, beta, dims='coef') + intercept + offset_da\n    exp_lin = xr.ufuncs.exp(lin)\n    log_lik = y_da * lin - exp_lin - factorial_term\n\n    log_prior_beta = (-0.5 * (beta / beta_prior_scale) ** 2).sum('coef')\n    log_prior_intercept = -0.5 * (intercept / alpha_prior_scale) ** 2\n    return log_lik.sum('obs_id') + log_prior_beta + log_prior_intercept\n\n\ndef log_lik_i_upars_fn(upars, i, design_matrix, y_da, offset_da, factorial_term, coef_names):\n    \"\"\"Compute log likelihood for observation i.\"\"\"\n    beta = upars.sel(uparam=coef_names).rename({'uparam': 'coef'})\n    intercept = upars.sel(uparam='Intercept')\n\n    features_i = design_matrix.isel(obs_id=i)\n    lin_i = (beta * features_i).sum('coef') + intercept + offset_da.isel(obs_id=i)\n\n    exp_lin_i = xr.ufuncs.exp(lin_i)\n    log_lik_i = y_da.isel(obs_id=i) * lin_i - exp_lin_i - factorial_term.isel(obs_id=i)\n    return log_lik_i\n\nNow we bind the data dependencies to these functions using functools.partial. This creates new functions where all the data parameters are fixed, leaving only the unconstrained parameters (and observation index for the likelihood function) as free parameters.\n\nlog_prob_upars = partial(\n    log_prob_upars_fn,\n    design_matrix=design_matrix,\n    y_da=y_da,\n    offset_da=offset_da,\n    factorial_term=factorial_term,\n    coef_names=coef_names,\n    beta_prior_scale=beta_prior_scale,\n    alpha_prior_scale=alpha_prior_scale\n)\n\nlog_lik_i_upars = partial(\n    log_lik_i_upars_fn,\n    design_matrix=design_matrix,\n    y_da=y_da,\n    offset_da=offset_da,\n    factorial_term=factorial_term,\n    coef_names=coef_names\n)\n\n\n\n9.2.2 Computational considerations\nWhile moment matching requires additional density evaluations compared to standard PSIS-LOO-CV, the computational cost remains modest. For each transformed draw \\(\\theta^{*(s)}\\), we must evaluate both the full-data posterior density \\(p(\\theta^{*(s)} \\mid y)\\) and the likelihood \\(p(y_i \\mid \\theta^{*(s)})\\), rather than just the likelihood as in standard PSIS. Even with multiple transformation iterations, this cost is substantially smaller than refitting the model for each problematic observation.\nThe key trade-off is between accuracy and computational efficiency. When several observations have high Pareto-\\(k\\) values, moment matching provides reliable estimates at a fraction of the cost of exact PSIS-LOO-CV, making it practical for routine model assessment even with computationally expensive models. In our roaches example with 13 problematic observations, moment matching requires a few additional seconds of computation compared to minutes or hours that would be needed to refit the model 13 times.\n\n\n9.2.3 Running moment matching\nWith the required functions defined, we can now run PSIS-LOO-CV with importance weighted moment matching. We will specify that we want to match the covariance structure of the posterior draws given the complicated structure of the data.\nKeep in mind that the split argument, which specifies whether to use the split proposal density, is a boolean that defaults to True. This is highly recommended in most applied cases. When split=True, the split proposal density is\n\\[\ng_{\\text{split,loo}}(\\theta) \\propto p(\\theta \\mid y) + |\\mathbf{J}_{T_w}|^{-1}\\, p(T_w^{-1}(\\theta) \\mid y),\n\\]\nso each fold mixes the full-data posterior with the transformed draws from the denominator adaptation.\n\nloo_mm_result = azp.loo_moment_match(\n    idata,\n    loo_result,\n    log_prob_upars_fn=log_prob_upars,\n    log_lik_i_upars_fn=log_lik_i_upars,\n    upars=upars,\n    var_name=\"y\",\n    cov=True,\n)\n\nloo_mm_result\n\nComputed from 4000 posterior samples and 262 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -5477.60   699.88\np_loo      275.34        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      262  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\nAfter moment matching, the Pareto-\\(k\\) values improve substantially, with all 262 observations now having Pareto-\\(k\\) values below 0.7. This indicates that moment matching has successfully resolved all problematic importance sampling issues, providing reliable PSIS-LOO-CV estimates for every observation. This represents a complete improvement over the original 13 problematic observations. Notably, the ELPD estimate decreases from -5461.14 to -5477.60 after moment matching, indicating that the original PSIS-LOO-CV estimate was too optimistic and loo() overestimated the predictive performance.\n\n\n9.2.4 Interpreting diagnostic quantities\nAfter moment matching, two distinct Pareto-\\(k\\) diagnostics are available. The values in pareto_k reflect the accuracy of the importance sampling approximation after applying the moment matching transformations. These are the values we use to assess whether our PSIS-LOO-CV estimates are reliable. The original values are preserved in influence_pareto_k and serve a different purpose: they indicate how much each observation influences the posterior distribution. An observation with high influence_pareto_k substantially affects the model fit when included, regardless of whether moment matching successfully improved the sampling accuracy.\nMoment matching also provides per-observation effective sample size estimates in n_eff_i. These values quantify how many independent samples effectively contribute to each observation’s PSIS-LOO-CV estimate after accounting for the importance weights and MCMC sampling efficiency. Lower effective sample sizes indicate observations where the importance weights are more variable, suggesting greater uncertainty in those particular estimates.\nThese diagnostic quantities can be accessed directly from the loo_mm_result object as attributes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moment matching for improved PSIS-LOO-CV</span>"
    ]
  },
  {
    "objectID": "Chapters/Moment_Matching.html#summary",
    "href": "Chapters/Moment_Matching.html#summary",
    "title": "9  Moment matching for improved PSIS-LOO-CV",
    "section": "9.3 Summary",
    "text": "9.3 Summary\nImportance weighted moment matching provides an efficient solution for improving PSIS-LOO-CV when influential observations lead to unreliable importance sampling approximations. Rather than refitting models, IWMM transforms existing posterior draws to match importance-weighted moments in the unconstrained parameter space. The approach is fully automated, requires no user tuning, and works with arbitrary posterior samples from probabilistic programming frameworks where density evaluation is possible.\nThe method has important limitations worth keeping in mind:\n\nIt targets only first and second moments, so improvements depend on whether these moments adequately capture differences between proposal and target distributions\nWhen importance weights have large variance, the computation of weighted moments can become unreliable (mitigated through weight regularization or larger sample sizes)\nThe algorithm may fail to find sufficiently helpful transformations when target and proposal distributions differ substantially in tail behavior, correlation structure, or number of modes\nFor extremely high-dimensional problems, the most sophisticated transformation (matching full covariance) may become numerically unstable\nThe split proposal approximation introduces some inefficiency by placing unnecessary probability mass in regions where the integrand is near its expectation, though this trade-off is least problematic precisely when adaptive methods are most needed\n\nDespite these limitations, moment matching succeeds in most practical applications when the original posterior simulation was reasonably successful, providing both computational efficiency and improved reliability for model assessment without requiring complex tuning or auxiliary assumptions.\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Moment matching for improved PSIS-LOO-CV</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html",
    "href": "Chapters/Case_study_model_comparison.html",
    "title": "10  Model Comparison (case study)",
    "section": "",
    "text": "10.1 Information criteria for hierarchical and multi-likelihood models\nThere are many situations where one model can be used for several prediction tasks at the same time. Hierarchical models or models with multiple observations are examples of such cases. With two observations for example, the same model can be used to predict only the first observation, only the second or both observations at the same time.\nBefore estimating the predictive accuracy, there are two important questions to answer: what is the predictive task we are interested in and, whether or not the exchangeability criteria is met. This section will show several alternative ways to define the predictive task using the same model.\nWe are going to analyze data from the 2022-2023 season of Spain’s highest men’s football league. In this notebook, we will start from InferenceData files, but the model and data used is also described because they are key to understanding what is going on. If you are interested in the model itself in more depth, or the coding of the models themselves, refer to TODO.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#the-data",
    "href": "Chapters/Case_study_model_comparison.html#the-data",
    "title": "10  Model Comparison (case study)",
    "section": "10.2 The data",
    "text": "10.2 The data\nThe data used to fit the models are the results of all matches from 2022-2023 and the budget of each team (for the 2nd model only). Our data therefore consists of two tables: one with one row per match, containing the home and away teams and the goals scored by each; another with one row per team, containing the team and its budget.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#base-model",
    "href": "Chapters/Case_study_model_comparison.html#base-model",
    "title": "10  Model Comparison (case study)",
    "section": "10.3 Base model",
    "text": "10.3 Base model\nThe model used is taken from this blog post which was added as an example notebook to PyMC docs.\nWe are trying to model a league in which all teams play against each other twice. We indicate the number of goals scored by the home and the away team in the \\(m\\)-th match of the season (\\(M\\) matches) as \\(y_{m,h}\\) and \\(y_{m,a}\\) respectively. The model assumes the goals scored by a team follow a Poisson distribution:\n\\[y_{m,f} | \\theta_{m,f} \\sim \\text{Poiss}(\\theta_{m,f})\\]\nwhere \\(f = {h, a}\\) indicates the field, representing either home or away team. We will therefore start with a model containing two observation vectors: \\(\\mathbf{y_h} = (y_{1,h}, y_{2,h}, \\dots, y_{M,h})\\) and \\(\\mathbf{y_a} = (y_{1,a}, \\dots, y_{M,a})\\). In order to take into account each team’s scoring and defensive power and also the advantage of playing home, we will use different formulas for \\(\\theta_{m,h}\\) and for \\(\\theta_{m,a}\\):\n\\[\n\\begin{align}\n\\theta_{m,h} &= \\alpha + home + atts_{home\\_team} + defs_{away\\_team}\\\\\n\\theta_{m,a} &= \\alpha + atts_{away\\_team} + defs_{home\\_team}\n\\end{align}\n\\]\nThe expected number of goals score by the home team \\(\\theta_{m,h}\\) depends on an intercept (\\(\\alpha\\)), \\(home\\) to quantify the home advantage, on the attacking power of the home team and on the defensive power of the away team. Similarly, the expected number of goals score by the away team \\(\\theta_{m,a}\\) also depends on the intercept but not on the home advantage, and now, consequently, we use the attacking power of the away team and the defensive power of the home team.\nSumming up and including the priors, our base model is the following one:\n\\[\n\\begin{align}\n\\alpha &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nhome &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nsd_{att} &\\sim \\text{HalfStudentT}(3,2.5) \\qquad &\\text{scalar} \\\\\nsd_{def} &\\sim \\text{HalfStudentT}(3,2.5) \\qquad &\\text{scalar} \\\\\natts_* &\\sim \\text{Normal}(0,sd_{att}) \\qquad &\\text{shape (T,)} \\\\\ndefs_* &\\sim \\text{Normal}(0,sd_{def}) \\qquad &\\text{shape (T,)} \\\\\natts &= atts_* - \\text{mean}(atts_*) \\qquad &\\text{shape (T,)} \\\\\ndefs &= defs_* - \\text{mean}(defs_*) \\qquad &\\text{shape (T,)} \\\\\n\\mathbf{y}_h &\\sim \\text{Poiss}(\\theta_h) \\qquad &\\text{shape (M,)} \\\\\n\\mathbf{y}_a &\\sim \\text{Poiss}(\\theta_a) \\qquad &\\text{shape (M,)}\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) has been defined above.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#budget-model",
    "href": "Chapters/Case_study_model_comparison.html#budget-model",
    "title": "10  Model Comparison (case study)",
    "section": "10.4 Budget model",
    "text": "10.4 Budget model\nThe budget model only represents a slight variation on the base model, adding two new parameters and modifying \\(atts\\) and \\(defs\\) variables:\n\\[\n\\begin{align}\nbudget_{att} &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nbudget_{def} &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\natts &= atts_* - \\text{mean}(atts_*) + budget_{att} \\log{\\mathbf{b}} \\qquad &\\text{shape (T,)} \\\\\ndefs &= defs_* - \\text{mean}(defs_*) + budget_{def} \\log{\\mathbf{b}} \\qquad &\\text{shape (T,)} \\\\\n\\end{align}\n\\]\nwith \\(\\mathbf{b} = (b_1, b_2, \\dots, b_T)\\) the budgets of each team.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#no-field-effect-model",
    "href": "Chapters/Case_study_model_comparison.html#no-field-effect-model",
    "title": "10  Model Comparison (case study)",
    "section": "10.5 No field effect model",
    "text": "10.5 No field effect model\nThis third model is another variation on the base model, where we remove the \\(home\\) variable. Thus, the \\(\\theta\\) variables become:\n\\[\n\\begin{align}\n\\theta_{m,h} &= \\alpha + atts_{home\\_team} + defs_{away\\_team}\\\\\n\\theta_{m,a} &= \\alpha + atts_{away\\_team} + defs_{home\\_team}\n\\end{align}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#variable-and-index-glossary",
    "href": "Chapters/Case_study_model_comparison.html#variable-and-index-glossary",
    "title": "10  Model Comparison (case study)",
    "section": "10.6 Variable and index glossary",
    "text": "10.6 Variable and index glossary\n\nMatches. The total number of matches in the season, 380. \\(M\\) denotes the total, and we use \\(m\\) as the index going from \\(1\\) to \\(M\\).\nTeams. The number of teams in the league, 20. \\(T\\) denotes the total, and we use \\(t\\) as the index going from \\(1\\) to \\(T\\).\nField. The field identifier. Two teams play in each game, one being the home team, the other the away one. We use \\(f\\) as the index indicating the field, which can take only two values \\(h\\) or \\(a\\).\nArbitrary index. For theoretical concepts, we use \\(i\\) to indicate an arbitrary index.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#information-criterion-calculation",
    "href": "Chapters/Case_study_model_comparison.html#information-criterion-calculation",
    "title": "10  Model Comparison (case study)",
    "section": "10.7 Information criterion calculation",
    "text": "10.7 Information criterion calculation\nDue to the presence of the two likelihoods in our model, we cannot call az.loo or az.waic straight away because the predictive task to evaluate is ambiguous. The calculation of information criteria requires pointwise likelihood values, \\(p(y_i|\\theta)\\) with \\(y_i\\) indicating observation \\(i\\)-th and \\(\\theta\\) representing all the parameters in the model. We need to define \\(y_i\\), what does one observation represent in our model.\nAs we were introducing above, this model alone can tackle several predictive tasks. These predictive tasks can be identified by the definition of one observation which at the same time defines how are pointwise likelihood values to be calculated. Here are some examples:\n\nWe could be a group of students supporting different teams with budget to travel only to one away match of our respective teams. We may want to travel to the match where our team will score the most goals (while being the away team and also independently of the winner of the match). We will therefore assess the predictive accuracy of our model using only \\(\\mathbf{y}_a\\).\nWe could also be football fans without any clear allegiance who love an intense match between two teams of similar strength. Based on previous experience, we may consider matches that end up 3-3 or 4-4 the ones that better fit our football taste. Now we need to assess the predictive accuracy using the result of the whole match.\nEven another alternative would be wanting to be present at the match where a single team scores the most goals. In this situation, we would have to put both home and away goals in the same bag and assess the predictive accuracy on the ability to predict values from this bag, we may call the observations in this hypothetical bag “number of goals scored per match and per team”.\n\nThere are even more examples of predictive tasks where this particular model can be of use. However, it is important to keep in mind that this model predicts the number of goals scored. Its results can be used to estimate probabilities of victory and other derived quantities, but calculating the likelihood of these derived quantities may not be straightforward. And as you can see above, there isn’t one unique predictive task: it all depends on the specific question you’re interested in. As often in statistics, the answer to these questions lies outside the model, you must tell the model what to do, not the other way around.\nEven though we know that the predictive task is ambiguous, we will start trying to calculate az.loo with idata_base and then work on the examples above and a couple more to show how would this kind of tasks be performed with ArviZ. But before that, let’s see what ArviZ says when you naively ask it for the LOO of a multi-likelihood model:\n\nazp.loo(base_idata)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 azp.loo(base_idata)\n\nFile /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/loo.py:160, in loo(data, pointwise, var_name, reff, log_weights, pareto_k, log_jacobian, mixture)\n     22 def loo(\n     23     data,\n     24     pointwise=None,\n   (...)     30     mixture=False,\n     31 ):\n     32     r\"\"\"Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n     33 \n     34     Estimates the expected log pointwise predictive density (elpd) using Pareto-smoothed\n   (...)    158        arXiv preprint https://arxiv.org/abs/1507.02646\n    159     \"\"\"\n--&gt; 160     loo_inputs = _prepare_loo_inputs(data, var_name)\n    161     pointwise = rcParams[\"stats.ic_pointwise\"] if pointwise is None else pointwise\n    163     if reff is None:\n\nFile /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:169, in _prepare_loo_inputs(data, var_name, thin_factor)\n    166 \"\"\"Prepare inputs for PSIS-LOO-CV.\"\"\"\n    167 data = convert_to_datatree(data)\n--&gt; 169 log_likelihood = get_log_likelihood(data, var_name=var_name)\n    170 if var_name is None and log_likelihood.name is not None:\n    171     var_name = log_likelihood.name\n\nFile /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/utils.py:89, in get_log_likelihood(idata, var_name)\n     87     var_names = list(idata.log_likelihood.data_vars)\n     88     if len(var_names) &gt; 1:\n---&gt; 89         raise TypeError(\n     90             f\"Found several log likelihood arrays {var_names}, var_name cannot be None\"\n     91         )\n     92     return idata.log_likelihood[var_names[0]]\n     93 try:\n\nTypeError: Found several log likelihood arrays ['away_goals', 'home_goals'], var_name cannot be None\n\n\n\nAs expected, ArviZ has no way of knowing what predictive task we have in mind so it raises an error.\n\n10.7.1 Predicting the goals scored by the away team\nIn this particular case, we are interested in predicting the goals scored by the away team. We will still use the goals scored by the home team, but won’t take them into account when assessing the predictive accuracy. Below there is an illustration of how would cross validation be performed to assess the predictive accuracy in this particular case:\n\nThis can also be seen from a mathematical point of view. We can write the pointwise log likelihood in the following way so it defines the predictive task at hand:\n\\[ p(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h}) = \\text{Poiss}(y_{i,h}; \\theta_{i,h}) \\]\nwith \\(i\\) being both the match indicator (\\(m\\), which varies with \\(i\\)) and the field indicator (\\(f\\), here always fixed at \\(h\\)). These are precisely the values stored in the home_goals of the log_likelihood group of idata_base.\nWe can tell ArviZ to use these values using the argument var_name.\n\nazp.loo(base_idata, var_name=\"home_goals\")\n\nComputed from 8000 posterior samples and 380 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -568.64    12.66\np_loo       15.66        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      380  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\n\nazp.compare(model_dict, var_name=\"home_goals\")\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nbudget\n0\n-567.043220\n10.816315\n0.000000\n7.288939e-01\n13.062672\n0.000000\nFalse\n\n\nbase\n1\n-568.638368\n15.658577\n1.595148\n2.711061e-01\n12.658792\n2.760170\nFalse\n\n\nnofield\n2\n-574.236398\n15.856731\n7.193178\n3.047095e-17\n15.315891\n4.363745\nFalse\n\n\n\n\n\n\n\n\nazp.compare(model_dict, var_name=\"away_goals\")\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nbudget\n0\n-480.396382\n6.934533\n0.000000\n1.000000e+00\n11.548033\n0.000000\nFalse\n\n\nbase\n1\n-482.986789\n10.281413\n2.590407\n0.000000e+00\n11.811082\n1.929305\nFalse\n\n\nnofield\n2\n-489.086355\n10.270825\n8.689973\n1.937339e-14\n9.464333\n3.399195\nFalse\n\n\n\n\n\n\n\n\n\n10.7.2 Predicting the outcome of a match\nAnother option is being interested in the outcome of the matches. In our current model, the outcome of a match is not who wins or the aggregate of scored goals by both teams, the outcome is the goals scored by the home team and by the away team, both quantities at the same time. Below there is an illustration on how would cross validation be used to assess the predictive accuracy in this situation:\n\nThe one observation in this situation is therefore a vector with two components: \\(y_i = (y_{i,h}, y_{i,a})\\). Like above, we also have \\(M\\) observations. The pointwise likelihood is therefore a product:\n\\[\np(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h})p(y_{i,a}|\\theta_{i,a}) =\n\\text{Poiss}(y_{i,h}; \\theta_{i,h})\\text{Poiss}(y_{i,a}; \\theta_{i,a})\n\\]\nwith \\(i\\) being equal to the match indicator \\(m\\). Therefore, we have \\(M\\) observations like in the previous example, but each observation has two components.\nWe can calculate the product as a sum of logarithms and store the result in a new variable inside the log_likelihood group.\n\ndef match_lik(idata):\n    log_lik = idata.log_likelihood\n    log_lik[\"matches\"] = log_lik.home_goals + log_lik.away_goals\n    return idata\n\nbase_idata = match_lik(base_idata)\nbudget_idata = match_lik(budget_idata)\nnofield_idata = match_lik(nofield_idata)\n\nazp.loo(base_idata, var_name=\"matches\")\n\nComputed from 8000 posterior samples and 380 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1051.61    16.91\np_loo       25.92        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      380  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\n\nazp.compare(model_dict, var_name=\"matches\")\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nbudget\n0\n-1047.433817\n17.747036\n0.000000\n0.86305\n17.076692\n0.000000\nFalse\n\n\nbase\n1\n-1051.613764\n25.922732\n4.179948\n0.13695\n16.908654\n3.381969\nFalse\n\n\nnofield\n2\n-1063.324209\n26.128229\n15.890393\n0.00000\n17.570441\n5.456613\nFalse\n\n\n\n\n\n\n\n\n\n10.7.3 Predicting the goals scored per match and per team\nAnother example described above is being interested in the scored goals per match and per team. In this situation, our observations are a scalar once again.\n\nThe expression of the likelihood is basically the same as the one in the first example (both cases are scalars), but the difference is in the index, but that does not make it less significant:\n\\[\np(y_i|\\theta) = p(y_{i}|\\theta_{i}) =\n\\text{Poiss}(y_{i}; \\theta_{i})\n\\]\nwith \\(i\\) being both the match indicator \\(m\\) and the field indicator \\(f\\), both varying with \\(i\\). Now, we will consider \\(i\\) as an index iterating over the values in\n\\[\\big\\{(1,h), (2,h), \\dots, (M-1,h), (M,h), (1,a), (2,a), \\dots, (M-1,a), (M,a)\\big\\}\\]\nTherefore, unlike in previous cases, we have \\(2M\\) observations.\nWe can obtain the pointwise log likelihood corresponding to this case by concatenating the pointwise log likelihoods of home_goals and away_goals. Then, like in the previous case, store the result in a new variable inside the log_likelihood group.\n\ndef goals_lik(idata):\n    log_lik = idata.log_likelihood\n    log_lik[\"goals\"] = xr.concat((log_lik.home_goals, log_lik.away_goals), \"match\").rename({\"match\": \"goal\"})\n    return idata\n    \nbase_idata = goals_lik(base_idata)\nbudget_idata = goals_lik(budget_idata)\nnofield_idata = goals_lik(nofield_idata)\n\nazp.loo(base_idata, var_name=\"goals\")\n\nComputed from 8000 posterior samples and 760 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1051.63    17.59\np_loo       25.94        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      760  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n    (1, Inf)   (very bad)    0    0.0%\n\n\n\nazp.compare(model_dict, var_name=\"goals\")\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nbudget\n0\n-1047.439602\n17.750849\n0.000000\n0.864661\n17.716348\n0.000000\nFalse\n\n\nbase\n1\n-1051.625157\n25.939989\n4.185556\n0.135339\n17.589757\n3.367797\nFalse\n\n\nnofield\n2\n-1063.322753\n26.127556\n15.883151\n0.000000\n18.267192\n5.531704\nFalse\n\n\n\n\n\n\n\n\n\n10.7.4 Predicting team level performance\nThe last example covered here is estimating the predictive accuracy at group level. This can be useful to assess the accuracy of predicting the whole season of a new team. In addition, this can also be used to evaluate the hierarchical part of the model.\nAlthough theoretically possible, importance sampling tends to fail at the group level due to all the observations being too informative. See this post for more details.\nIn this situation, we could describe the cross validation as excluding a team. When we exclude a team, we will exclude all the matches played by the team, not only the goals scored by the team but the whole match. Here is the illustration:\n\nIn the first column, we are excluding “Levante U.D.” which in the rows shown only appears once. In the second one, we are excluding “Athletic Club” which appears two times. This goes on following the order of appearance in the away team column.\n\ndef team_lik(idata):\n    log_lik = idata.log_likelihood.to_dataset()\n    const = idata.constant_data.to_dataset()\n    groupby_sum_home = log_lik.groupby(const.home_team).sum().rename({\"home_team\": \"team\"})\n    groupby_sum_away = log_lik.groupby(const.away_team).sum().rename({\"away_team\": \"team\"})\n\n    idata.log_likelihood[\"teams_match\"] = (\n        groupby_sum_home.home_goals + groupby_sum_home.away_goals +\n        groupby_sum_away.home_goals + groupby_sum_away.away_goals\n    )\n    return idata\n    \nbase_idata = team_lik(base_idata)\nbudget_idata = team_lik(budget_idata)\nnofield_idata = team_lik(nofield_idata)\n\n\nazp.loo(base_idata, var_name=\"teams_match\")\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 8000 posterior samples and 20 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -2110.30    20.03\np_loo       53.29        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)        1    5.0%\n   (0.70, 1]   (bad)        15   75.0%\n    (1, Inf)   (very bad)    4   20.0%\n\n\nTODO: it would probably be best to run reloo for the three models for this case and include that on figshare too.\n\nazp.compare(model_dict, var_name=\"teams_match\")\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/arviz_stats/loo/helper_loo.py:1149: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd\np\nelpd_diff\nweight\nse\ndse\nwarning\n\n\n\n\nbudget\n0\n-2098.355462\n36.701964\n0.000000\n0.821164\n22.638990\n0.000000\nTrue\n\n\nbase\n1\n-2110.296027\n53.290944\n11.940565\n0.153488\n20.034114\n6.858487\nTrue\n\n\nnofield\n2\n-2132.258357\n51.606961\n33.902894\n0.025348\n20.272027\n9.532154\nTrue",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Variable_selection.html",
    "href": "Chapters/Variable_selection.html",
    "title": "11  Variable Selection",
    "section": "",
    "text": "11.1 Methods for variable selection\nVariable selection refers to the process of identifying the most relevant variables in a model from a larger set of predictors. When performing this process we usually assume that variables contribute unevenly to the outcome and we want to identify the most important ones. Sometimes we also care about the order in which variables are included in the model (Piironen, Paasiniemi, and Vehtari 2020; Tadesse and Vannucci 2022).\nOne might argue that “the most Bayesian thing to do” is to always include all conceivable variables in a model and then use the posterior distribution to make predictions or understand variable relationships. This approach is considered “most Bayesian” because it leverages the maximum amount of data and incorporates uncertainty about variable importance into the posterior distribution. However, being more Bayesian than Bayes is not always the best idea.\nVariable selection becomes particularly useful when:\nThere are many methods for variable selection we will restrict the discussion to two methods for variable selection. Even more, we are going to restrict the discussion to two particular implementations of these methods.\nFor both methods is of central importance to understand the concept of “reference model” as they work by comparing the reference model with a series of submodels. The reference model is the model that has all the variables we consider relevant a priori and is the only model that we will fit using standard inference methods, like MCMC. In both methods, the submodels are created by selecting a subset of the covariates in the reference model. As the number of possible submodels grows very fast with the number of covariates, both methods can use different heuristics to select the most promising submodels to fit. The main difference between these two methods is how they approximate the posterior distribution of the submodels and how they use this approximation to select the most promising submodels.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Variable Selection</span>"
    ]
  },
  {
    "objectID": "Chapters/Variable_selection.html#methods-for-variable-selection",
    "href": "Chapters/Variable_selection.html#methods-for-variable-selection",
    "title": "11  Variable Selection",
    "section": "",
    "text": "The first one it’s available in PyMC-BART (Quiroga et al. 2022) and works for Bayesian Additive Regression Models.\nThe second method is implemented in Kulprit and is currently compatible with a subset of models supported by Bambi. However, the aim is to extend compatibility to all models that can be handled by Bambi. A very accesible paper discussing this method in general and not the particular implementation in Kulprit is McLatchie et al. (2023).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Variable Selection</span>"
    ]
  },
  {
    "objectID": "Chapters/Variable_selection.html#variable-selection-with-bart",
    "href": "Chapters/Variable_selection.html#variable-selection-with-bart",
    "title": "11  Variable Selection",
    "section": "11.2 Variable selection with BART",
    "text": "11.2 Variable selection with BART\nBART is a non-parametric regression method based on a sum of trees, this is all we need to understand at the moment for details you can read the original paper (Chipman, George, and McCulloch 2010) and the paper introducing PyMC-BART (Quiroga et al. 2022).\nOnce we fit a BART model we can count the number of times each covariate is used in the posterior distribution of trees. This count can be used as a measure of the importance of the covariate. We will call this the “variable inclusion” measure, or VI for short, and is normalized to sum to 1. One heuristic is to start with a model with a single covariate and then add covariates in decreasing order of VI. This is fast as we only need to evaluate as many models as covariates we have. This is the default heuristic in PyMC-BART. Then to evaluate the quality of each submodel we compute the predictions from it and compare the predictions with those from the reference model. Currently, PyMC-BART uses the R², but other metrics could be used. For the submodels the predictions are computed by pruning the trees in the posterior distribution of the reference model. That is we remove the trees the branches that do not use the covariates in the submodel. In this way, we approximate the posterior distribution of the submodel and we can compute the predictions without the need to refit each submodel.\nLet’s see one example. We have a record of the number of rented bikes in a city and other variables like temperature, hour of the day etc. We want to model the relationship between temperature and the number of rented bikes.\nLet’s first load the data and define the variables.\n\nbikes = pd.read_csv(\"../data/bikes.csv\")\n\nY = bikes.pop(\"count\")\nX = bikes\n\nNow we define the BART model using PyMC-BART, for details on how to define BART models, please check the PyMC-BART documentation\n\nwith pm.Model() as model:\n    α = pm.HalfNormal(\"α\", 2)\n    μ_ = pmb.BART(\"μ_\", X, np.log(Y))\n    μ = pm.Deterministic(\"μ\", np.exp(μ_))\n    y = pm.NegativeBinomial(\"y\", mu=μ, alpha=α, observed=Y)\n    idata_bart = pm.sample(random_seed=seed,\n                      compute_convergence_checks=False)\n\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n&gt;NUTS: [α]\n&gt;PGBART: [μ_]\n\n\n\n\n\n\n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 52 seconds.\n\n\nWe can now compute the variable importance and plot the results. The dashed lines represent the mean R² for the predictions from the reference model against themselves, the band captures the uncertainty. Then from left to right, we add covariates in decreasing order of VI. The first submodel is the one with only hour, the second is hour + temperature, the third is hour + temperature + month and so on.\n\nvi_results = pmb.compute_variable_importance(idata_bart, μ_, X);\nax = pmb.plot_variable_importance(vi_results, plot_kwargs={\"rotation\": 45}, figsize=(10, 4))\nax.set_ylim(0.5, 1)\n\n\n\n\n\n\n\n\nWe can see that the most relevant variable is hour, followed by temperature after that it is difficult to see any improvement and all differences appear to be noise.\nPyMC-BART offers two other heuristics. \"Backward\" and VI-Backward, the first one ignores the variable inclusion information and instead begins by computing all models with one variable less than the total, it removes the one with the lowest R² and then repeats the process until only one variable is left. This method is more expensive as it needs to compute a lot of models and it will not scale well with the number of covariates. It is only recommended when the result of the default VI method looks suspicious. For instance, we should always expect that the R² increases monotonically with the number of covariates, if this is not the case we should use the Backward method. The VI-Backward method is a blend of both heuristics. It performs a backward search but the variables with the highest variable inclusion are fixed during this search. How many variables we consider fixed is a user choice. This method is more expensive than the default VI method but less than the Backward method.\n\n11.2.1 Partial dependence plot\nStrictly speaking when we say we prune the trees we are actually computing the partial dependence, that is, we are computing the expected value of the outcome for a subset of the covariates while averaging over the complement of that subset. With PyMC-BART we can visually inspect the partial dependence when excluding all but one covariate. As in the following figure:\n\naxes = pmb.plot_pdp(μ_, X=X, Y=Y, grid=(2, 3), func=np.exp, var_discrete=[0, 2], xs_interval=\"insample\", figsize=(12, 5))\n\n\n\n\n\n\n\n\nThe dashed lines represent the null model, the more a variable deviates from these lines the more its impact on the response variable. We can see a qualitative agreement between the variable importance and the partial dependence plots.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Variable Selection</span>"
    ]
  },
  {
    "objectID": "Chapters/Variable_selection.html#variable-selection-with-kulprit",
    "href": "Chapters/Variable_selection.html#variable-selection-with-kulprit",
    "title": "11  Variable Selection",
    "section": "11.3 Variable selection with Kulprit",
    "text": "11.3 Variable selection with Kulprit\nKulprit is an implementation of a method known as projective inference. The main idea is that we can “project” the reference model’s posterior into the submodels. Conceptually, it is easier to understand this projection as a procedure to find a posterior distribution for the submodel that will induce a posterior predictive distribution that is as close as possible to the posterior predictive distribution of the reference model. Intuitively, this makes sense in the context of variable selection, as we want to find a model that is smaller than the reference model but makes predictions that are as close as possible to it.\nIt turns out, that this can be achieved as an optimization problem. Let’s see.\nDenote \\(\\theta\\) as the parameter of the posterior from the reference model, and \\(\\theta_\\perp\\) those of the posterior for a particular submodel. Denote \\(\\tilde{y}\\) the samples from the posterior predictive distribution of the reference model \\(p(\\tilde{y} \\mid \\theta)\\). Then we want to find a posterior that induces the posterior predictive distribution \\(q(\\tilde{y} \\mid \\theta_\\perp)\\). We want \\(p\\) and \\(q\\) to be as close as possible. As we already discussed in Section 7.4.2 we can use the Kullback-Leibler divergence to measure how close two distributions are. Then we can write:\n\\[\\begin{align}\n\\mathbb{KL}\\{p(\\tilde{y}\\mid\\theta) \\lVert q(\\tilde{y})\\} &= \\mathbb{E}_{\\tilde{y}\\sim p(\\tilde{y}\\mid\\theta)} \\left[ \\log \\frac{p(\\tilde{y}\\mid\\theta)}{q(\\tilde{y}\\mid\\theta_\\perp)} \\right] \\\\\n&= \\underbrace{\\mathbb{E}_{\\tilde{y}\\sim p(\\tilde{y}\\mid\\theta)} \\left[ \\log p(\\tilde{y}\\mid\\theta)\\right]}_{\\text{constant}} - \\mathbb{E}_{\\tilde{y}\\sim p(\\tilde{y}\\mid\\theta)} \\left[ \\log q(\\tilde{y}\\mid\\theta_\\perp)\\right] \\\\\n&\\propto - \\mathbb{E}_{\\tilde{y}\\sim p(\\tilde{y}\\mid\\theta)} \\left[ \\log q(\\tilde{y}\\mid\\theta_\\perp)\\right]\n\\end{align}\\]\nIn the proposed approach \\(\\log q(\\tilde{y} \\mid \\theta_\\perp)\\) is the log-likelihood of our model evaluated with respect to samples from the posterior predictive distribution \\(\\tilde{y}\\sim p(\\tilde{y}\\mid\\theta)\\). Thus to minimize the KL divergence we can maximize the model’s log-likelihood with respect to the posterior predictive samples from the reference model. This is the optimization problem we need to solve to find the posterior distribution of the submodel.\nLet’s use Kulprit to perform variable selection on the same dataset we used for PyMC-BART.\nThe first thing we need to do is to define the model using Bambi. We need to set idata_kwargs={'log_likelihood': True} as we will later need to compute the ELPD of the reference models and submodels.\n\nmodel = bmb.Model(\"count ~\" +  \" + \".join([c for c in bikes.columns if c!=\"count\"]), data=bikes, family=\"negativebinomial\")\nidata = model.fit(idata_kwargs={'log_likelihood': True}, random_seed=seed)\n\nTo use Kulprit we first instantiate the ProjectionPredictive class and then call the project method, which is the one doing all the hard work.\n\nppi = kpt.ProjectionPredictive(model, idata)\nppi.project()\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytensor/link/c/cmodule.py:2986: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\nThis usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\nAlternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\nFor more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n  warnings.warn(\n\n\nOnce this is finished we can inspect the ppi object manually, but a figure is usually a better idea. By default, the compare function plots all the models, including the intercept-only model, i.e. a model without any covariate. In the following block of code, we are asking to omit this model.\n\nkpt.plot_compare(ppi.compare(min_model_size=1));\n\n\n\n\n\n\n\n\nWe can see that this figure is very similar to the one generated with pmb.plot_variable_importance and its interpretation is also similar. One important difference is that plot_compare uses the ELPD to compare the models.\nWe can see that for Kulprit the most relevant variable is hour, followed by temperature and then humidity, after that adding more variables does not improve the model. The order agrees with PyMC-BART but Kulprit considers 3 variables as relevant instead of 2. This is most likely because the effect of hour is non-linear and thus difficult to capture with a simple linear model. Let’s put this idea to the test.\nInstead of using the variable hour, let’s apply a transformation first.\n\nbikes = pd.read_csv(\"../data/bikes.csv\")\nbikes[\"hour_sin\"] = np.sin(bikes.hour * np.pi / 12)\nbikes[\"hour_cos\"] = np.cos(bikes.hour * np.pi / 12)\nbikes.drop(columns=\"hour\", inplace=True)\n\nNow we can repeat the process, first we fit the model with the transformed variables\n\nmodel = bmb.Model(\"count ~\" +  \" + \".join([c for c in bikes.columns if c!=\"count\"]), data=bikes, family=\"negativebinomial\")\nidata = model.fit(idata_kwargs={'log_likelihood': True}, random_seed=seed)\n\nand then we use Kulprit to perform variable selection.\n\nppi = kpt.ProjectionPredictive(model, idata)\nppi.project()\nkpt.plot_compare(ppi.compare(min_model_size=1));\n\n\n\n\n\n\n\n\nAnd voilà! If we consider hour_cos and hour_sin as a single variable, then the two most relevant variables are hour and temperature, in agreement with PyMC-BART.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Variable Selection</span>"
    ]
  },
  {
    "objectID": "Chapters/Variable_selection.html#combining-pymc-bart-and-kulprit",
    "href": "Chapters/Variable_selection.html#combining-pymc-bart-and-kulprit",
    "title": "11  Variable Selection",
    "section": "11.4 Combining PyMC-BART and Kulprit",
    "text": "11.4 Combining PyMC-BART and Kulprit\nBoth methods are not necessarily competitors and there is room for collaboration. When this is useful is still open to research but we can think of some scenarios where this could be useful. For instance, we could use PyMC-BART to find the most relevant variables, according to the variable inclusion metric, and then use Kulprit to perform the projection and evaluation of those selected submodels. We could also use the posterior predictive samples from PyMC-BART instead of a GLM. The second option is not currently supported in Kulprit, but the first one is straightforward as the method project has a user_terms argument that allows us to specify the submodels that we want to project. Notice that if we provide the user_terms there is no search done, Kulprit will project and evaluate just the submodels we specify.\n\nterms = [[\"hour_cos\", \"hour_sin\"],\n         [\"hour_cos\", \"hour_sin\", \"temperature\"],\n         [\"hour_cos\", \"hour_sin\", \"temperature\", \"humidity\"],\n         [\"hour_cos\", \"hour_sin\", \"temperature\", \"humidity\", \"month\"],\n         [\"hour_cos\", \"hour_sin\", \"temperature\", \"humidity\", \"month\", \"weekday\"],\n         [\"hour_cos\", \"hour_sin\", \"temperature\", \"humidity\", \"month\", \"weekday\", \"windspeed\"],\n        ]\nppi.project(user_terms=terms)\n\nIn the previous code block we have manually specfified the terms, but we can also ask PyMC-BART to return a list for us.\n\npmb.vi_to_kulprit(vi_results)\n\n[[],\n ['hour'],\n ['hour', 'temperature'],\n ['hour', 'temperature', 'humidity'],\n ['hour', 'temperature', 'humidity', 'month'],\n ['hour', 'temperature', 'humidity', 'month', 'windspeed']]\n\n\n\n\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010. “BART: Bayesian Additive Regression Trees.” The Annals of Applied Statistics 4 (1): 266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nMcLatchie, Yann, Sölvi Rögnvaldsson, Frank Weber, and Aki Vehtari. 2023. “Robust and Efficient Projection Predictive Inference.” https://arxiv.org/abs/2306.15581.\n\n\nPiironen, Juho, Markus Paasiniemi, and Aki Vehtari. 2020. “Projective inference in high-dimensional problems: Prediction and feature selection.” Electronic Journal of Statistics 14 (1): 2155–97. https://doi.org/10.1214/20-EJS1711.\n\n\nQuiroga, Miriana, Pablo G Garay, Juan M. Alonso, Juan Martin Loyola, and Osvaldo A Martin. 2022. “Bayesian Additive Regression Trees for Probabilistic Programming.” arXiv. https://doi.org/10.48550/ARXIV.2206.03619.\n\n\nTadesse, Mahlet G., and Marina Vannucci, eds. 2022. Handbook of Bayesian Variable Selection. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003089018.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Variable Selection</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html",
    "href": "Chapters/Prior_elicitation.html",
    "title": "12  Prior Elicitation",
    "section": "",
    "text": "12.1 Priors and Bayesian Statistics\nSpecification of the prior distribution for a Bayesian model is key, but it is often difficult even for statistical experts. Having to choose a prior distribution is portrayed both as a burden and as a blessing. We choose to affirm that is a necessity, if you are not choosing your priors someone else is doing it for you (Mikkola et al. 2024). Letting others decide for you is not always a bad idea. Default priors provided by tools like Bambi or brms can be very useful in many problems, but it’s advantageous to be able to specify custom priors when needed.\nThe process of transforming domain knowledge into well-defined prior distributions is called Prior Elicitation and as we already said is an important part of Bayesian modelling. In this chapter, we will discuss some general approaches to prior elicitation and provide some examples. Two other sources of information about prior elicitation that complement this chapter are the PreliZ documentation, a Python package for prior elicitation that we will discuss here, and PriorDB, a database of prior distributions for Bayesian analysis.\nIf you are reading this guide, you probably already know what a prior distribution is. But let’s do a quick recap. In Bayesian statistics, the prior distribution is the probability distribution that expresses information about the parameters of the model before observing the data. The prior distribution is combined with the likelihood to obtain the posterior distribution, which is the distribution of the parameters after observing the data.\nPriors are one way to convey domain-knowledge information into models. Other ways to include information in a model are the type of model or its overall structure, e.g. using a linear model, and the choice of likelihood.\nLet use a couple of examples to think about priors and it’s role in Bayesian statistics, Figure 12.2 shows two priors, one in blue (Beta(0.5, 0.5)) and one in red (Beta(10, 10)).\nWe are going to combine these priors with data using a Binomial likelihood. And we are going to update these priors sequentially, i.e. we will be adding some data and compute the posterior. And then add some more data and keep updating. So, at each step \\(i\\) we add data and the distribution we are computing are the posteriors, but these posteriors are also the priors for step \\(i+1\\). This sequential updating, where a posterior becomes the prior of the next analysis, is possible because the Beta distribution is conjugate with the Binomial. Conjugate prior are priors that when combined with a likelihood function result in a posterior distribution that is of the same form as the prior distribution. Usually, we don’t care too much about conjugate priors, but sometimes, like for this animation, they can be useful.\nWe have represented these sequential updating in an animation (see Figure 12.3). As the animation moves forward, i.e. we add more data, we will see that the posteriors gradually converge to the same distribution.\nAsymptotically, priors have no meaning. If we have infinite data, the posterior will be the same regardless of the chosen prior. When there is large amount of data, the update is dominated by the likelihood function, and the prior has little influence. Different reasonable priors converge to the same posterior as data size is increased.\nBut there are two catches. If we assign 0 prior probability to a value, no amount of data will turn that into a positive value. In other words, if a particular value or hypothesis is assigned zero prior probability, it will also have zero posterior probability, regardless of the data observed. Alternatively, if we assign a prior probability of 1 to a value (and zero to the rest), no amount of data will allow us to update that prior neither. This is known as Cromwell’s rule and states that the use of prior probabilities of 1 (“the event will definitely occur”) or 0 (“the event will definitely not occur”) should be avoided, except when applied to statements that are logically true or false, such as \\(2+2=4\\).\nOK, but if we take Cromwell’s advice and avoid these corner cases eventually the data will dominate the priors. That’s true, as well as that asymptotically, we are all dead. For real, finite data, we should expect priors to have some impact on the results. The actual impact will depend on the specific combinations of priors, likelihood and data. Section 12.3 shows a couple of combinations. In practice we often need to worry about our priors, but maybe less than we think.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#priors-and-bayesian-statistics",
    "href": "Chapters/Prior_elicitation.html#priors-and-bayesian-statistics",
    "title": "12  Prior Elicitation",
    "section": "",
    "text": "Figure 12.2: Two different priors\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: Priors updating as we keep adding data\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: If we give 0 prior probability, then we will always get 0 posterior probability\n\n\n\n\n\n\n\n\n\n\nFigure 12.5: The posterior is an interplay of prior and likelihood",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#types-of-priors",
    "href": "Chapters/Prior_elicitation.html#types-of-priors",
    "title": "12  Prior Elicitation",
    "section": "12.2 Types of Priors",
    "text": "12.2 Types of Priors\nUsually, priors are described as informative vs non-informative. Informative priors are priors that convey specific information about the parameters of the model, while non-informative priors do not convey specific information about the parameters of the model. Non-informative priors are often used when little or no domain knowledge is available. A simple, intuitive and old rule for specifying a non-informative prior is the principle of indifference, which assigns the same probability to all possible events. Non-informative priors are also called objective priors especially when the main motivation for using them is to avoid the need to specify a prior distribution.\nNon-informative priors can be detrimental and difficult to implement or use. Informative priors can also be problematic in practice, as the information needed to specify them may be absent or difficult to obtain. And even if the information is available specifying informative priors can be time-consuming. A middle ground is to use weakly informative priors, which are priors that convey some information about the parameters of the model but are not overly specific. Weakly informative priors can help to regularize inference and even have positive side effects like improving sampling efficiency.\n\n\n\n\n\n\nFigure 12.6: Priors are often defined in terms of how much information they convey\n\n\n\nIt is important to recognize that the amount of information a prior carries can vary continuously and that the categories we use to discuss priors are a matter of convenience and not a matter of principle. These categories are qualitative and not well-defined. Still, they can be helpful when talking about priors more intuitively.\nSo far we have discussed the amount of information. There are at least two issues that seem fishy about this discussion. First, the amount of information is a relative concept, against what are we evaluating if a prior is informative or not? Second, the amount of information does not necessarily mean the information is good or correct. For instance, it’s possible to have a very informative prior based on wrong assumptions. Thus when we say informative we don’t necessarily mean reliable or that the prior will bias the inference in the correct direction and amount.\nThere is one way to frame the discussion about priors that can help to address these issues. That is to think about priors in terms of the prior predictive distribution they induce. In other words, we think about the priors in terms of their predictions about unobserved, potentially observable data. This mental scaffold can be helpful in many ways:\n\nFirst, it naturally leads us to think about priors in relation to other priors and the likelihood, i.e. it reflects the fact that we cannot understand a prior without the context of the model (Gelman, Simpson, and Betancourt 2017).\nSecond, it gives us an operational definition of what we mean by vague, informative, or weakly informative prior. An informative prior is a prior that makes predictions that are about the same. A weakly informative prior is a prior that makes predictions that are somewhere in between. The distinctions are still qualitative and subjective, but we have a criteria that is context-dependent and we can evaluate during a Bayesian data analysis. Figure 12.7 shows a very schematic representation of this idea.\nThird, it provides us with a way to evaluate the priors for consistency, because the priors we are setting should agree with the prior predictive distribution we imagine. For instance, if we are setting an informative prior that induces a prior predictive distribution that is narrower, shifted or very different in any other way from the one we imagine either the prior or our expectation of the prior predictive distribution is wrong. We have specified two conflicting pieces of information. Reconciling these two pieces of information does not guarantee that the prior or any other part of the model is correct, but it provides internal consistency, which is a good starting point for a model.\n\n\n\n\n\n\n\nFigure 12.7: Prior amount of information in terms of the prior predictive distribution induced by them\n\n\n\nUsing the prior predictive distribution to evaluate priors is inherently a global approach, as it assesses the combined impact of all priors and the likelihood. However, during prior elicitation, we may sometimes focus on making one or two priors more informative while keeping the others vague. In these cases, we can think of this as having a “local” mix of priors with varying levels of informativeness. In practice, we often balance this global perspective with the local approach, tailoring priors to the specific needs of the model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#sec-prior-elicitation-workflow",
    "href": "Chapters/Prior_elicitation.html#sec-prior-elicitation-workflow",
    "title": "12  Prior Elicitation",
    "section": "12.3 Bayesian Workflow for Prior Elicitation",
    "text": "12.3 Bayesian Workflow for Prior Elicitation\nPrior elicitation is a key part of a flexible iterative workflow. It can be specific to the needs of the model and data at hand. It may need revision as we develop the model and analyse the data.\nKnowing when to perform prior elicitation is central to a prior elicitation workflow. In some situations, default priors and models may be sufficient, especially for routine inference that applies the same, or very similar, model to similar new datasets. But even for new datasets, default priors can be a good starting point, adjusting them only after initial analysis reveals issues with the posterior or computational problems. As with other components of the Bayesian workflow, prior elicitation isn’t just a one-time task. It’s not even one that is always done at the beginning of the analysis.\nFor simple models with strong data, the prior may have minimal impact, and starting with default or weakly informed priors may be more appropriate and provide better results than attempting to generate very informative priors. The key is knowing when it’s worth investing resources in prior elicitation. Or more nuanced how much time and domain knowledge is needed in prior specification. Usually, getting rough estimates can be sufficient to improve inference. Thus, in practice, weakly informative priors are often enough. In a model with many parameters eliciting all of them one by one may be too time-consuming and not worth the effort. Refining just a few priors in a model can be sufficient to improve inference.\nThe prior elicitation process should also include a step to verify the usefulness of the information and assess how sensitive the results are to the choice of priors, including potential conflicts with the data. This process can help identify when more or less informative priors are needed and when the model may need to be adjusted.\nFinally, we want to highlight that prior elicitation isn’t just about choosing the right prior but also about understanding the model and the problem. So even if we end up with a prior that has little impact on the posterior, compared to a vague or default prior, performing prior elicitation could be useful for the modeller. Especially among newcomers setting priors can be seen as an anxiogenic task. Spending some time thinking about priors, with the help of proper tools, can help reduce this brain drain and save mental resources for other modelling tasks.\nNevertheless, usually, the selling point when discussing in favour of priors is that they allow the inclusion of domain information. But there are potentially other advantages of:\n\nSampling efficiency. Often a more informed priors results in better sampling. This does not mean we should tweak the prior distribution to solve sampling problems, instead incorporating some domain-knowledge information can help to avoid them.\nRegularization. More informed priors can help to regularize the model, reducing the risk of overfitting. We make a distinction between regularization and “conveying domain-knowledge information” because motivations and justifications can be different in each case.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#priors-and-entropy",
    "href": "Chapters/Prior_elicitation.html#priors-and-entropy",
    "title": "12  Prior Elicitation",
    "section": "12.4 Priors and entropy",
    "text": "12.4 Priors and entropy\nThe entropy is a property of probability distributions the same way the mean or variance are, actually it’s the expected value of the negative log probability of the distribution. We can think of entropy as a measure of the information or uncertainty of a distribution has. Loosely speaking the entropy of a distribution is high when the distribution is spread out and low when the distribution is concentrated. In the context of prior elicitation maximum entropy can be a guiding principle to pick priors. According to this principle we should choose the prior that maximizes the entropy, subject to known constraints of the prior (Jaynes 2003). This is a way to choose a prior that is as vague as possible, given the information we have. Figure 12.8 shows a distribution with support in [0, 1]. On the first panel we have the distribution with maximum entropy and no other restrictions. We can see that this is a uniform distribution. On the middle we have the distribution with maximum entropy and a given mean. This distribution looks similar to an exponential distribution. On the last panel we have the distribution with maximum entropy and 70% of its mass between 0.5 and 0.75.\n\n\n\n\n\n\nFigure 12.8: 3 maximum entropy distributions subject to different constrains\n\n\n\nFor some priors in a model, we may know or assume that most of the mass is within a certain interval. This information is useful for determining a suitable prior, but this information alone may not be enough to obtain a unique set of parameters. Figure 12.9 shows Beta distributions with 90% of the mass between 0.1 and 0.7. As you can see we can obtain very different distributions, conveying very different prior knowledge. The red distribution is the one with maximum entropy, given the constraints.\n\n\n\n\n\n\nFigure 12.9: Beta distributions with a 90% of it mass between 0.1 and 0.7, the red one is the one with maximum entropy",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#preliz",
    "href": "Chapters/Prior_elicitation.html#preliz",
    "title": "12  Prior Elicitation",
    "section": "12.5 Preliz",
    "text": "12.5 Preliz\nPreliZ (Icazatti et al. 2023) is a Python package that helps practitioners choose prior distributions by offering a set of tools for the various facets of prior elicitation. It covers a range of methods, from unidimensional prior elicitation on the parameter space to predictive elicitation on the observed space. The goal is to be compatible with probabilistic programming languages (PPL) in the Python ecosystem like PyMC and CmdStanPy, while remaining agnostic of any specific PPL.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#maximum-entropy-distributions-with-maxent",
    "href": "Chapters/Prior_elicitation.html#maximum-entropy-distributions-with-maxent",
    "title": "12  Prior Elicitation",
    "section": "12.6 Maximum entropy distributions with maxent",
    "text": "12.6 Maximum entropy distributions with maxent\nIn PreliZ we can compute maximum entropy priors using the function maxent. It works for unidimensional distributions. The first argument is a PreliZ distribution. Then we specify an upper and lower bound and the probability between them.\nAs an example, we want to elicit a scale parameter. From domain knowledge we know the parameter has a relatively high probability of being less than 3. Hence, we could use a HalfNormal distribution and do:\n\npz.maxent(pz.HalfNormal(), 0, 3, 0.8);\n\n\n\n\n\n\n\n\nWhen we want to avoid values too close to zero, other distributions like Gamma or InverseGamma may be a better choice.\n\npz.maxent(pz.Gamma(), 0, 3, 0.8);\n\n\n\n\n\n\n\n\nWe could also have extra restrictions like knowledge about the mean or mode. Let’s say we think a mean of 2 is very likely. The Gamma distribution can be parametrized in terms of the mean as pz.Gamma(mu=2). If we instead believe the mode is likely to be 2, then maxent takes a mode argument.\n\ndist_mean = pz.Gamma(mu=2)\npz.maxent(dist_mean, 0, 3, 0.8)\n\ndist_mode = pz.Gamma()\npz.maxent(dist_mode, 0, 3, 0.8, fixed_stat=(\"mode\",2));\n\n\n\n\n\n\n\n\nNotice that if you call maxent several times in the same cell, as we just did, we will get all the distributions in the same figure. This can be very useful to visually compare several alternatives.\nThe function maxent as others in PreliZ modify distribution in place, so a common workflow is to instantiate a distribution first, perform the elicitation, and then inspect its properties, plot it, or use it in some other way. For instance, we may want to check a summary of some of its properties:\n\ndist_mean.summary(), dist_mode.summary()\n\n(Gamma(mean=2.0, median=1.67, std=1.43, lower=0.37, upper=4.65),\n Gamma(mean=2.32, median=2.22, std=0.86, lower=1.13, upper=3.84))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#other-direct-elicitation-methods-from-preliz",
    "href": "Chapters/Prior_elicitation.html#other-direct-elicitation-methods-from-preliz",
    "title": "12  Prior Elicitation",
    "section": "12.7 Other direct elicitation methods from PreliZ",
    "text": "12.7 Other direct elicitation methods from PreliZ\nThere are many other method for direct elicitation of parameters. For instance the quartile function identifies a distribution that matches specified quartiles, and Quartine_int provides an interactive approach to achieve the same, offering a more hands-on experience for refining distributions.\nOne method worth of special mention is the Roulette method allows which allows users to find a prior distribution by drawing it interactively (Morris, Oakley, and Crowe 2014). The name “roulette” comes from the analogy of placing a limited set of chips where one believes the mass of a distribution should be concentrated. In this method, a grid of m equally sized bins is provided, covering the range of x, and users allocate a total of n chips across the bins. Effectively, this creates a histogram,representing the user’s information about the distribution. The method then identifies the best-fitting distribution from a predefined pool of options, translating the drawn histogram into a suitable probabilistic model.\nAs this is an interactive method we can’t show it here, but you can run the following cell to see how it works.\n\n%matplotlib widget\nresult = pz.Roulette()\n\nAnd this gif should give you an idea on how to use it.\n\n\n\n\n\n\nFigure 12.10: To elicit a distribution, we can interactively draw a histogram, and Roulette will identify the distribution that best matches it.\n\n\n\nOnce we have elicited the distribution we can call .dist attribute to get the selected distribution. In this example, it will be result.dist.\nIf needed, we can combine results for many independent “roulette sessions” with the combine_roulette function. Combining information from different elicitation sessions can be useful to aggregate information from different domain experts. Or even from a single person unable to pick a single option. For instance if we run Roulette twice, and for the first one we get result0 and for the second result1. Then, we can combine both solutions into a single one using:\n\npz.combine_roulette([result0.inputs, result1.inputs], weights=[0.3, 0.7])\n\nIn this example, we assign a larger weight to the results from the second elicitation session, we can do this to reflect uneven degrees of trust. By default, all sessions are weighted equally.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#predictive-elicitation",
    "href": "Chapters/Prior_elicitation.html#predictive-elicitation",
    "title": "12  Prior Elicitation",
    "section": "12.8 Predictive elicitation",
    "text": "12.8 Predictive elicitation\nThe simplest way to perform predictive elicitation is to generate a model, sample from its prior predictive distribution and then evaluate if the samples are consistent with the domain knowledge. If there is disagreement, we can refine the prior distribution and repeat the process. This is usually known as prior predictive check and we discussed them in Chapter 5 together with posterior predictive checks.\nTo assess the agreement between the domain knowledge and the prior predictive distribution we may be tempted to use the observed data, as in posterior predictive checks. But, this can be problematic in many ways. Instead, we recommend using “reference values”. We can obtain a reference value from domain knowledge, like previous studies, asking clients or experts, or educated guesses. They can be typical values, or usually “extreme” values. For instance, if we are studying the temperature of a city, we may use the historical record of world temperature and use -90 as the minimum, 60 as the maximum and 15 as the average. These are inclusive values. Hence this will lead us to very broad priors. If we want something tighter we should use historical records of areas more similar to the city we are studying or even the same city we are studying. These will lead to more informative priors.\n\n12.8.1 Predator vs prey example\nWe are interested in modelling the relationship between the masses of organisms that are prey and organisms that are predators, and since masses vary in orders of magnitude from a 1e-9 grams for a typical cell to a 1.3e8 grams for the blue whale, it is convenient to work on a logarithmic scale.\nLet’s load the data and define the reference values.\n\npp_mass = pd.read_csv(\"../data/pp_mass.csv\")\npp_mass[\"predator_log\"] = np.log(pp_mass[\"predator\"])\npp_mass[\"prey_log\"] = np.log(pp_mass[\"prey\"])\n\n\n# Reference values in log-scale\nrefs = {\"Blue whale\":np.log(1.3e8),\n       \"Typical cell\":np.log(1e-9)}\n\nSo a model might be something like:\n\\[\\begin{align}\n    \\mu =& Normal(\\cdots, \\cdots) \\\\\n    \\sigma =& HalfNormal(\\cdots) \\\\\n    log(mass) =& Normal(\\mu, \\sigma)\n\\end{align}\\]\nLet’s now define a model with some priors and see what these priors imply on the scale of the data. To sample from the predictive prior we use pm.sample_prior_predictive() instead of sample and we need to define dummy observations. This is necessary to indicate to PyMC which term is the likelihood and to control the size of each predicted distribution, but the actual values do not affect the prior predictive distributions.\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", 0, 100)\n    β = pm.Normal(\"β\", 0, 100)\n    σ = pm.HalfNormal(\"σ\", 5)\n    pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n    idata = pm.sample_prior_predictive(samples=100)\n\nSampling: [prey, α, β, σ]\n\n\nNow we can plot the prior predictive distribution and compare it with the reference values.\n\npc = azp.plot_ppc_dist(idata, group=\"prior_predictive\", kind=\"ecdf\")\nazp.add_lines(pc, refs)\n\n\n\n\n\n\n\n\nPriors are so vague that we can not even distinguish the reference values from each other. Let’s try refining our priors.\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", 0, 1)\n    β = pm.Normal(\"β\", 0, 1)\n    σ = pm.HalfNormal(\"σ\", 5)\n    prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n    idata = pm.sample_prior_predictive(samples=100)\n\nSampling: [prey, α, β, σ]\n\n\nWe can plot the prior predictive distribution and compare it with the reference values.\n\npc = azp.plot_ppc_dist(idata, group=\"prior_predictive\", kind=\"ecdf\")\nazp.add_lines(pc, refs)\n\n\n\n\n\n\n\n\nThe new priors still generate some values that are too wide, but at least the bulk of the model predictions are in the right range. So, without too much effort and extra information, we were able to move from a very vague prior to a weakly informative prior. If we decided this prior is still very vague we can add more domain-knowledge.\n\n\n12.8.2 Interactive predictive elicitation\nThe process described in the previous section is straightforward: sample from the prior predictive –&gt; plot –&gt; refine –&gt; repeat. On the good side, this is a very flexible approach and can be a good way to understand the effect of individual parameters in the predictions of a model. But it can be time-consuming and it requires some understanding of the model so you know which parameters to tweak and in which direction.\nOne way to improve this workflow is by adding interactivity. We can do this with PreliZ’s function, predictive_explorer. Which we can not show here, in a full glory but you can see a static image in Figure 12.11, and you can try it for yourself by running the following block of code.\n\ndef pp_model(α_μ=0, α_σ=100, β_μ=0, β_σ=100, σ_σ=5):\n    α = pz.Normal(α_μ, α_σ).rvs()\n    β = pz.Normal(β_μ, β_σ).rvs()\n    σ = pz.HalfNormal(σ_σ).rvs()\n    prey = pz.Normal(α + β * pp_mass.predator_log, σ).rvs()\n    return prey\n\npz.predictive_explorer(pp_model, references=refs)\n\n\n\n\n\n\n\nFigure 12.11: We can use the boxes to specify different prior values and see how the prior predictive changes, here we have changed the initial values of α_σ and β_σ from 100 to 1",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#projective-predictive-elicitation",
    "href": "Chapters/Prior_elicitation.html#projective-predictive-elicitation",
    "title": "12  Prior Elicitation",
    "section": "12.9 Projective predictive elicitation",
    "text": "12.9 Projective predictive elicitation\nIn the previous section we saw that a general route to find good priors is to tweak them until we get a prior predictive distribution that match our previous knowledge. This is fine, but it could be cumbersome.\nIn this section we discuss a procedure to automatize the tweaking. We first think what a reasonable prior predictive distribution could be and then apply and automatic procedure to find the parameters of the priors that indice a prior predictive distribution that is as close as possible to the target distribution. This method is particularly useful when we have a good idea of how the data should look like, but we are not sure how to translate this into a prior distribution.\nIn principle, there are many ways to achieve this. We are going to discuss a method called projective predictive elicitation, as it is inspired in the projective inference method discussed in Chapter 11. The method is experimental and it is implemented in PreliZ.\nTo keep things concrete and familiar let’s assume we are still interested in the predator-prey example. As in the previous examples, our initial model could be something like this. We know the priors are very wide, and we want to refine them.\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", 0, 100)\n    β = pm.Normal(\"β\", 0, 100)\n    σ = pm.HalfNormal(\"σ\", 5)\n    prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n\nThe first thing to do is to define our target distribution. We may be tempted to describe it a Gaussian with most of its mass between the weight of a typical cell and the weight of a blue whale. While this is a resonable assumption, is not enough. We are modeling \\(p(Y \\mid X)\\), so it’s not enough to just describe \\(p(Y)\\). What we can do is to think what values of \\(Y\\) (mass of the predator) we should expect for \\(X\\) the mass of preys. This of course rellies of expert knowledge (as priors should). For this example we are going to use some information from the data and some prior knowledge. We can argue that we are only weakly informing the prior from the data. We are going to take a few quantiles from our \\(X\\), the mass of the preys. And then we are going to assume that usually predator are larger that prey.\n\nq = [0.1, 0.25, 0.5, 0.75, 0.9]\nx_prior = np.quantile(pp_mass[\"prey_log\"], q)\ntarget = pz.Normal(x_prior+np.log(10), pp_mass[\"predator_log\"].std()/2)\n\nTo be sure we are on the same page, the target distribution should be the distribution that you expect to match in a prior predictive check as discussed in previous sections. Additionally, if the goal is to define weakly informative prior, as it’s usually the case in most analysis, the target will usually be a very approximate distribution.\nNow that we have the target, we write a model using the x_prior, for the observations we just pass a dummy values. The actual values are ignored, only the shape must be correct. Finally we pass the model and target distributions to the ppe function.\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", 0, 100)\n    β = pm.Normal(\"β\", 0, 100)\n    σ = pm.HalfNormal(\"σ\", 5)\n    prey = pm.Normal(\"prey\", α + β * x_prior, σ, observed=[0]*len(x_prior))\n\nprint(pz.ppe(model, target)) \n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/preliz/predictive/ppe.py:63: UserWarning: This method is experimental and under development with no guarantees of correctness.\n                  Use with caution and triple-check the results.\n  warnings.warn(\n\n\n\nwith pm.Model() as model:\n\n    α = pm.Normal(\"α\", mu=2.29, sigma=3.65)\n\n    β = pm.Normal(\"β\", mu=0.992, sigma=0.457)\n\n    σ = pm.HalfNormal(\"σ\", sigma=1.74)\n\n\n\n\n\n\nNow we can check that the suggested prior is reasonable given the provided information. To do this we write the model with the new prior and sample from the prior predictive. Notice that we could have copied the priors as-is, but we’ve rounded them instead. Rounded numbers are easier to read and we don’t care about the exact solution provided by ppe. Feel free to question answer from algorithms, blind obedience is how you end up in a lake.\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", mu=2, sigma=3.5)\n    β = pm.Normal(\"β\", mu=1, sigma=0.5)\n    σ = pm.HalfNormal(\"σ\", sigma=1.7)\n    prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n    idata = pm.sample_prior_predictive()\n\nSampling: [prey, α, β, σ]\n\n\nNow we can plot the prior predictive distribution, the target distribution and compare it with the reference values.\n\npc = azp.plot_ppc_dist(idata, group=\"prior_predictive\", kind=\"ecdf\")\nazp.add_lines(pc, list(refs.values()))\n\n\n\n\n\n\n\n\n\n12.9.1 OK, but what’s under the hood?\nThe main idea is that once we have a target distribution we can define the prior elicitation problem as finding the parameters of the model that induce a prior predictive distribution as close as possible to the target distribution. Put like this, it’s just a standard inference task—except instead of conditioning on real data, we’re conditioning on synthetic data, a.k.a. our target distribution. Conceptually that’s what ppe is doing, but we still have two plot twists ahead of us.\nThe procedure is as follows:\n\nGenerate a sample from the target distribution.\nMaximize the model’s likelihood wrt that sample (i.e. we find the parameters for a fixed “observation”).\nRepeat 1 and 2 a few hundred times.\nCollect the optimization results, one per prior parameter in the original model.\nUse MLE to fit the optimized values to their corresponding families in the original model.\n\nInstead of using standard inference methods like MCMC in steps 1-2 we are using projection inference, see Chapter 11 for details. Essentially we are approximating a posterior using an optimization method. We say posterior, because from the inference point we are computing a posterior, from the prior elicitation perspective this is a prior. On the last step, we use a second approximation, we fit the marginals of the projected posterior into standard distributions used by PPLs, i.e. Gaussian, exponential, Poisson, etc. We need this step so we can write the resulting priors in a way that we could feed into PPLs like PyMC or CmdStanPy.\nThis procedure partially ignores the prior information in the model. For example, we would have obtained the same result with the prior α = pm.Normal(\"α\", 0, 0.001) or α = pm.Normal(\"α\", 0, 100). The reason is for this behaviour is that the function we use for the optimization procedure is just the likelihood. In step 5 (the last step) we use the information about each prior families i.e. α = pm.Normal(\"α\", 20, 1) will not produce the same result as α = pm.Gamma(\"α\", mu=20, sigma=1). Because the MLE is done for a given family. In principle, we could even ignore this information and fit the optimized values to many families and pick the best fit. This allows the procedure to suggest alternative families. For instance, it could be that we use a Normal for a given parameters but the optimization only found positive values so a Gamma or HalfNormal could be a better choice.\nInformation from the prior it’s also used to initialize the optimization routine. So it can have an effect of the results, but for that to happen the prior has to be very off with respect to the target. Internally, ppe performs many optimizations (step 3), each time with respect to a different sample from the target. The result of one optimization is stored as one projected posterior “sample” and also used as the initial guess for the next one. For the very first optimization, the one initialized from the prior, the result is discarded and only used as the initial guess for the next step.\nAnother piece of information that is ignored is the observed data, the procedure only takes into account the sample size, but not the actual values. So you can pass dummy values, changing the sample size can we used to obtain more narrow (larger sample size) or more vague (smaller sample size) priors. This effect needs further evaluations.\n\n\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The Prior Can Often Only Be Understood in the Context of the Likelihood.” Entropy 19 (10): 555. https://doi.org/10.3390/e19100555.\n\n\nIcazatti, Alejandro, Oriol Abril-Pla, Arto Klami, and Osvaldo A Martin. 2023. “PreliZ: A tool-box for prior elicitation.” Journal of Open Source Software 8 (89): 5499. https://doi.org/10.21105/joss.05499.\n\n\nJaynes, E. T. 2003. Probability Theory: The Logic of Science. Edited by G. Larry Bretthorst. Cambridge, UK ; New York, NY: Cambridge University Press. https://bayes.wustl.edu/etj/prob/book.pdf.\n\n\nMikkola, Petrus, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, et al. 2024. “Prior Knowledge Elicitation: The Past, Present, and Future.” Bayesian Analysis 19 (4): 1129–61. https://doi.org/10.1214/23-BA1381.\n\n\nMorris, David E., Jeremy E. Oakley, and John A. Crowe. 2014. “A Web-Based Tool for Eliciting Probability Distributions from Experts.” Environmental Modelling & Software 52: 1–4. https://doi.org/10.1016/j.envsoft.2013.10.010.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Simulation_based_calibration.html",
    "href": "Chapters/Simulation_based_calibration.html",
    "title": "13  Simulation-based calibration checking",
    "section": "",
    "text": "13.1 Simulation-based calibration\nAs we already discussed in Chapter 4 we need to validate the statistical computation we perform in Bayesian inference. Such validation is necessary to ensure that the statistical computation is done correctly and in a reasonable amount of time (Gelman et al. 2020). For that we usually rely on a battery of numerical and visuals summaries including, \\(\\hat R\\), effective sample size (ESS), divergences and energy (for HMC) Betancourt (2016), rank plots, trace plots. However, while convergence diagnostics can indicate issues, they do not indicate the direction or magnitude of the induced bias. Additionally, for very difficult-to-sample models we may want to complement these sampling diagnostics with some extra test, as we will see next. Finally, for some inference methods like SMC, variational inference, or amortized inference, good “inference” diagnostics are not yet available.\nAs and old adage says, “The only truth is the reality”, or in Bayesian terms, the only way to know for sure if a model is well specified is to compare with some data and evaluate the fit, we already discussed this in Chapter 5. In that section we saw how to use prior/posterior predictive checks to evaluate the fit of a model to domain-knowledge/data. But real data can be messy so sometimes is preferable to use simulated data.\nThe basic idea of using simulated data to validate inference is to generate data with known parameters and then fit the model to that data, then we check if we are able to recover the correct parameter values. This is a common practice when developing new models and can help us to understand the limitations of our models, planning experiments, and evaluating the performance of inference methods. Typically we choose parameter values that seem reasonable a priori and then simulate data often mimicking the structure of the real data we are interested in.\nA key aspect of this approach is the selection of the parameters to simulate the data. When planning experiments, i.e. when the data is not yet available, we can use domain knowledge to select the parameters. We may want to test for expected values or maybe interested in testing the model under extreme conditions. Alternatively, if we have the data we can use the posterior distribution so select the parameters\nInstead of manually selecting the parameters to generate the data we can perform a more robust, systematic approach by using and that is the main topic of this chapter.\nSimulation-based calibration (SBC) is a method to validate Bayesian computation (Cook, Gelman, and and 2006; Talts et al. 2020; Modrák et al. 2025; Säilynoja et al. 2025). SBC can be used to validate sampling algorithms such as MCMC, or any other numerical Bayesian inference method. The central idea in SBC refers to the validation of an inference algorithm and/or model implementation through repeated inference on data simulated from a generative model.\nIn the literature you are going to find the terms “simulated based calibration” or “simulated based calibration checking”, both refer to the same idea. But the “checking” term is sometimes added to emphasize that these methods are not designed to produce calibrated models, but to measure departure from calibration. In other words, these are diagnostic tools.\nThere are two main variants of SBC:\nPrior SBC is well-suited for evaluating whether inference performs correctly across a broad range of possible datasets. If the prior approximatelly represents the information we have about the parameters, then the prior predictive distribution will be a good approximation of the data we expect to observe under very general conditions. However, in practice, it is common to define models using vague priors or weakly informative priors that hence will induce a prior predictive distribution that is too broad, or even unrealistic. In those cases, the simulated data may not be representative of the data that we will observe in practice, or at lest not representative on the conditions we are really interested in. For those cases we should prefer Posterior SBC, which is more appropriate to evaluate the inference algorithm in the neighbourhood of the posterior distribution. And hence closer to the data we expect to observe in practice.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation-based calibration checking</span>"
    ]
  },
  {
    "objectID": "Chapters/Simulation_based_calibration.html#simulation-based-calibration",
    "href": "Chapters/Simulation_based_calibration.html#simulation-based-calibration",
    "title": "13  Simulation-based calibration checking",
    "section": "",
    "text": "Prior SBC (Modrák et al. 2025), where the generative model uses parameters drawn from the prior.\nPosterior SBC (Säilynoja et al. 2025), where the generative model uses parameters drawn from the posterior distribution.\n\n\n\n13.1.1 Prior SBC\nThe basic steps of prior SBC are:\n\nSample parameters from the prior distribution.\nSimulate simulated data using the sampled parameters, i.e sample from the prior predictive distribution.\nFit the model to the simulated data to obtain a posterior distribution.\nCompare the posterior distribution to the prior distribution.\n\nIf everything is working correctly, we should be able to recover the prior distribution. In practice, there are many ways to compare the posterior distribution to the prior distribution on way is using the PIT-ECDFs similar as we saw in Chapter 5, for posterior predictive checking, with the difference that now instead of comparing predicted and observed data, we compare samples generated from the prior \\(\\theta^{\\star}\\) to samples generated from the posterior distribution \\(\\theta^{\\star\\star}\\), conditioned on the simulated data \\(y^{\\star}\\), which is itself generated from the prior distribution \\(p(\\theta)\\).\n\\[\np(\\theta^{\\star\\star}_i \\le \\theta^{\\star}_i \\mid y^{\\star}),\n\\]\nAs we saw for the posterior predictive checks we expected the distribution to be uniform.\nSBC works because of the self-consistency of Bayesian models. We discuss this idea in the next section. If you already known this result, you can skip it.\n\n\n13.1.2 Self-consistency of Bayesian models\nA mental model when doing Bayesian inference is that there is a data generating process that produces the data we observe. We do not know this process, but we have a prior distribution \\(p(\\theta)\\) that represents our information about the parameters of the model and a likelihood function \\(p(y \\mid \\theta)\\) that describes how the data is generated given the parameters. We then combine these two components to compute a posterior distribution \\(p(\\theta \\mid y)\\) using Bayes’ theorem:\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}.\n\\]\nor equivalently,\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta).\n\\]\nTo compute the posterior distribution we need to evaluate the data \\(y\\), but for SBC we don’t have any real data. It’s all simulated data. So start by sampling from the prior:\n\\[\n\\theta^{\\star} \\sim p(\\theta),\n\\]\nand then we simulate data\n\\[\ny^{\\star} \\sim p(y \\mid \\theta^{\\star}).\n\\]\nThis two step process is equivalent as sampling from the joint distribution of simulated data and parameters \\((y^{\\star}, \\theta^{\\star})\\)\n\\[\n(y^{\\star}, \\theta^{\\star}) \\sim p(y, \\theta)\n\\]\nWe also know that the joint distribution of data and parameters is proportional to the posterior distribution.\n\\[\n(y, \\theta) \\propto p(\\theta \\mid y)\n\\]\nThe above is valid for any data \\(y\\), but in this case we are using the simulated data \\(y^{\\star}\\), so we can write:\n\\[\n\\theta^{\\star} \\sim p(\\theta \\mid y^{\\star}).\n\\]\nWhich tell us that the simulated parameter \\(\\theta^{\\star}\\) is sampled from the posterior distribution.\nIf this explanation does not resonate with you, we can try another approach.\nWe start from a Bayesian model specified using a prior \\(p(\\theta)\\) and a likelihood \\(p(y \\mid \\theta)\\). We know define the following quantities:\n\n\\(\\theta^{\\star} \\sim p(\\theta)\\), the simulated parameter.\n\\(y^{\\star} \\sim p(y \\mid \\theta^{\\star})\\), the syntethic data.\n\\(\\theta^{\\star\\star} \\sim p(\\theta \\mid y^{\\star})\\), the posterior parameter conditioned on \\(y^{\\star}\\).\n\nIt is common in the literature to overload the notation for the densities and use the same symbol, like \\(p\\) (or some other symbol), for all the densities involved in the model. In this case we are going to avoid this notation and write the densities explicitly to avoid confusion. \\[\np_{\\text{SBC}}(y^{\\star}, \\theta^{\\star}, \\theta^{\\star\\star}) = p_{\\text{prior}}(\\theta^{\\star\\star}) \\: p_{\\text{obs}}(y \\mid \\theta^{\\star\\star}) \\: p_{\\text{post}}(\\theta^{\\star} \\mid y^{\\star}, \\theta^{\\star\\star}).\n\\]\nWe can simplify this expression by noticing that\n\\[p_{\\text{post}}(\\theta^{\\star} \\mid y^{\\star}, \\theta^{\\star\\star}) = p_{\\text{post}}(\\theta^{\\star} \\mid y^{\\star})\\],\nbecause \\(\\theta^{\\star\\star}\\) provides no additional information once $ y^{}$ is known. Thus, we have:\n\\[\np_{\\text{SBC}}(y^{\\star}, \\theta^{\\star}, \\theta^{\\star\\star}) = p_{\\text{prior}}(\\theta^{\\star\\star}) \\: p_{\\text{obs}}(y^{\\star} \\mid \\theta^{\\star\\star}) \\: p_{\\text{post}}(\\theta^{\\star} \\mid y^{\\star}).\n\\]\nNow, using Bayes’ Theorem:\n\\[p_{\\text{prior}}(\\theta^{\\star\\star}) p_{\\text{obs}}(y^{\\star} \\mid \\theta^{\\star\\star}) = p_{\\text{marginal}}(y^{\\star}) p_{\\text{posterior}}(\\theta^{\\star\\star} \\mid y^{\\star})\\],\nSo we can rewrite the expression as:\n\\[\np_{\\text{SBC}}(y^{\\star}, \\theta^{\\star}, \\theta^{\\star\\star}) = p_{\\text{marginal}}(y^{\\star}) \\: p_{\\text{posterior}}(\\theta^{\\star\\star} \\mid y^{\\star}) \\: p_{\\text{posterior}}(\\theta^{\\star} \\mid y^{\\star}).\n\\]\nThis expression shows that both \\(\\theta^{\\star}\\) and \\(\\theta^{\\star\\star}\\) are distributed according to the same posterior distribution. In other words, even though \\(\\theta^{\\star}\\) was used to generate the simulated data \\(y^{\\star}\\), and \\(\\theta^{\\star\\star}\\) was drawn from the posterior given that data, both end up being samples from the same distribution. This is the key idea behind SBC: if we sample from the prior and then use that sample to generate simulated data, the posterior distribution computed from that data should be indistinguishable from the prior distribution.\n\n\n13.1.3 Prior SBC in practice\nTo show how to implement SBC in practice we are going to use the simuk package. This package provides a simple interface for performing SBC using different probabilistic programming languages (PPLs). The package is designed to be easy to use and flexible, allowing users to perform SBC with minimal effort. This is still in development, so some features may not be available yet.\n\nPyMCCmdStanPy\n\n\n\nimport pymc as pm\nimport simuk as sim\n\ndata = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\nsigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n\nwith pm.Model() as centered_eight:\n    mu = pm.Normal('mu', mu=0, sigma=5)\n    tau = pm.HalfCauchy('tau', beta=5)\n    theta = pm.Normal('theta', mu=mu, sigma=tau, shape=8)\n    y_obs = pm.Normal('y', mu=theta, sigma=sigma, observed=data)\n\nsbc = simuk.SBC(centered_eight,\n    num_simulations=1000,\n    sample_kwargs={'draws': 500, 'tune': 500})\n\nsbc.run_simulations();\n\nsbc_results = sbc.simulations\n\n\n\n## coming soon\n\n\n\nTo assess the calibration of the inference, we inspect the uniformity of the PIT values of the posterior samples with respect to the prior samples. To do that we can simply call the plot_ecdf_pit function from the ArviZ package.\n\nazp.plot_ecdf_pit(sbc_results,\n              visuals={\"xlabel\":False},\n);\n\n\n\n\n\n\n\n\n\n\n13.1.4 Posterior SBC\nThe basic steps of posterior SBC are:\n\nSample parameters from the posterior distribution.\nSimulate data using the sampled parameters, i.e sample from the posterior predictive distribution.\nFit the model to the simulated data to obtain a posterior distribution.\nCompare the new posterior distribution to the posterior distribution of the original data.\n\nIn practice what we do in step 3 is to sample from the augmented posterior distribution, which is the posterior distribution of the original data plus the simulated data.\n\n\n13.1.5 Posterior SBC in practice\nComing soon, we are still working on the implementation of posterior SBC in the simuk package.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation-based calibration checking</span>"
    ]
  },
  {
    "objectID": "Chapters/Simulation_based_calibration.html#quantities-to-test",
    "href": "Chapters/Simulation_based_calibration.html#quantities-to-test",
    "title": "13  Simulation-based calibration checking",
    "section": "13.2 Quantities to test",
    "text": "13.2 Quantities to test\nThe most direct quantities to test are the parameter themselves, but we may want to evaluate other quantities. For instance, we can use of the joint log-likelihood as a test statistic for assessing the calibration (Modrák et al. 2025). That is, instead of \\(\\theta^{\\star}\\) and \\(\\theta^{\\star\\star}\\), we can use \\(p(y \\mid \\theta^{\\star})\\) and \\(p(y \\mid \\theta^{\\star\\star})\\).\nUsing the log-likelihood as a test quantity helps detect calibration issues that arise when the model partially ignores the data. An extreme case of this is a model that completely ignores the data, always resulting in a posterior that is identical to the prior. The log-likelihood is particularly useful for models with high-dimensional parameter spaces, where we may not be interested in checking each individual parameter. Instead, the log-likelihood provides a simple summary of the model’s parameters. Moreover, relying on the joint log-likelihood avoids the need for multiple comparison corrections that would be necessary if we evaluated each parameter separately.\n\n\n\n\nBetancourt, Michael. 2016. “Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.” https://arxiv.org/abs/1604.00695.\n\n\nCook, Samantha R, Andrew Gelman, and Donald B Rubin and. 2006. “Validation of Software for Bayesian Models Using Posterior Quantiles.” Journal of Computational and Graphical Statistics 15 (3): 675–92. https://doi.org/10.1198/106186006X136976.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nModrák, Martin, Angie H. Moon, Shinyoung Kim, Paul Bürkner, Niko Huurre, Kateřina Faltejsková, Andrew Gelman, and Aki Vehtari. 2025. “Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity.” Bayesian Analysis 20 (2): 461–88. https://doi.org/10.1214/23-BA1404.\n\n\nSäilynoja, Teemu, Marvin Schmitt, Paul-Christian Bürkner, and Aki Vehtari. 2025. “Posterior SBC: Simulation-Based Calibration Checking Conditional on Data.” https://arxiv.org/abs/2502.03279.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” https://arxiv.org/abs/1804.06788.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\widehat{R}\\) for Assessing Convergence of MCMC (with Discussion).” Bayesian Analysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation-based calibration checking</span>"
    ]
  },
  {
    "objectID": "Chapters/Presenting_results.html",
    "href": "Chapters/Presenting_results.html",
    "title": "14  Presentation of Results",
    "section": "",
    "text": "14.1 What to report?\nFor Bayesian analysis, we consider that adopting a Bayesian workflow, rather than just Bayesian inference, is central to reducing most of the typical problems found in Bayesian data analysis (Gelman et al. 2020; Martin and Teste 2022). Such workflow is discussed in detail by (Gelman et al. 2020) and we provide a brief summary of it in Chapter 15. A report of the results of a Bayesian analysis should help reflect such workflow.\nThe presentation of the results will depend on the audience, the context, and the goals of the analysis. For example, a presentation for a scientific paper will be different from a presentation for a business meeting. However, there are some general principles that can be applied to any presentation of results. Additionally, modern Bayesian analysis can be a complex process, and it is important to present the results in a way that is clear and understandable for the audience. This can be achieved by using visualizations, summaries, and explanations that are tailored to the audience’s level of expertise and familiarity with Bayesian analysis. Usually, the results will be split into two parts, one that provides the most important results and a second part that provides more details for those who are interested in the technical aspects of the analysis. For example, for a scientific paper, the first part should be included in the main text, while the second part can be included in the supplementary material. For a business meeting, the first part can be presented in a slide deck, while the second part can be included in a report or a technical document. Additionally, many of the details could be included in the code itself, that is stored in a repository, so that interested readers can access it and reproduce the analysis. For scientific results public repositories like Zenodo, GitHub or GitLab can be used to store the code and data.\n(Kruschke 2021) provides guidelines on what to report in a Bayesian analysis. These guidelines are based on his experience but also insights from previous recommendations. Here we reproduce a summary of his recommendations with some modifications and additions. Some of the differences reflect our own experience and preferences, for example we usually don’t recommend using Bayes factors, and instead we recommend using PSIS-LOO-CV for model comparison as discussed in Chapter 7.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Presentation of Results</span>"
    ]
  },
  {
    "objectID": "Chapters/Presenting_results.html#what-to-report",
    "href": "Chapters/Presenting_results.html#what-to-report",
    "title": "14  Presentation of Results",
    "section": "",
    "text": "Explain the model.\n\nExplain the variables in your model, for regression models make it clear which are the dependent (predicted) variables and which are the independent (predictor) variables. Clearly distinguish between the parameters that are of direct interest for your analysis and those are solely required for the model to fit the data but not of direct interest. If the model is hierarchical/multilevel, be sure that the hierarchical structure is clearly explained.\nExplain and justify the prior distribution of the parameters in the model.\nInclude a formal specification (mathematical or computer code) of the models. A graphical representations like plate-notation, like the ones generated by PyMC or Bambi can also help, especially for nonstandard models and/or more applied audiences.\nReport prior predictive checks (Chapter 5) to demonstrate that the prior generates simulated data consistent with the assumed prior knowledge.\n\nReport details of the computation\n\nReport the software used. This helps to ensure that the analysis can be reproduced. But it’s also a way to support the software developers, in particular if the software is open source.\nReport MCMC diagnostics as discussed in Chapter 4. For non-MCMC methods, Simulation-Based Calibration can be used to assess the quality of the inference (Chapter 13).\n\nDescribe the posterior distribution\n\nProvide a posterior predictive check to show that the model usefully mimics the data (Chapter 5). Discuss mismatches between the model and the data, if any.\nSummarize posterior of variables. For continuous parameters, derived variables and predicted values, report the central tendency (usually mean or median) and credible intervals. Be clear if you are using HDI or ETI, and make it clear the mass of the credible interval (for example, 94%).\n\nReport decisions (if any) and their criteria\n\nWhy decisions? Explain why the decisions are theoretically meaningful and which decision procedure is being used. Regardless of which decision procedure is used, if it addresses null values, it should be able to accept the null value not only reject it.\nLoss function. If utilities and a loss function for a decision rule are defined, these should be explained and reported.\nROPE limits. If using a continuous-parameter posterior distribution as the basis for decision, state and justify the limits of the ROPE and the required probability mass.\nDecision threshold and model probabilities. If using model comparison or hypothesis testing as the basis for a decision, state and justify the decision threshold for the posterior model probability, and the minimum prior model probability that would make the posterior model probability exceed the decision threshold.\nEstimated values. If deciding about null values, always also report the estimate of the parameter value (central tendency and credible interval).\n\nReport sensitivity analysis\n\nUse prior predictive checks and sensitivity analysis like the ones discussed in Chapter 6 to assess the robustness of the results to the choice of priors.\nIf making decisions, report whether decisions change under different priors.\n\nReport alternative models\n\nReport the results of models that were discarded. This is important to assess the robustness of the results to the choice of model. Explain why these models were discarded and why the final model was chosen.\nIf using PSIS-LOO-CV Chapter 7 for model comparison, report the expected log predictive density (ELPD) and the standard error of the ELPD. This is important to assess the robustness of the results to the choice of model. If the model comparison is not relevant, explain why it is not relevant.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Presentation of Results</span>"
    ]
  },
  {
    "objectID": "Chapters/Presenting_results.html#monte-carlo-standard-error-and-accuracy",
    "href": "Chapters/Presenting_results.html#monte-carlo-standard-error-and-accuracy",
    "title": "14  Presentation of Results",
    "section": "14.2 Monte Carlo Standard error and accuracy",
    "text": "14.2 Monte Carlo Standard error and accuracy\n\n\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nKruschke, John K. 2021. “Bayesian Analysis Reporting Guidelines.” Nature Human Behaviour 5 (10): 1282–91. https://doi.org/10.1038/s41562-021-01177-7.\n\n\nMartin, Osvaldo A., and François P. Teste. 2022. “A Call for Changing Data Analysis Practices: From Philosophy and Comprehensive Reporting to Modeling Approaches and Back.” Plant and Soil 476 (1): 743–53. https://doi.org/10.1007/s11104-022-05329-0.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Presentation of Results</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html",
    "href": "Chapters/Bayesian_workflow.html",
    "title": "15  Bayesian Workflow",
    "section": "",
    "text": "15.1 A picture of a Bayesian Workflow\nIn the previous sections, we discussed a range of non-inferential tasks essential for a successful Bayesian analysis, such as diagnosing the quality of sampling methods, assessing model assumptions, comparing models, and more. Drawing an analogy to Exploratory Data Analysis (EDA), which is used to understand data, we can view these tasks as part of an Exploratory Analysis of Bayesian Models. This involves exploring the models, their relationship with the data, and the resulting outcomes. Recognising the need for tasks beyond pure inference is a cornerstone of modern Bayesian analysis. However, we can take this further by acknowledging that these tasks are interconnected within a broader Bayesian workflow (Gelman et al. 2020).\nApplying Bayesian inference to real-world problems demands not only statistical expertise, subject-matter knowledge, and programming skills but also an acute awareness of the decisions made throughout the data analysis process. These interrelated components form a complex workflow encompassing iterative model building, model checking, validation, troubleshooting computational issues, understanding models, and comparing them.\nWe can think of a Bayesian Workflow in a very abstract way as a graph with infinite nodes and edges representing all the potential alternatives we could take when analysing all the potential datasets. In this sense, there is “THE” Bayesian workflow and for any concrete analysis we only explore a few realizations from it. Alternatively, we can think of a myriad of Bayesian workflows, and thus we should talk about “A” Bayesian workflow. In any way, we are faced with potentially many instances that are context-dependent and we can not concretely talk about any of them without knowing the details of a particular analysis. But what we can do is to discuss some of the elements we should take into account, like we already did in previous chapters. And then provide some guidelines and general recommendations about how to proceed.\nThe methods, tools, and practices for Bayesian analysis will improve over time. As technology advances, we expect automation through software tools, and this guide will evolve accordingly.\nFigure 15.1 shows a simplified Bayesian workflow (Martin, Kumar, and Lao 2021), check the Bayesian Workflow paper for a more detailed representation (Gelman et al. 2020). As you see there are many steps. We need all these steps because models are just lucubrations of our mind with no guarantee of helping us understand the data. We need to first be able to build such a model and then check its usefulness, and if not useful enough keep working, or sometimes stop trying. You may also have noticed the “evaluate samples” step. We need this because we, usually, use computational methods to solve Bayesian models, and here again we have no guarantee these methods always return the correct result (see Chapter 4 for details).\nDesigning a suitable model for a given data analysis task usually requires a mix of statistical expertise, domain knowledge, understanding of computational tools, and perseverance. Rarely a modelling effort is a one-shot process, instead, typically we need to iteratively write, test, and refine models. If you are familiar with writing code, then you already know what we are talking about. Even very short programs require some trial and error. Usually, you need to test it, debug it, and refine it, and sometimes try alternative approaches. The same is true for statistical models, especially when we use code to write and solve them.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html#a-picture-of-a-bayesian-workflow",
    "href": "Chapters/Bayesian_workflow.html#a-picture-of-a-bayesian-workflow",
    "title": "15  Bayesian Workflow",
    "section": "",
    "text": "Figure 15.1: A Bayesian workflow. Solid lines show a linear workflow starting at problem framing and ending in summarizing the results. The dotted lines indicate that workflows usually are non-linear as practitioners usually skip steps or go back to previous steps.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html#a-blueprint-for-effective-bayesian-workflows",
    "href": "Chapters/Bayesian_workflow.html#a-blueprint-for-effective-bayesian-workflows",
    "title": "15  Bayesian Workflow",
    "section": "15.2 A Blueprint for Effective Bayesian Workflows",
    "text": "15.2 A Blueprint for Effective Bayesian Workflows\nOften, especially for newcomers, Bayesian analysis can be overwhelming. In this section, we have collected a series of tips and recommendations so you can get a quick reference. Here we write the recommendations linearly, but in practice, you may need to come back one or more steps and sometimes skip steps. Think of these notes, not as a partiture of a classical piece that a violinist aims at playing almost exactly, but as the musical score that a Jazz bassist follows, you are free to improvise, rearrange some parts, and omit others, and you can even add your notes!\n\n15.2.1 Summarize the problem\nSummarize the key points of your problem, and what you would like to learn from the data. Think also about others, what your boss, client, or colleague would like to find out or learn. This does not need to be super thorough, you can revisit goals later, but they can help you organize your modelling efforts and avoid excessive wandering.\nSometimes you will not have any clear idea of what to expect or what to do, your only expectation will be to get something useful from a dataset, and that’s fine. But other times you may even know what kind of model you want, perhaps your boss explicitly asked you to run this or that analysis. If you already know what kind of problem you want to solve, but are not very familiar with the approach, search for what methods, metrics or visualizations are common for that problem/data, and ask others for advice. This is more important the less familiar you are with that type of problem/data. If you are familiar, then you may already know which methods, visualizations, and summaries you want to use or obtain. Either way write an outline, a roadmap to help you keep focus, and later to track what you have already tried.\n\n\n15.2.2 Get familiar with the data\nIt is always a good idea to perform Exploratory Data Analysis on your data. Blindly modelling your data leads you to all sorts of problems. Taking a look first will save you time and may provide useful ideas. Sometimes it saves you from having to write a Bayesian model at all, perhaps the answer is a scatter plot! In the early stages, a quick dirty figure could be enough but try to be organized as you may need to refer to these plots later on during the modelling or presentation phases.\nWhen exploring the data we want to make sure, we get a really good understanding of it. How to achieve this can vary a lot from dataset to dataset and from analysis to analysis. But there are useful sanity checks that we usually do, like checking for missing values, or errors in the data. Are the data types correct? Are all the values that should be numbers, numbers (usually integers or floats) or they are strings? Which variables are categorical? Which ones are continuous? At this stage, you may need to do some cleaning of your data. This will save you time in the future.\nUsually, we would like to also do some plots, histograms, boxplots, scatter plots, etc. Numerical summaries are also useful, like the mean, and median, for all your data, or by grouping the data, etc.\n\n\n15.2.3 Tell a story for the data\nIt is often helpful to think about how the data could have been generated. This is usually called the data-generating process or data-generating mechanism. We don’t need to find out the True mechanism, many times we just need to think about plausible scenarios.\nMake drawings, and try to be very schematic, doodles and geometrical figures should be enough unless you are a good sketcher. This step can be tricky, so let us use an example. Let’s say you are studying the water levels of a lake, think about what makes the water increase; rain, rivers, etc, and what makes it decrease; evaporation, animals drinking water, energy production, etc. Try to think which elements may be relevant and which could be negligible. Use as much context as you have for your problem. If you feel you don’t have enough context, write down questions and find out who knows.\nTry to keep it simple but not simpler. For instance, a mechanism could be “Pigs’ weight increases the more corn they are fed”, that’s a good mechanism if all you need to predict are your earnings from selling pigs. But it will be an over-simplistic mechanism if you are studying intestine absorption at the cellular level.\nIf you can think of alternative stories and you don’t know how to decide which one is better. Don’t worry, list them all! Maybe we can use the data to decide!\n\n\n15.2.4 Write a model\nTry to translate the data-generating mechanism into a model. If you feel comfortable with math, use that. If you prefer a visual representation like a graphical model, use that. If you like code, then go for it. Incomplete models are fine as a first step. For instance, if you use code, feel free to use pseudo code or add comments to signal missing elements as you think about the model. You can refine it later. A common blocker is trying to do too much too soon.\nTry to start simple, don’t use hierarchies, keep prior 1D (instead of multivariate), skip interactions for linear models, etc. If for some reason you come first with a complex model, that’s ok, but you may want to save it for later use, and try with a simplified version.\nSometimes you may be able to use a standard textbook model or something you saw on a blog post or a talk. It is common that for certain problems people tend to use certain “default” models. That may be a good start, or your final model. Keep things simple, unless you need something else.\nThis is a good step to think about your priors (see Chapter 12 for details), not only which family are you going to use, but what specific parameters. If you don’t have a clue just use some vague prior. But if you have some information, use it. Try to encode very general information, like this parameter can not be negative, or this parameter is likely to be smaller than this, or within this range. Look for the low-hanging fruit, usually that will be enough. The exception will be when you have enough good quality information to define a very precise prior, but even then, that’s something you can add later.\n\n\n15.2.5 Implement the model\nWrite the model in a probabilistic programming language. If you used code in the previous example the line between this step and the previous one, may be diffuse, that’s fine. Try to keep the model simple at first, we can add more layers later as we keep iteration through this workflow. Starting simple usually saves you time in the long run. Simple models are easier to debug and debugging one issue at a time is generally less frustrating than having to fix several issues before our model even runs.\nOnce you have a model, check that the model compiles and/or runs without error. When debugging a model, especially at an earlier stage of the workflow, you may want to reduce the number of tuning and sampling steps, at the beginning a crude posterior approximation is usually enough. Sometimes, it may also be a good idea to reduce the size of the dataset. For large datasets setting aside 50 or 90% of the data could help iterate faster and catch errors earlier. A potential downside is that you may miss the necessary data to uncover some relevant pattern but it could be ok at the very beginning when most of the time is spent fixing simple mistakes or getting familiar with the problem.\n\n\n15.2.6 Evaluate prior predictive distribution\nIt is usually a good idea to generate data from the prior predictive distribution and compare that to your prior knowledge (Mikkola et al. 2024). Is the bulk of the simulated distribution in a reasonable range? Are there any extreme values? Use reference values as a guide. Reference values are empirical data or historical observations, usually, they will be minimum, maximum or expected values. Avoid comparing with the observed data, as that can lead to issues if you are not careful enough (see Chapter 12 for details).\n\n\n15.2.7 Compute posterior\nThere are many ways to compute the posterior, in this document, we have assumed the use of MCMC methods as they are the most general and commonly used methods to estimate the posterior in modern Bayesian analysis.\n\n\n15.2.8 Evaluate samples\nWhen using MCMC methods, we need to check that the samples are good enough. For this, we need to compute diagnostics such as \\(\\hat R\\) (r-hat) and effective sample size (ESS). And evaluate plots such as trace plots and rank plots. We can be more tolerant with diagnostics at the early stages of the workflow, for instance, an \\(\\hat R\\) of 1.1 is acceptable. At the same time, very bad diagnostics could be a signal of a problem with our model(s). We discuss these steps in detail in Chapter 4.\n\n\n15.2.9 Validate the model\nThere are many ways to validate your model, like a posterior predictive check, Bayesian p-values, residual analysis, and recovery parameters from synthetic data (or the most costly simulated-based calibration). Or a combination of all of this. Sometimes you may be able to use a holdout set to evaluate the predictive performance of your model. The main goal here is to find if the model is good enough for your purpose and what limitations the model can have. All models will have limitations, but some limitations may be irrelevant in the context of your analysis, some may be worth removing by improving the models, and others are simply worth knowing they are there. We discuss these steps in detail in Chapter 4.\n\n\n15.2.10 Compare models\nIf you manage to get more than one model (usually a good idea), you may need to define which one you would like to keep (assuming you only need one). To compare models you can use cross-validation and/or information criteria. But you can also use the results from the previous step (model validation). Sometimes we compare models to keep a single model, model comparison can also help us to better understand a model, its strengths and its limitations, and it can also be a motivation to improve a model or try a new one. Model averaging, i.e. combining several models, is usually a simple and effective strategy to improve predictive performance. We discuss these steps in detail in Chapter 7.\n\n\n15.2.11 Summarize results\nSummarize results in a way that helps you reach your goals, did you manage to answer the key questions? Is this something that will convince your boss, your peers or the marketing department? Think of effective ways to show the results. If your audience is very technical do a technical summary, but if your audience only cares about maximizing profit focus on that. Try to use summaries that are easy to understand without hiding valuable details, you don’t want to mislead your audience.\n\n\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. 1st edition. Boca Raton London New York: Chapman; Hall/CRC. https://bayesiancomputationbook.com/.\n\n\nMikkola, Petrus, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, et al. 2024. “Prior Knowledge Elicitation: The Past, Present, and Future.” Bayesian Analysis 19 (4): 1129–61. https://doi.org/10.1214/23-BA1381.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/References.html",
    "href": "Chapters/References.html",
    "title": "References",
    "section": "",
    "text": "Akaike, H. 1974. “A New Look at the Statistical Model\nIdentification.” IEEE Transactions on Automatic Control\n19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nAyer, Miriam, H. D. Brunk, G. M. Ewing, W. T. Reid, and Edward\nSilverman. 1955. “An Empirical Distribution\nFunction for Sampling with Incomplete Information.”\nThe Annals of Mathematical Statistics 26 (4): 641–47. https://doi.org/10.1214/aoms/1177728423.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2004. “No Unbiased Estimator\nof the Variance of k-Fold Cross-Validation.” Journal of\nMachine Learning Research 5: 1089–1105. https://jmlr.csail.mit.edu/papers/v5/grandvalet04a.html.\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel\nMekhnacha. 2013. Bayesian Programming. 1 edition.\nBoca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nBrockmann, H. Jane. 1996. “Satellite Male Groups in Horseshoe\nCrabs, Limulus Polyphemus.” Ethology 102 (1): 1–21. https://doi.org/10.1111/j.1439-0310.1996.tb01099.x.\n\n\nBrooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng, eds. 2011.\nHandbook of Markov Chain\nMonte Carlo. 1 edition. Boca Raton:\nChapman; Hall/CRC. https://doi.org/10.1201/b10905.\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010.\n“BART: Bayesian Additive Regression\nTrees.” The Annals of Applied Statistics 4 (1): 266–98.\nhttps://doi.org/10.1214/09-AOAS285.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nCook, Samantha R, Andrew Gelman, and Donald B Rubin and. 2006.\n“Validation of Software for Bayesian Models Using Posterior\nQuantiles.” Journal of Computational and Graphical\nStatistics 15 (3): 675–92. https://doi.org/10.1198/106186006X136976.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data\nAnalysis: From Magical\nThinking Through Classical\nStatistics.” In Exploring Data\nTables, Trends, and Shapes,\n1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nDimitriadis, Timo, Tilmann Gneiting, and Alexander I. Jordan. 2021.\n“Stable Reliability Diagrams for Probabilistic\nClassifiers.” Proceedings of the National Academy of\nSciences 118 (8): e2016191118. https://doi.org/10.1073/pnas.2016191118.\n\n\nDowney, Allen B. 2025. Think Stats:\nExploratory Data Analysis.\nSebastopol: O’Reilly Media. https://allendowney.github.io/ThinkStats/.\n\n\nFernandes, Michael, Logan Walls, Sean Munson, Jessica Hullman, and\nMatthew Kay. 2018. “Uncertainty Displays\nUsing Quantile Dotplots or\nCDFs Improve Transit\nDecision-Making.” In Proceedings of\nthe 2018 CHI Conference on Human\nFactors in Computing\nSystems, 1–12. CHI ’18. New York, NY,\nUSA: Association for Computing Machinery. https://doi.org/10.1145/3173574.3173718.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and\nAndrew Gelman. 2019. “Visualization in Bayesian Workflow.”\nJournal of the Royal Statistical Society Series A: Statistics in\nSociety 182 (2): 389–402. https://doi.org/10.1111/rssa.12378.\n\n\nGelman, Andrew. 2013. “Two simple examples\nfor understanding posterior p-values whose distributions are far from\nuniform.” Electronic Journal of Statistics 7\n(none): 2595–2602. https://doi.org/10.1214/13-EJS854.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data\nAnalysis. Boca Raton. https://doi.org/10.1201/b16018.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge: Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The\nPrior Can Often Only\nBe Understood in the Context of\nthe Likelihood.” Entropy 19 (10): 555. https://doi.org/10.3390/e19100555.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine\nLearning and Artificial\nIntelligence.” Nature 521 (7553): 452–59.\nhttps://doi.org/10.1038/nature14541.\n\n\nHansen, Morris H., and William N. Hurwitz. 1943. “On the Theory of\nSampling from Finite Populations.” The Annals of Mathematical\nStatistics 14 (4): 333–62. https://doi.org/10.1214/aoms/1177731360.\n\n\nHealy, Kieran. 2019. Data Visualization: A\nPractical Introduction. Princeton, New\nJersey ; Oxford, Oxfordshire: Princeton University Press. https://kieranhealy.org/publications/dataviz/.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical\nPerception: Using Mechanical Turk to Assess Visualization\nDesign.” In Proceedings of the SIGCHI\nConference on Human Factors in\nComputing Systems, 203–12.\nCHI ’10. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/1753326.1753357.\n\n\nHoyer, Stephan, and Joe Hamman. 2017. “Xarray:\nN-D Labeled Arrays\nand Datasets in Python.” Journal of\nOpen Research Software 5 (1). https://doi.org/10.5334/jors.148.\n\n\nIcazatti, Alejandro, Oriol Abril-Pla, Arto Klami, and Osvaldo A Martin.\n2023. “PreliZ: A tool-box for prior\nelicitation.” Journal of Open Source Software 8\n(89): 5499. https://doi.org/10.21105/joss.05499.\n\n\nJaynes, E. T. 2003. Probability Theory:\nThe Logic of Science. Edited\nby G. Larry Bretthorst. Cambridge, UK ; New York, NY: Cambridge\nUniversity Press. https://bayes.wustl.edu/etj/prob/book.pdf.\n\n\nJohnson, Roger W. 1996. “Fitting Percentage of Body Fat to Simple\nBody Measurements.” Journal of Statistics Education 4\n(1). https://doi.org/10.1080/10691898.1996.11910505.\n\n\nKallioinen, Noa, Topi Paananen, Paul-Christian Bürkner, and Aki Vehtari.\n2023. “Detecting and Diagnosing Prior and Likelihood Sensitivity\nwith Power-Scaling.” Statistics and Computing 34 (1):\n57. https://doi.org/10.1007/s11222-023-10366-5.\n\n\nKay, Matthew, Tara Kola, Jessica R. Hullman, and Sean A. Munson. 2016.\n“When (Ish) Is My Bus?\nUser-Centered Visualizations of\nUncertainty in Everyday, Mobile\nPredictive Systems.” In Proceedings\nof the 2016 CHI Conference on\nHuman Factors in Computing\nSystems, 5092–5103. CHI ’16. New York,\nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/2858036.2858558.\n\n\nKleiber, Christian, and Achim Zeileis. 2016. “Visualizing Count\nData Regressions Using Rootograms.” The American\nStatistician 70 (3): 296–303. https://doi.org/10.1080/00031305.2016.1173590.\n\n\nKruschke, John K. 2021. “Bayesian Analysis\nReporting Guidelines.” Nature Human\nBehaviour 5 (10): 1282–91. https://doi.org/10.1038/s41562-021-01177-7.\n\n\nLink, William A., and Mitchell J. Eaton. 2012. “On Thinning of\nChains in MCMC.” Methods in Ecology and Evolution 3 (1):\n112–15. https://doi.org/10.1111/j.2041-210X.2011.00131.x.\n\n\nMacEachern, Steven N., and L. Mark Berliner. 1994. “Subsampling\nthe Gibbs Sampler.” The American\nStatistician 48 (3): 188–90. https://doi.org/10.2307/2684714.\n\n\nMagnusson, Måns, Michael Riis Andersen, Johan Jonasson, and Aki Vehtari.\n2020. “Leave-One-Out Cross-Validation for Model Comparison in\nLarge Data.” In Proceedings of the 23rd International\nConference on Artificial Intelligence and Statistics. Vol. 108.\nProceedings of Machine Learning Research. PMLR. https://arxiv.org/abs/2001.00980.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian\nModeling and Computation in\nPython. 1st edition. Boca Raton London New York:\nChapman; Hall/CRC. https://bayesiancomputationbook.com/.\n\n\nMartin, Osvaldo A., and François P. Teste. 2022. “A Call for\nChanging Data Analysis Practices: From Philosophy and Comprehensive\nReporting to Modeling Approaches and Back.” Plant and\nSoil 476 (1): 743–53. https://doi.org/10.1007/s11104-022-05329-0.\n\n\nMcLatchie, Yann, Sölvi Rögnvaldsson, Frank Weber, and Aki Vehtari. 2023.\n“Robust and Efficient Projection Predictive Inference.” https://arxiv.org/abs/2306.15581.\n\n\nMeng, Xiao-Li. 1994. “Posterior Predictive p-Values.” The\nAnnals of Statistics 22 (3): 1142–60. https://doi.org/10.1214/aos/1176325622.\n\n\nMikkola, Petrus, Osvaldo A. Martin, Suyog Chandramouli, Marcelo\nHartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, et al. 2024.\n“Prior Knowledge Elicitation: The Past, Present, and\nFuture.” Bayesian Analysis 19 (4): 1129–61. https://doi.org/10.1214/23-BA1381.\n\n\nModrák, Martin, Angie H. Moon, Shinyoung Kim, Paul Bürkner, Niko Huurre,\nKateřina Faltejsková, Andrew Gelman, and Aki Vehtari. 2025. “Simulation-Based Calibration Checking for Bayesian\nComputation: The Choice of Test Quantities Shapes\nSensitivity.” Bayesian Analysis 20 (2): 461–88.\nhttps://doi.org/10.1214/23-BA1404.\n\n\nMorris, David E., Jeremy E. Oakley, and John A. Crowe. 2014. “A\nWeb-Based Tool for Eliciting Probability Distributions from\nExperts.” Environmental Modelling & Software 52:\n1–4. https://doi.org/10.1016/j.envsoft.2013.10.010.\n\n\nNguyen, Hoang-Vu, and Jilles Vreeken. 2015. “Non-Parametric\nJensen-Shannon Divergence.” In Machine Learning and Knowledge\nDiscovery in Databases, edited by Annalisa Appice, Pedro Pereira\nRodrigues, Vítor Santos Costa, João Gama, Alípio Jorge, and Carlos\nSoares, 173–89. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-23525-7_11.\n\n\nPaananen, Topi, Juho Piironen, Paul-Christian Bürkner, and Aki Vehtari.\n2020. “Implicitly Adaptive Importance Sampling.” https://arxiv.org/abs/1906.08850.\n\n\nPiironen, Juho, Markus Paasiniemi, and Aki Vehtari. 2020. “Projective inference in high-dimensional problems:\nPrediction and feature selection.” Electronic Journal\nof Statistics 14 (1): 2155–97. https://doi.org/10.1214/20-EJS1711.\n\n\nQuiroga, Miriana, Pablo G Garay, Juan M. Alonso, Juan Martin Loyola, and\nOsvaldo A Martin. 2022. “Bayesian Additive Regression Trees for\nProbabilistic Programming.” arXiv. https://doi.org/10.48550/ARXIV.2206.03619.\n\n\nSäilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022.\n“Graphical Test for Discrete Uniformity and Its Applications in\nGoodness-of-Fit Evaluation and Multiple Sample Comparison.”\nStatistics and Computing 32 (2): 32. https://doi.org/10.1007/s11222-022-10090-6.\n\n\nSäilynoja, Teemu, Andrew R. Johnson, Osvaldo A. Martin, and Aki Vehtari.\n2025. “Recommendations for Visual Predictive Checks in Bayesian\nWorkflow.” https://arxiv.org/abs/2503.01509.\n\n\nSäilynoja, Teemu, Marvin Schmitt, Paul-Christian Bürkner, and Aki\nVehtari. 2025. “Posterior SBC: Simulation-Based Calibration\nChecking Conditional on Data.” https://arxiv.org/abs/2502.03279.\n\n\nSoch, Joram, Thomas J Faulkenberry, Kenneth Petrykowski, and Carsten\nAllefeld. 2024. “The Book of Statistical Proofs.” https://doi.org/10.5281/ZENODO.4305949.\n\n\nSuorsa, Saku, and Aki Vehtari. 2026. “Predictive Assessment and\nComparison of Bayesian Survival Models for Cancer Recurrence.” https://arxiv.org/abs/2601.01662.\n\n\nTadesse, Mahlet G., and Marina Vannucci, eds. 2022. Handbook of\nBayesian Variable Selection.\nBoca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003089018.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew\nGelman. 2020. “Validating Bayesian Inference Algorithms with\nSimulation-Based Calibration.” https://arxiv.org/abs/1804.06788.\n\n\nTukey, John W. 1977. Exploratory Data\nAnalysis. 1 edition. Pearson.\n\n\nUnwin, Antony. 2024. Getting (More Out of) Graphics:\nPractice and Principles of Data\nVisualisation. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003131212.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical\nBayesian Model Evaluation Using Leave-One-Out Cross-Validation and\nWAIC.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and\nPaul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An\nImproved R̂ for Assessing\nConvergence of MCMC (with Discussion).” Bayesian\nAnalysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied\nStatistics with S. 4th edition. New York:\nSpringer. https://doi.org/10.1007/978-0-387-21706-2.\n\n\nWatanabe, Sumio. 2013. “A Widely\nApplicable Bayesian Information\nCriterion.” Journal of Machine Learning\nResearch 14 (March): 867–97. https://dl.acm.org/doi/10.5555/2567709.2502609.\n\n\nWilke, Claus O. 2019. Fundamentals of Data\nVisualization: A Primer on\nMaking Informative and Compelling\nFigures. Beijing Boston Farnham Sebastopol Tokyo:\nO’Reilly Media. https://clauswilke.com/dataviz/.\n\n\nWilkinson, Leland. 1999. “Dot Plots.” The\nAmerican Statistician 53 (3): 276–81. https://doi.org/10.1080/00031305.1999.10474474.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018.\n“Using Stacking to Average Bayesian\nPredictive Distributions (with Discussion).” Bayesian\nAnalysis 13 (3): 917–1007. https://doi.org/10.1214/17-BA1091.",
    "crumbs": [
      "References"
    ]
  }
]